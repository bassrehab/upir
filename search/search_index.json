{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"UPIR: Universal Plan Intermediate Representation","text":"<p>Formal verification, automatic synthesis, and continuous optimization of distributed system architectures.</p>"},{"location":"#overview","title":"Overview","text":"<p>UPIR (Universal Plan Intermediate Representation) is an open-source framework for formally specifying, verifying, synthesizing, and optimizing distributed system architectures. It bridges the gap between high-level architectural requirements and production-ready implementations.</p>"},{"location":"#what-upir-does","title":"What UPIR Does","text":"<ul> <li>Formal Verification: Prove that your architecture satisfies correctness properties using SMT solvers</li> <li>Automatic Synthesis: Generate implementation code from architectural specifications using CEGIS</li> <li>Continuous Optimization: Learn from production metrics to improve architectures using reinforcement learning</li> <li>Pattern Management: Extract and reuse proven architectural patterns</li> <li>Incremental Verification: Cache proofs for faster iteration</li> </ul>"},{"location":"#why-upir","title":"Why UPIR?","text":"<p>Designing distributed systems is hard. Traditional approaches rely on:</p> <ul> <li>Manual design prone to errors</li> <li>Ad-hoc validation that misses edge cases</li> <li>Trial-and-error optimization that wastes resources</li> <li>Reinventing patterns instead of reusing proven solutions</li> </ul> <p>UPIR automates these processes using formal methods, program synthesis, and machine learning.</p>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#formal-specifications-with-temporal-logic","title":"Formal Specifications with Temporal Logic","text":"<p>Define requirements using Linear Temporal Logic (LTL):</p> <pre><code>from upir.core.temporal import TemporalOperator, TemporalProperty\n\n# ALWAYS: Data consistency must always hold\nalways_consistent = TemporalProperty(\n    operator=TemporalOperator.ALWAYS,\n    predicate=\"data_consistent\"\n)\n\n# WITHIN: Respond within 100ms\nlow_latency = TemporalProperty(\n    operator=TemporalOperator.WITHIN,\n    predicate=\"respond\",\n    time_bound=100  # milliseconds\n)\n</code></pre>"},{"location":"#smt-based-verification","title":"SMT-Based Verification","text":"<p>Verify architectures satisfy specifications:</p> <pre><code>from upir.verification.verifier import Verifier\nfrom upir.verification.solver import VerificationStatus\n\nverifier = Verifier()\nresults = verifier.verify_specification(upir)\n\nif results.status == VerificationStatus.PROVED:\n    print(\"\u2713 Architecture verified!\")\n</code></pre>"},{"location":"#cegis-synthesis","title":"CEGIS Synthesis","text":"<p>Generate code from specifications:</p> <pre><code>from upir.synthesis.cegis import Synthesizer\n\nsynthesizer = Synthesizer(max_iterations=10)\nresult = synthesizer.synthesize(upir, sketch)\n\nif result.status.value == \"SUCCESS\":\n    print(f\"Generated code:\\n{result.implementation}\")\n</code></pre>"},{"location":"#reinforcement-learning-optimization","title":"Reinforcement Learning Optimization","text":"<p>Optimize architectures from production metrics:</p> <pre><code>from upir.learning.learner import ArchitectureLearner\n\nlearner = ArchitectureLearner(upir)\noptimized_upir = learner.learn(production_metrics, episodes=100)\nprint(f\"Improved cost by {improvement}%\")\n</code></pre>"},{"location":"#pattern-library","title":"Pattern Library","text":"<p>Store and reuse proven patterns:</p> <pre><code>from upir.patterns.library import PatternLibrary\n\nlibrary = PatternLibrary()\nmatches = library.match_architecture(upir, threshold=0.8)\n\nfor pattern, similarity in matches:\n    print(f\"{pattern.name}: {similarity:.2%} match\")\n</code></pre>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Installation Guide - Get started with UPIR</li> <li>Quick Start - Your first UPIR program</li> <li>Core Concepts - Understand key ideas</li> <li>API Reference - Complete API documentation</li> <li>Examples - Real-world use cases</li> </ul>"},{"location":"#attribution","title":"Attribution","text":"<p>This is a clean-room implementation based solely on public sources:</p> <p>Primary Source: Automated Synthesis and Verification of Distributed Systems Using UPIR by Subhadip Mitra, published at TD Commons under CC BY 4.0 license.</p> <p>Author: Subhadip Mitra</p> <p>License: Apache 2.0</p> <p>Project Status: Personal open source project, no affiliations</p>"},{"location":"#getting-help","title":"Getting Help","text":"<ul> <li>Documentation: Browse the User Guide</li> <li>Examples: Check out working examples</li> <li>Issues: Report bugs on GitHub</li> <li>API Reference: Explore the complete API</li> </ul>"},{"location":"#next-steps","title":"Next Steps","text":"<p>Ready to get started?</p> <ol> <li>Install UPIR</li> <li>Follow the Quick Start Guide</li> <li>Explore Core Concepts</li> <li>Try the Streaming Pipeline Example</li> </ol>"},{"location":"architecture_spec/","title":"UPIR Architecture Specification","text":""},{"location":"architecture_spec/#purpose","title":"Purpose","text":"<p>This document specifies the architecture for the clean room implementation of UPIR based on the TD Commons disclosure and standard software engineering practices.</p>"},{"location":"architecture_spec/#design-principles","title":"Design Principles","text":"<ol> <li>Modularity: Clear separation of concerns (verification, synthesis, learning, patterns)</li> <li>Extensibility: Easy to add new system types, patterns, and optimization strategies</li> <li>Type Safety: Comprehensive type hints for all public APIs</li> <li>Testability: Each component testable in isolation</li> <li>Documentation: Self-documenting code with comprehensive docstrings</li> </ol>"},{"location":"architecture_spec/#project-structure","title":"Project Structure","text":"<pre><code>upir/\n\u251c\u2500\u2500 __init__.py                 # Package exports\n\u251c\u2500\u2500 core/                       # Core data model\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 temporal.py            # Temporal logic (TemporalProperty, TemporalOperator)\n\u2502   \u251c\u2500\u2500 specification.py       # FormalSpecification\n\u2502   \u251c\u2500\u2500 evidence.py            # Evidence, ReasoningNode\n\u2502   \u251c\u2500\u2500 architecture.py        # Architecture\n\u2502   \u2514\u2500\u2500 upir.py                # Main UPIR class\n\u251c\u2500\u2500 verification/               # Formal verification\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 solver.py              # ProofCertificate, VerificationResult\n\u2502   \u251c\u2500\u2500 verifier.py            # Verifier, ProofCache\n\u2502   \u2514\u2500\u2500 encoder.py             # SMT encoding utilities\n\u251c\u2500\u2500 synthesis/                  # Code synthesis\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 sketch.py              # Hole, ProgramSketch\n\u2502   \u251c\u2500\u2500 cegis.py               # Synthesizer, CEGIS loop\n\u2502   \u2514\u2500\u2500 templates/             # Code templates\n\u2502       \u251c\u2500\u2500 streaming.py       # Streaming pipeline templates\n\u2502       \u251c\u2500\u2500 batch.py           # Batch processing templates\n\u2502       \u2514\u2500\u2500 api.py             # API service templates\n\u251c\u2500\u2500 learning/                   # RL optimization\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 ppo.py                 # PPO algorithm\n\u2502   \u2514\u2500\u2500 learner.py             # ArchitectureLearner\n\u251c\u2500\u2500 patterns/                   # Pattern extraction\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 extractor.py           # PatternExtractor\n\u2502   \u251c\u2500\u2500 library.py             # PatternLibrary\n\u2502   \u2514\u2500\u2500 pattern.py             # Pattern dataclass\n\u2514\u2500\u2500 utils/                      # Shared utilities\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 logging.py             # Logging configuration\n    \u2514\u2500\u2500 serialization.py       # JSON/serialization helpers\n\ntests/                          # Test suite\n\u251c\u2500\u2500 core/\n\u251c\u2500\u2500 verification/\n\u251c\u2500\u2500 synthesis/\n\u251c\u2500\u2500 learning/\n\u2514\u2500\u2500 patterns/\n\nexamples/                       # Usage examples\n\u251c\u2500\u2500 streaming_pipeline.py\n\u251c\u2500\u2500 batch_processing.py\n\u2514\u2500\u2500 api_service.py\n\ndocs/                           # Documentation\n\u251c\u2500\u2500 user_guide.md\n\u251c\u2500\u2500 api_reference.md\n\u251c\u2500\u2500 architecture.md\n\u2514\u2500\u2500 examples.md\n</code></pre>"},{"location":"architecture_spec/#module-specifications","title":"Module Specifications","text":""},{"location":"architecture_spec/#core-package-upircore","title":"Core Package (upir.core)","text":""},{"location":"architecture_spec/#temporalpy","title":"temporal.py","text":"<pre><code>from enum import Enum\nfrom dataclasses import dataclass\nfrom typing import Optional, Dict, Any\n\nclass TemporalOperator(Enum):\n    \"\"\"Temporal logic operators.\"\"\"\n    ALWAYS = \"always\"        # \u25a1P - must always hold\n    EVENTUALLY = \"eventually\"  # \u25c7P - must hold at some point\n    WITHIN = \"within\"        # \u25c7\u2264tP - must hold within time bound\n    UNTIL = \"until\"          # P U Q - P holds until Q\n    SINCE = \"since\"          # P S Q - P has held since Q\n\n@dataclass\nclass TemporalProperty:\n    \"\"\"A temporal property with formal semantics.\"\"\"\n    operator: TemporalOperator\n    predicate: str\n    time_bound: Optional[float] = None\n    parameters: Dict[str, Any] = field(default_factory=dict)\n\n    def to_smt(self) -&gt; str:\n        \"\"\"Convert to SMT-LIB format.\"\"\"\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Serialize to dictionary.\"\"\"\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -&gt; 'TemporalProperty':\n        \"\"\"Deserialize from dictionary.\"\"\"\n</code></pre>"},{"location":"architecture_spec/#specificationpy","title":"specification.py","text":"<pre><code>@dataclass\nclass FormalSpecification:\n    \"\"\"Formal specification of system requirements.\"\"\"\n    invariants: List[TemporalProperty]\n    properties: List[TemporalProperty]\n    constraints: Dict[str, Dict[str, Any]]\n    assumptions: List[str] = field(default_factory=list)\n\n    def validate(self) -&gt; bool:\n        \"\"\"Validate specification consistency.\"\"\"\n\n    def hash(self) -&gt; str:\n        \"\"\"Generate SHA-256 hash.\"\"\"\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Serialize.\"\"\"\n</code></pre>"},{"location":"architecture_spec/#evidencepy","title":"evidence.py","text":"<pre><code>@dataclass\nclass Evidence:\n    \"\"\"Evidence supporting decisions.\"\"\"\n    source: str\n    type: str  # \"benchmark\", \"test\", \"production\", \"formal_proof\"\n    data: Dict[str, Any]\n    confidence: float  # [0, 1]\n    timestamp: datetime\n\n    def update_confidence(self, observation: bool, weight: float = 0.1):\n        \"\"\"Bayesian confidence update.\"\"\"\n\n@dataclass\nclass ReasoningNode:\n    \"\"\"Node in reasoning DAG.\"\"\"\n    id: str\n    decision: str\n    rationale: str\n    evidence_ids: List[str]\n    parent_ids: List[str]\n    alternatives: List[Dict[str, Any]]\n    confidence: float\n\n    def compute_confidence(self, evidence_map: Dict[str, Evidence]) -&gt; float:\n        \"\"\"Aggregate evidence using geometric mean.\"\"\"\n</code></pre>"},{"location":"architecture_spec/#upirpy","title":"upir.py","text":"<pre><code>@dataclass\nclass UPIR:\n    \"\"\"Main UPIR class - ties everything together.\"\"\"\n    id: str = field(default_factory=uuid4)\n    name: str = \"\"\n    specification: Optional[FormalSpecification] = None\n    architecture: Optional[Architecture] = None\n    evidence: Dict[str, Evidence] = field(default_factory=dict)\n    reasoning: Dict[str, ReasoningNode] = field(default_factory=dict)\n    implementation: Optional[Implementation] = None\n    metadata: Dict[str, Any] = field(default_factory=dict)\n\n    def add_evidence(self, evidence: Evidence) -&gt; str:\n        \"\"\"Add evidence, return ID.\"\"\"\n\n    def add_reasoning(self, node: ReasoningNode) -&gt; str:\n        \"\"\"Add reasoning node.\"\"\"\n\n    def compute_overall_confidence(self) -&gt; float:\n        \"\"\"Compute overall architecture confidence.\"\"\"\n\n    def validate(self) -&gt; bool:\n        \"\"\"Validate UPIR consistency.\"\"\"\n\n    def generate_signature(self) -&gt; str:\n        \"\"\"Cryptographic signature.\"\"\"\n\n    def to_json(self) -&gt; str:\n        \"\"\"Serialize to JSON.\"\"\"\n</code></pre>"},{"location":"architecture_spec/#verification-package-upirverification","title":"Verification Package (upir.verification)","text":""},{"location":"architecture_spec/#verifierpy","title":"verifier.py","text":"<pre><code>class Verifier:\n    \"\"\"Main verification engine using Z3.\"\"\"\n\n    def __init__(self, timeout: int = 30000, enable_cache: bool = True):\n        \"\"\"Initialize verifier.\"\"\"\n\n    def verify_specification(self, upir: UPIR) -&gt; List[VerificationResult]:\n        \"\"\"Verify all properties in specification.\"\"\"\n\n    def verify_property(\n        self,\n        property: TemporalProperty,\n        architecture: Architecture,\n        assumptions: List[str] = None\n    ) -&gt; VerificationResult:\n        \"\"\"Verify single property.\"\"\"\n\n    def verify_incremental(\n        self,\n        upir: UPIR,\n        changed_properties: Set[str] = None\n    ) -&gt; List[VerificationResult]:\n        \"\"\"Incremental verification with caching.\"\"\"\n\nclass ProofCache:\n    \"\"\"Cache for verification proofs.\"\"\"\n\n    def get(self, property, architecture) -&gt; Optional[VerificationResult]:\n        \"\"\"Retrieve cached proof.\"\"\"\n\n    def put(self, property, architecture, result: VerificationResult):\n        \"\"\"Store proof.\"\"\"\n\n    def invalidate(self, architecture: Architecture):\n        \"\"\"Invalidate cached proofs.\"\"\"\n</code></pre>"},{"location":"architecture_spec/#synthesis-package-upirsynthesis","title":"Synthesis Package (upir.synthesis)","text":""},{"location":"architecture_spec/#cegispy","title":"cegis.py","text":"<pre><code>class Synthesizer:\n    \"\"\"CEGIS-based synthesis engine.\"\"\"\n\n    def __init__(self, max_iterations: int = 100, timeout: int = 60000):\n        \"\"\"Initialize synthesizer.\"\"\"\n\n    def synthesize(\n        self,\n        upir: UPIR,\n        examples: List[SynthesisExample] = None\n    ) -&gt; CEGISResult:\n        \"\"\"Main synthesis entry point.\"\"\"\n\n    def generate_sketch(self, spec: FormalSpecification) -&gt; ProgramSketch:\n        \"\"\"Generate program sketch from spec.\"\"\"\n\n    def synthesize_holes(\n        self,\n        sketch: ProgramSketch,\n        spec: FormalSpecification,\n        examples: List[SynthesisExample],\n        counterexamples: List[Dict]\n    ) -&gt; bool:\n        \"\"\"Use SMT to fill holes.\"\"\"\n\n    def verify_synthesis(\n        self,\n        implementation: Implementation,\n        spec: FormalSpecification\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Verify synthesized code.\"\"\"\n</code></pre>"},{"location":"architecture_spec/#learning-package-upirlearning","title":"Learning Package (upir.learning)","text":""},{"location":"architecture_spec/#ppopy","title":"ppo.py","text":"<pre><code>@dataclass\nclass PPOConfig:\n    \"\"\"PPO hyperparameters.\"\"\"\n    learning_rate: float = 3e-4\n    gamma: float = 0.99\n    epsilon: float = 0.2\n    value_coef: float = 0.5\n    entropy_coef: float = 0.01\n\nclass PPO:\n    \"\"\"Proximal Policy Optimization.\"\"\"\n\n    def __init__(self, state_dim: int, action_dim: int, config: PPOConfig):\n        \"\"\"Initialize PPO.\"\"\"\n\n    def select_action(self, state: np.ndarray) -&gt; Tuple[int, float, float]:\n        \"\"\"Select action using policy.\"\"\"\n\n    def update(\n        self,\n        states, actions, old_log_probs, returns, advantages\n    ) -&gt; Dict[str, float]:\n        \"\"\"Update policy and value networks.\"\"\"\n\n    def compute_gae(\n        self,\n        rewards, values, dones, lambda_: float = 0.95\n    ) -&gt; np.ndarray:\n        \"\"\"Compute Generalized Advantage Estimation.\"\"\"\n</code></pre>"},{"location":"architecture_spec/#learnerpy","title":"learner.py","text":"<pre><code>class ArchitectureLearner:\n    \"\"\"Learn to optimize architectures.\"\"\"\n\n    def __init__(self, state_dim: int, action_dim: int):\n        \"\"\"Initialize learner.\"\"\"\n\n    def encode_state(self, upir: UPIR) -&gt; np.ndarray:\n        \"\"\"Encode architecture as state vector.\"\"\"\n\n    def decode_action(self, action: int, upir: UPIR) -&gt; UPIR:\n        \"\"\"Apply action to architecture.\"\"\"\n\n    def compute_reward(\n        self,\n        metrics: Dict[str, float],\n        spec: FormalSpecification\n    ) -&gt; float:\n        \"\"\"Compute reward from metrics.\"\"\"\n\n    def learn_from_metrics(\n        self,\n        upir: UPIR,\n        metrics: Dict[str, float]\n    ) -&gt; UPIR:\n        \"\"\"Main learning entry point.\"\"\"\n</code></pre>"},{"location":"architecture_spec/#patterns-package-upirpatterns","title":"Patterns Package (upir.patterns)","text":""},{"location":"architecture_spec/#extractorpy","title":"extractor.py","text":"<pre><code>class PatternExtractor:\n    \"\"\"Extract patterns via clustering.\"\"\"\n\n    def __init__(self, n_clusters: int = 10):\n        \"\"\"Initialize extractor.\"\"\"\n\n    def extract_features(self, upir: UPIR) -&gt; np.ndarray:\n        \"\"\"Extract feature vector.\"\"\"\n\n    def cluster_architectures(\n        self,\n        upirs: List[UPIR]\n    ) -&gt; Dict[int, List[UPIR]]:\n        \"\"\"Cluster similar architectures.\"\"\"\n\n    def extract_pattern(self, cluster: List[UPIR]) -&gt; Pattern:\n        \"\"\"Extract pattern from cluster.\"\"\"\n\n    def discover_patterns(self, upirs: List[UPIR]) -&gt; List[Pattern]:\n        \"\"\"Main pattern discovery entry point.\"\"\"\n</code></pre>"},{"location":"architecture_spec/#data-flow","title":"Data Flow","text":""},{"location":"architecture_spec/#1-specification-verification","title":"1. Specification \u2192 Verification","text":"<pre><code>User creates FormalSpecification\n    \u2193\nUPIR instance created with spec\n    \u2193\nVerifier.verify_specification(upir)\n    \u2193\nFor each property:\n    Check cache \u2192 Encode \u2192 Z3 solve \u2192 Generate certificate\n    \u2193\nReturn list of VerificationResults\n</code></pre>"},{"location":"architecture_spec/#2-specification-synthesis","title":"2. Specification \u2192 Synthesis","text":"<pre><code>UPIR with verified specification\n    \u2193\nSynthesizer.synthesize(upir)\n    \u2193\nGenerate sketch based on system type\n    \u2193\nCEGIS loop:\n    SMT solve holes \u2192 Instantiate \u2192 Verify \u2192 Refine\n    \u2193\nReturn Implementation with SynthesisProof\n</code></pre>"},{"location":"architecture_spec/#3-metrics-optimization","title":"3. Metrics \u2192 Optimization","text":"<pre><code>UPIR with implementation\n    \u2193\nCollect production metrics\n    \u2193\nArchitectureLearner.learn_from_metrics(upir, metrics)\n    \u2193\nEncode state \u2192 PPO select action \u2192 Decode action\n    \u2193\nVerify properties still hold\n    \u2193\nUpdate policy, return optimized UPIR\n</code></pre>"},{"location":"architecture_spec/#4-upirs-patterns","title":"4. UPIRs \u2192 Patterns","text":"<pre><code>Collection of UPIR instances\n    \u2193\nPatternExtractor.discover_patterns(upirs)\n    \u2193\nExtract features \u2192 Cluster \u2192 Abstract patterns\n    \u2193\nPatternLibrary.add_pattern(pattern)\n    \u2193\nFuture UPIRs can match against library\n</code></pre>"},{"location":"architecture_spec/#error-handling-strategy","title":"Error Handling Strategy","text":""},{"location":"architecture_spec/#verification-errors","title":"Verification Errors","text":"<ul> <li>Z3 timeout: Return VerificationStatus.TIMEOUT</li> <li>Invalid SMT encoding: Return VerificationStatus.ERROR with details</li> <li>Z3 not available: Graceful degradation, return UNKNOWN</li> </ul>"},{"location":"architecture_spec/#synthesis-errors","title":"Synthesis Errors","text":"<ul> <li>Max iterations: Return SynthesisStatus.PARTIAL with best attempt</li> <li>Timeout: Return SynthesisStatus.TIMEOUT</li> <li>Invalid spec: Return SynthesisStatus.INVALID_SPEC</li> <li>Verification failure: Continue CEGIS loop with counterexample</li> </ul>"},{"location":"architecture_spec/#learning-errors","title":"Learning Errors","text":"<ul> <li>Invalid state: Raise ValueError with helpful message</li> <li>Property violation: Reject action, return previous state</li> <li>Metrics unavailable: Use default/heuristic reward</li> </ul>"},{"location":"architecture_spec/#performance-targets","title":"Performance Targets","text":"<p>Based on TD Commons benchmarks:</p> Component Target Measurement Verification (cached) O(1) lookup &lt;1ms for cache hit Verification (uncached) &lt;100ms For typical property Incremental verification 100x+ speedup vs full reverification Synthesis &lt;10ms average Per iteration CEGIS convergence &lt;100 iterations For typical system RL convergence &lt;50 episodes For parameter tuning Pattern matching &lt;100ms Against library of 1000"},{"location":"architecture_spec/#testing-strategy","title":"Testing Strategy","text":""},{"location":"architecture_spec/#unit-tests","title":"Unit Tests","text":"<ul> <li>Test each class in isolation</li> <li>Mock external dependencies (Z3, etc.)</li> <li>Test edge cases and error conditions</li> <li>Aim for &gt;90% coverage</li> </ul>"},{"location":"architecture_spec/#integration-tests","title":"Integration Tests","text":"<ul> <li>Test component interactions</li> <li>End-to-end flows (spec \u2192 verify \u2192 synthesize \u2192 optimize)</li> <li>Test with real Z3 solver</li> </ul>"},{"location":"architecture_spec/#performance-tests","title":"Performance Tests","text":"<ul> <li>Benchmark cache hit rates</li> <li>Measure verification time vs component count</li> <li>Validate incremental verification speedup</li> </ul>"},{"location":"architecture_spec/#property-based-tests","title":"Property-Based Tests","text":"<ul> <li>Use hypothesis for property testing</li> <li>Test invariants (e.g., verification is deterministic)</li> </ul>"},{"location":"architecture_spec/#extensibility-points","title":"Extensibility Points","text":""},{"location":"architecture_spec/#adding-new-system-types","title":"Adding New System Types","text":"<ol> <li>Add template in synthesis/templates/</li> <li>Update Synthesizer._infer_system_type()</li> <li>Implement _generate_X_sketch()</li> </ol>"},{"location":"architecture_spec/#adding-new-patterns","title":"Adding New Patterns","text":"<ol> <li>Define pattern in patterns/library.py</li> <li>Add to built-in patterns list</li> <li>Document pattern applicability</li> </ol>"},{"location":"architecture_spec/#adding-new-optimizers","title":"Adding New Optimizers","text":"<ol> <li>Implement optimizer in learning/</li> <li>Inherit from base optimizer interface</li> <li>Register with ArchitectureLearner</li> </ol>"},{"location":"architecture_spec/#dependencies-management","title":"Dependencies Management","text":""},{"location":"architecture_spec/#core-dependencies-required","title":"Core Dependencies (Required)","text":"<ul> <li>z3-solver: SMT solving</li> <li>numpy: Numerical operations</li> <li>scikit-learn: Clustering</li> </ul>"},{"location":"architecture_spec/#optional-dependencies","title":"Optional Dependencies","text":"<ul> <li>google-cloud-*: GCP integration</li> <li>pytest: Testing</li> <li>mypy: Type checking</li> </ul>"},{"location":"architecture_spec/#dependency-injection","title":"Dependency Injection","text":"<ul> <li>Verifier accepts solver parameter (default Z3)</li> <li>Learner accepts optimizer parameter (default PPO)</li> <li>PatternExtractor accepts clusterer parameter (default KMeans)</li> </ul>"},{"location":"architecture_spec/#configuration","title":"Configuration","text":""},{"location":"architecture_spec/#environment-variables","title":"Environment Variables","text":"<ul> <li><code>UPIR_CACHE_SIZE</code>: Proof cache size (default 1000)</li> <li><code>UPIR_LOG_LEVEL</code>: Logging level (default INFO)</li> <li><code>UPIR_Z3_TIMEOUT</code>: Z3 timeout in ms (default 30000)</li> </ul>"},{"location":"architecture_spec/#configuration-files","title":"Configuration Files","text":"<ul> <li><code>upir.toml</code>: Optional configuration file</li> <li>Follows XDG base directory spec</li> </ul>"},{"location":"architecture_spec/#logging","title":"Logging","text":""},{"location":"architecture_spec/#log-levels","title":"Log Levels","text":"<ul> <li>DEBUG: Detailed SMT formulas, cache operations</li> <li>INFO: Verification results, synthesis progress</li> <li>WARNING: Cache misses, heuristic fallbacks</li> <li>ERROR: Verification failures, invalid specs</li> </ul>"},{"location":"architecture_spec/#log-format","title":"Log Format","text":"<pre><code>[2025-11-15 10:30:45] INFO upir.verification: Proved property 'data_consistency' in 45ms (cached: false)\n</code></pre>"},{"location":"architecture_spec/#security-considerations","title":"Security Considerations","text":""},{"location":"architecture_spec/#input-validation","title":"Input Validation","text":"<ul> <li>Validate all user inputs (specs, metrics)</li> <li>Sanitize predicates before SMT encoding</li> <li>Limit resource consumption (cache size, timeout)</li> </ul>"},{"location":"architecture_spec/#code-generation-safety","title":"Code Generation Safety","text":"<ul> <li>Validate synthesized code syntax</li> <li>Sandbox code execution during verification</li> <li>Include warnings in generated code</li> </ul>"},{"location":"architecture_spec/#cryptographic-integrity","title":"Cryptographic Integrity","text":"<ul> <li>Use SHA-256 for all hashing</li> <li>Sign proof certificates</li> <li>Validate signatures on load</li> </ul>"},{"location":"architecture_spec/#documentation-standards","title":"Documentation Standards","text":""},{"location":"architecture_spec/#docstrings","title":"Docstrings","text":"<ul> <li>Google-style docstrings</li> <li>Include examples for public APIs</li> <li>Document complexity and performance</li> </ul>"},{"location":"architecture_spec/#type-hints","title":"Type Hints","text":"<ul> <li>All public functions fully typed</li> <li>Use typing.* for complex types</li> <li>Enable strict mypy checking</li> </ul>"},{"location":"architecture_spec/#comments","title":"Comments","text":"<ul> <li>Explain \"why\", not \"what\"</li> <li>Document design decisions</li> <li>Reference papers for algorithms</li> </ul>"},{"location":"architecture_spec/#release-strategy","title":"Release Strategy","text":""},{"location":"architecture_spec/#versioning","title":"Versioning","text":"<ul> <li>Follow Semantic Versioning 2.0</li> <li>Major: Breaking API changes</li> <li>Minor: New features, backward compatible</li> <li>Patch: Bug fixes</li> </ul>"},{"location":"architecture_spec/#release-checklist","title":"Release Checklist","text":"<ul> <li> All tests passing</li> <li> Type checking clean</li> <li> Documentation updated</li> <li> CHANGELOG.md updated</li> <li> Version bumped</li> <li> Git tag created</li> <li> PyPI package published</li> </ul> <p>This architecture provides a solid foundation for clean room implementation while maintaining flexibility for future enhancements.</p>"},{"location":"td_commons_summary/","title":"TD Commons Disclosure Summary","text":""},{"location":"td_commons_summary/#citation","title":"Citation","text":"<p>Title: \"Automated Synthesis and Verification of Distributed Systems Using Universal Plan Intermediate Representation (UPIR)\"</p> <p>Author: Subhadip Mitra</p> <p>Published: November 10, 2025</p> <p>URL: https://www.tdcommons.org/dpubs_series/8852/</p> <p>License: Creative Commons Attribution 4.0 (CC BY 4.0)</p>"},{"location":"td_commons_summary/#abstract","title":"Abstract","text":"<p>The disclosure describes UPIR, an intermediate representation that combines formal verification, program synthesis, and machine learning to automatically generate verified code from specifications for distributed systems. It addresses the implementation gap where high-level architectural designs fail to meet requirements during deployment.</p>"},{"location":"td_commons_summary/#key-technical-components","title":"Key Technical Components","text":""},{"location":"td_commons_summary/#1-compositional-verification-engine","title":"1. Compositional Verification Engine","text":"<p>Description: Uses SMT solving with dependency tracking and proof caching to achieve incremental verification.</p> <p>Key Features: - O(1) incremental verification complexity - Proof caching with cryptographic certificates - 274x speedup for 64-component systems vs monolithic approaches</p> <p>Implementation Guidance: - Use Z3 theorem prover for SMT solving - Cache proofs indexed by property and architecture hash - Track dependencies between properties for selective invalidation</p>"},{"location":"td_commons_summary/#2-cegis-based-synthesis-engine","title":"2. CEGIS-based Synthesis Engine","text":"<p>Description: Applies Counterexample-Guided Inductive Synthesis to distributed systems, generating program sketches and filling holes through SMT solving.</p> <p>Key Features: - Average synthesis time: 1.97ms - Success rates: 43-75% for different system types - Generates code for streaming, batch, and API systems</p> <p>Implementation Guidance: - Create templates for common architectures (streaming, batch, API) - Identify parameters as holes (window size, parallelism, timeouts) - Use counterexamples to refine synthesis</p>"},{"location":"td_commons_summary/#3-constrained-rl-optimizer","title":"3. Constrained RL Optimizer","text":"<p>Description: Uses Proximal Policy Optimization (PPO) to improve system parameters while maintaining formal invariants.</p> <p>Key Features: - Converges within 45 episodes - 60.1% latency reduction - 194.5% throughput improvement - Maintains formal properties during optimization</p> <p>Implementation Guidance: - Encode architecture as state (component count, connections, etc.) - Define actions as parameter modifications - Reward based on constraint satisfaction and performance - Verify properties before accepting changes</p>"},{"location":"td_commons_summary/#4-pattern-extraction-via-clustering","title":"4. Pattern Extraction via Clustering","text":"<p>Description: Discovers reusable architectural patterns through clustering similar system designs.</p> <p>Key Features: - 89.9% pattern reuse potential identified - Similarity-based pattern matching - Success rate tracking for patterns</p> <p>Implementation Guidance: - Extract features from architectures (components, connections, patterns) - Use K-means or DBSCAN clustering - Abstract clusters into parameterized patterns</p>"},{"location":"td_commons_summary/#performance-benchmarks","title":"Performance Benchmarks","text":"<p>Per the disclosure:</p> Metric Value Verification speedup (64 components) 274x Synthesis time (average) 1.97ms Synthesis success rate 43-75% RL convergence 45 episodes Latency reduction 60.1% Throughput improvement 194.5% Pattern reuse potential 89.9% Test iterations 100"},{"location":"td_commons_summary/#core-data-model","title":"Core Data Model","text":"<p>From the disclosure:</p>"},{"location":"td_commons_summary/#formalspecification","title":"FormalSpecification","text":"<ul> <li>Temporal properties (ALWAYS, EVENTUALLY, WITHIN)</li> <li>Resource constraints (latency, cost, throughput)</li> <li>Environmental assumptions</li> </ul>"},{"location":"td_commons_summary/#architecture","title":"Architecture","text":"<ul> <li>Components with properties</li> <li>Connections between components</li> <li>Deployment configuration</li> <li>Applied patterns</li> </ul>"},{"location":"td_commons_summary/#evidence","title":"Evidence","text":"<ul> <li>Source and type (benchmark, test, production)</li> <li>Confidence tracking (Bayesian updates)</li> <li>Timestamp and metadata</li> </ul>"},{"location":"td_commons_summary/#reasoningnode","title":"ReasoningNode","text":"<ul> <li>Decision and rationale</li> <li>Supporting evidence references</li> <li>Alternative options considered</li> <li>Confidence aggregation</li> </ul>"},{"location":"td_commons_summary/#implementation","title":"Implementation","text":"<ul> <li>Generated code</li> <li>Synthesis proof</li> <li>Performance profile</li> </ul>"},{"location":"td_commons_summary/#key-algorithms","title":"Key Algorithms","text":""},{"location":"td_commons_summary/#incremental-verification","title":"Incremental Verification","text":"<pre><code>1. Compute hash of property and architecture\n2. Check proof cache\n3. If cached: return result (O(1))\n4. Else:\n   a. Encode architecture as SMT constraints\n   b. Encode property as temporal formula\n   c. Use Z3 to prove or find counterexample\n   d. Generate proof certificate\n   e. Cache result\n</code></pre>"},{"location":"td_commons_summary/#cegis-synthesis","title":"CEGIS Synthesis","text":"<pre><code>1. Generate program sketch with holes from spec\n2. Loop (max 100 iterations):\n   a. Use SMT to find values for holes\n   b. Instantiate program\n   c. Verify against specification\n   d. If verified: SUCCESS, return with proof\n   e. Else: Add counterexample, continue\n3. If max iterations: return PARTIAL\n</code></pre>"},{"location":"td_commons_summary/#rl-optimization","title":"RL Optimization","text":"<pre><code>1. Encode architecture as state vector\n2. Select action using PPO policy\n3. Decode action to modify architecture\n4. Compute reward from metrics vs constraints\n5. Verify formal properties still hold\n6. If valid: Update policy, accept change\n7. Else: Reject change, penalize action\n</code></pre>"},{"location":"td_commons_summary/#novel-contributions","title":"Novel Contributions","text":"<p>According to the disclosure, the key innovations are:</p> <ol> <li>Combination: First system to combine formal verification + synthesis + RL for distributed systems</li> <li>Incremental verification: O(1) complexity through proof caching</li> <li>CEGIS for distributed systems: Applying synthesis to cloud architectures</li> <li>Constrained RL: Optimization that maintains formal invariants</li> <li>Pattern extraction: Automated discovery of reusable patterns</li> </ol>"},{"location":"td_commons_summary/#implementation-priorities","title":"Implementation Priorities","text":"<p>Based on the disclosure, implement in this order:</p> <ol> <li>Week 1: Core data model (FormalSpecification, Evidence, UPIR)</li> <li>Week 2: Verification engine (Z3 integration, proof caching)</li> <li>Week 3: Synthesis engine (CEGIS, sketch generation)</li> <li>Week 4: Learning system (PPO, architecture optimization)</li> <li>Week 5: Pattern extraction and examples</li> </ol>"},{"location":"td_commons_summary/#usage-scenarios","title":"Usage Scenarios","text":"<p>The disclosure describes these use cases:</p>"},{"location":"td_commons_summary/#streaming-etl-pipeline","title":"Streaming ETL Pipeline","text":"<ul> <li>Specify: latency &lt;100ms, exactly-once, data consistency</li> <li>Synthesize: Apache Beam pipeline with optimal window size</li> <li>Verify: Temporal properties hold</li> <li>Optimize: Learn parallelism and buffer sizes from production</li> </ul>"},{"location":"td_commons_summary/#batch-processing","title":"Batch Processing","text":"<ul> <li>Specify: completion time, cost constraints</li> <li>Synthesize: MapReduce job with optimal batch size</li> <li>Verify: Resource bounds satisfied</li> <li>Optimize: Learn chunk size from execution metrics</li> </ul>"},{"location":"td_commons_summary/#api-service","title":"API Service","text":"<ul> <li>Specify: response time, availability, throughput</li> <li>Synthesize: Flask/FastAPI service with timeouts</li> <li>Verify: Performance requirements met</li> <li>Optimize: Learn connection pool size, cache TTLs</li> </ul>"},{"location":"td_commons_summary/#references-in-disclosure","title":"References in Disclosure","text":"<p>The TD Commons disclosure cites these foundational works:</p> <ul> <li>CEGIS: Solar-Lezama et al. \"Program Synthesis by Sketching\" (2008)</li> <li>PPO: Schulman et al. \"Proximal Policy Optimization Algorithms\" (2017)</li> <li>Temporal Logic: Pnueli \"The Temporal Logic of Programs\" (1977)</li> <li>SMT: Z3 Theorem Prover, De Moura &amp; Bj\u00f8rner</li> </ul>"},{"location":"td_commons_summary/#implementation-notes","title":"Implementation Notes","text":""},{"location":"td_commons_summary/#what-the-disclosure-specifies","title":"What the Disclosure Specifies:","text":"<ul> <li>High-level architecture and algorithms</li> <li>Performance benchmarks to achieve</li> <li>Core data structures</li> <li>Workflow and integration</li> </ul>"},{"location":"td_commons_summary/#what-you-must-design","title":"What You Must Design:","text":"<ul> <li>Exact class hierarchies</li> <li>Method signatures and APIs</li> <li>Error handling strategies</li> <li>Testing approaches</li> <li>Code organization</li> </ul> <p>This gives you freedom to make independent implementation choices while following the disclosed concepts.</p>"},{"location":"td_commons_summary/#legal-status","title":"Legal Status","text":"<ul> <li>Public disclosure under CC BY 4.0</li> <li>Not patented (defensive publication)</li> <li>Anyone can implement with attribution</li> <li>No patent claims can be filed on these concepts</li> </ul> <p>This is your primary reference for the clean room implementation.</p>"},{"location":"about/attribution/","title":"Attribution","text":"<p>UPIR is a clean-room implementation based solely on public sources.</p>"},{"location":"about/attribution/#primary-source","title":"Primary Source","text":"<p>Automated Synthesis and Verification of Distributed Systems Using UPIR</p> <p>Published by Subhadip Mitra at TD Commons under CC BY 4.0 license on November 10, 2025.</p>"},{"location":"about/attribution/#additional-references","title":"Additional References","text":"<p>See SOURCES.md for complete list of references.</p>"},{"location":"about/attribution/#author","title":"Author","text":"<p>Subhadip Mitra</p> <p>Personal open source project, no affiliations.</p>"},{"location":"about/license/","title":"License","text":"<p>UPIR is licensed under the Apache License 2.0.</p>"},{"location":"about/license/#apache-20","title":"Apache 2.0","text":"<pre><code>Copyright 2025 Subhadip Mitra\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n</code></pre>"},{"location":"about/license/#full-license","title":"Full License","text":"<p>See LICENSE for the complete license text.</p>"},{"location":"about/references/","title":"References","text":"<p>Academic papers and resources used in UPIR.</p>"},{"location":"about/references/#primary-source","title":"Primary Source","text":"<ul> <li>TD Commons Disclosure: Automated Synthesis and Verification of Distributed Systems Using UPIR</li> </ul>"},{"location":"about/references/#cegis","title":"CEGIS","text":"<ul> <li>Armando Solar-Lezama et al., \"Program Synthesis by Sketching\", 2008</li> </ul>"},{"location":"about/references/#ppo","title":"PPO","text":"<ul> <li>Schulman et al., \"Proximal Policy Optimization Algorithms\", 2017</li> </ul>"},{"location":"about/references/#z3","title":"Z3","text":"<ul> <li>Z3 SMT Solver</li> </ul>"},{"location":"about/references/#see-also","title":"See Also","text":"<ul> <li>SOURCES.md - Complete list of references</li> </ul>"},{"location":"api/core/architecture/","title":"Architecture","text":"<p>System architecture representation with components and connections.</p>"},{"location":"api/core/architecture/#overview","title":"Overview","text":"<p>The <code>Architecture</code> class represents the structure of a distributed system:</p> <ul> <li>Components: Individual services, databases, queues, etc.</li> <li>Connections: Network links between components</li> <li>Metrics: Total latency, cost, and other aggregate metrics</li> </ul>"},{"location":"api/core/architecture/#class-documentation","title":"Class Documentation","text":""},{"location":"api/core/architecture/#upir.core.architecture.Architecture","title":"<code>upir.core.architecture.Architecture</code>  <code>dataclass</code>","text":"<p>A simple representation of a distributed system architecture.</p> <p>This is a placeholder structure that will be expanded in future versions with richer modeling capabilities, component templates, and validation.</p> <p>Based on TD Commons disclosure, architectures consist of components (services, databases, caches), connections (network links, APIs), deployment configuration, and applied patterns.</p> <p>Attributes:</p> Name Type Description <code>components</code> <code>List[Dict[str, Any]]</code> <p>List of architectural components (services, DBs, etc.)        Each component is a dict with keys like: name, type, config</p> <code>connections</code> <code>List[Dict[str, Any]]</code> <p>List of connections between components         Each connection is a dict with: from, to, protocol, etc.</p> <code>deployment</code> <code>Dict[str, Any]</code> <p>Deployment configuration (regions, resources, scaling)</p> <code>patterns</code> <code>List[str]</code> <p>List of architectural patterns applied (e.g., \"CQRS\", \"event-sourcing\")</p> Example <p>arch = Architecture( ...     components=[ ...         {\"name\": \"api-service\", \"type\": \"service\", \"replicas\": 3}, ...         {\"name\": \"postgres\", \"type\": \"database\", \"size\": \"large\"} ...     ], ...     connections=[ ...         {\"from\": \"api-service\", \"to\": \"postgres\", \"protocol\": \"TCP\"} ...     ], ...     deployment={ ...         \"regions\": [\"us-west-2\", \"us-east-1\"], ...         \"strategy\": \"blue-green\" ...     }, ...     patterns=[\"microservices\", \"CQRS\"] ... )</p> <p>References: - TD Commons: Architecture representation</p> Source code in <code>upir/core/architecture.py</code> <pre><code>@dataclass\nclass Architecture:\n    \"\"\"\n    A simple representation of a distributed system architecture.\n\n    This is a placeholder structure that will be expanded in future versions\n    with richer modeling capabilities, component templates, and validation.\n\n    Based on TD Commons disclosure, architectures consist of components\n    (services, databases, caches), connections (network links, APIs),\n    deployment configuration, and applied patterns.\n\n    Attributes:\n        components: List of architectural components (services, DBs, etc.)\n                   Each component is a dict with keys like: name, type, config\n        connections: List of connections between components\n                    Each connection is a dict with: from, to, protocol, etc.\n        deployment: Deployment configuration (regions, resources, scaling)\n        patterns: List of architectural patterns applied (e.g., \"CQRS\", \"event-sourcing\")\n\n    Example:\n        &gt;&gt;&gt; arch = Architecture(\n        ...     components=[\n        ...         {\"name\": \"api-service\", \"type\": \"service\", \"replicas\": 3},\n        ...         {\"name\": \"postgres\", \"type\": \"database\", \"size\": \"large\"}\n        ...     ],\n        ...     connections=[\n        ...         {\"from\": \"api-service\", \"to\": \"postgres\", \"protocol\": \"TCP\"}\n        ...     ],\n        ...     deployment={\n        ...         \"regions\": [\"us-west-2\", \"us-east-1\"],\n        ...         \"strategy\": \"blue-green\"\n        ...     },\n        ...     patterns=[\"microservices\", \"CQRS\"]\n        ... )\n\n    References:\n    - TD Commons: Architecture representation\n    \"\"\"\n    components: List[Dict[str, Any]] = field(default_factory=list)\n    connections: List[Dict[str, Any]] = field(default_factory=list)\n    deployment: Dict[str, Any] = field(default_factory=dict)\n    patterns: List[str] = field(default_factory=list)\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Serialize architecture to JSON-compatible dictionary.\n\n        Returns:\n            Dictionary with all architecture fields\n\n        Example:\n            &gt;&gt;&gt; arch = Architecture(\n            ...     components=[{\"name\": \"service\"}],\n            ...     patterns=[\"microservices\"]\n            ... )\n            &gt;&gt;&gt; d = arch.to_dict()\n            &gt;&gt;&gt; d[\"patterns\"]\n            ['microservices']\n        \"\"\"\n        return {\n            \"components\": [comp.copy() for comp in self.components],\n            \"connections\": [conn.copy() for conn in self.connections],\n            \"deployment\": self.deployment.copy(),\n            \"patterns\": self.patterns.copy()\n        }\n\n    def to_json(self) -&gt; str:\n        \"\"\"\n        Serialize architecture to JSON string.\n\n        Returns:\n            JSON string representation\n\n        Example:\n            &gt;&gt;&gt; arch = Architecture(components=[{\"id\": \"c1\"}])\n            &gt;&gt;&gt; json_str = arch.to_json()\n            &gt;&gt;&gt; isinstance(json_str, str)\n            True\n        \"\"\"\n        return json.dumps(self.to_dict(), indent=2)\n\n    def hash(self) -&gt; str:\n        \"\"\"\n        Generate SHA-256 hash of this architecture.\n\n        Uses deterministic JSON serialization to ensure same architecture\n        always produces the same hash, enabling caching and integrity checks.\n\n        Returns:\n            Hexadecimal SHA-256 hash string\n\n        Example:\n            &gt;&gt;&gt; arch = Architecture(components=[{\"id\": \"c1\"}])\n            &gt;&gt;&gt; hash1 = arch.hash()\n            &gt;&gt;&gt; hash2 = arch.hash()\n            &gt;&gt;&gt; hash1 == hash2  # Deterministic\n            True\n\n        References:\n        - SHA-256: Industry standard cryptographic hash\n        - Python hashlib: https://docs.python.org/3/library/hashlib.html\n        \"\"\"\n        arch_dict = self.to_dict()\n        json_str = json.dumps(arch_dict, sort_keys=True)\n        hash_obj = hashlib.sha256(json_str.encode('utf-8'))\n        return hash_obj.hexdigest()\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -&gt; \"Architecture\":\n        \"\"\"\n        Deserialize architecture from dictionary.\n\n        Args:\n            data: Dictionary containing architecture fields\n\n        Returns:\n            Architecture instance\n\n        Example:\n            &gt;&gt;&gt; data = {\n            ...     \"components\": [{\"name\": \"service\"}],\n            ...     \"connections\": [],\n            ...     \"deployment\": {},\n            ...     \"patterns\": [\"microservices\"]\n            ... }\n            &gt;&gt;&gt; arch = Architecture.from_dict(data)\n            &gt;&gt;&gt; arch.patterns\n            ['microservices']\n        \"\"\"\n        return cls(\n            components=data.get(\"components\", []),\n            connections=data.get(\"connections\", []),\n            deployment=data.get(\"deployment\", {}),\n            patterns=data.get(\"patterns\", [])\n        )\n\n    @property\n    def total_latency_ms(self) -&gt; float:\n        \"\"\"\n        Total latency across all components and connections.\n\n        Sums latency_ms from all components and connections.\n\n        Returns:\n            Total latency in milliseconds\n\n        Example:\n            &gt;&gt;&gt; arch = Architecture(\n            ...     components=[\n            ...         {\"id\": \"api\", \"latency_ms\": 10.0},\n            ...         {\"id\": \"db\", \"latency_ms\": 50.0}\n            ...     ],\n            ...     connections=[\n            ...         {\"from\": \"api\", \"to\": \"db\", \"latency_ms\": 5.0}\n            ...     ]\n            ... )\n            &gt;&gt;&gt; arch.total_latency_ms\n            65.0\n        \"\"\"\n        component_latency = sum(c.get(\"latency_ms\", 0.0) for c in self.components)\n        connection_latency = sum(c.get(\"latency_ms\", 0.0) for c in self.connections)\n        return component_latency + connection_latency\n\n    @property\n    def total_cost(self) -&gt; float:\n        \"\"\"\n        Total monthly cost of all components.\n\n        Sums cost_monthly from all components.\n\n        Returns:\n            Total monthly cost in USD\n\n        Example:\n            &gt;&gt;&gt; arch = Architecture(\n            ...     components=[\n            ...         {\"id\": \"api\", \"cost_monthly\": 300.0},\n            ...         {\"id\": \"db\", \"cost_monthly\": 500.0}\n            ...     ]\n            ... )\n            &gt;&gt;&gt; arch.total_cost\n            800.0\n        \"\"\"\n        return sum(c.get(\"cost_monthly\", 0.0) for c in self.components)\n\n    def __str__(self) -&gt; str:\n        \"\"\"Human-readable string representation.\"\"\"\n        parts = []\n        if self.components:\n            parts.append(f\"{len(self.components)} component(s)\")\n        if self.connections:\n            parts.append(f\"{len(self.connections)} connection(s)\")\n        if self.patterns:\n            parts.append(f\"{len(self.patterns)} pattern(s)\")\n\n        if not parts:\n            return \"Architecture(empty)\"\n\n        return f\"Architecture({', '.join(parts)})\"\n</code></pre>"},{"location":"api/core/architecture/#upir.core.architecture.Architecture-attributes","title":"Attributes","text":""},{"location":"api/core/architecture/#upir.core.architecture.Architecture.total_latency_ms","title":"<code>total_latency_ms</code>  <code>property</code>","text":"<p>Total latency across all components and connections.</p> <p>Sums latency_ms from all components and connections.</p> <p>Returns:</p> Type Description <code>float</code> <p>Total latency in milliseconds</p> Example <p>arch = Architecture( ...     components=[ ...         {\"id\": \"api\", \"latency_ms\": 10.0}, ...         {\"id\": \"db\", \"latency_ms\": 50.0} ...     ], ...     connections=[ ...         {\"from\": \"api\", \"to\": \"db\", \"latency_ms\": 5.0} ...     ] ... ) arch.total_latency_ms 65.0</p>"},{"location":"api/core/architecture/#upir.core.architecture.Architecture.total_cost","title":"<code>total_cost</code>  <code>property</code>","text":"<p>Total monthly cost of all components.</p> <p>Sums cost_monthly from all components.</p> <p>Returns:</p> Type Description <code>float</code> <p>Total monthly cost in USD</p> Example <p>arch = Architecture( ...     components=[ ...         {\"id\": \"api\", \"cost_monthly\": 300.0}, ...         {\"id\": \"db\", \"cost_monthly\": 500.0} ...     ] ... ) arch.total_cost 800.0</p>"},{"location":"api/core/architecture/#upir.core.architecture.Architecture-functions","title":"Functions","text":""},{"location":"api/core/architecture/#upir.core.architecture.Architecture.to_dict","title":"<code>to_dict()</code>","text":"<p>Serialize architecture to JSON-compatible dictionary.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with all architecture fields</p> Example <p>arch = Architecture( ...     components=[{\"name\": \"service\"}], ...     patterns=[\"microservices\"] ... ) d = arch.to_dict() d[\"patterns\"]['microservices']</p> Source code in <code>upir/core/architecture.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Serialize architecture to JSON-compatible dictionary.\n\n    Returns:\n        Dictionary with all architecture fields\n\n    Example:\n        &gt;&gt;&gt; arch = Architecture(\n        ...     components=[{\"name\": \"service\"}],\n        ...     patterns=[\"microservices\"]\n        ... )\n        &gt;&gt;&gt; d = arch.to_dict()\n        &gt;&gt;&gt; d[\"patterns\"]\n        ['microservices']\n    \"\"\"\n    return {\n        \"components\": [comp.copy() for comp in self.components],\n        \"connections\": [conn.copy() for conn in self.connections],\n        \"deployment\": self.deployment.copy(),\n        \"patterns\": self.patterns.copy()\n    }\n</code></pre>"},{"location":"api/core/architecture/#upir.core.architecture.Architecture.to_json","title":"<code>to_json()</code>","text":"<p>Serialize architecture to JSON string.</p> <p>Returns:</p> Type Description <code>str</code> <p>JSON string representation</p> Example <p>arch = Architecture(components=[{\"id\": \"c1\"}]) json_str = arch.to_json() isinstance(json_str, str) True</p> Source code in <code>upir/core/architecture.py</code> <pre><code>def to_json(self) -&gt; str:\n    \"\"\"\n    Serialize architecture to JSON string.\n\n    Returns:\n        JSON string representation\n\n    Example:\n        &gt;&gt;&gt; arch = Architecture(components=[{\"id\": \"c1\"}])\n        &gt;&gt;&gt; json_str = arch.to_json()\n        &gt;&gt;&gt; isinstance(json_str, str)\n        True\n    \"\"\"\n    return json.dumps(self.to_dict(), indent=2)\n</code></pre>"},{"location":"api/core/architecture/#upir.core.architecture.Architecture.hash","title":"<code>hash()</code>","text":"<p>Generate SHA-256 hash of this architecture.</p> <p>Uses deterministic JSON serialization to ensure same architecture always produces the same hash, enabling caching and integrity checks.</p> <p>Returns:</p> Type Description <code>str</code> <p>Hexadecimal SHA-256 hash string</p> Example <p>arch = Architecture(components=[{\"id\": \"c1\"}]) hash1 = arch.hash() hash2 = arch.hash() hash1 == hash2  # Deterministic True</p> <p>References: - SHA-256: Industry standard cryptographic hash - Python hashlib: https://docs.python.org/3/library/hashlib.html</p> Source code in <code>upir/core/architecture.py</code> <pre><code>def hash(self) -&gt; str:\n    \"\"\"\n    Generate SHA-256 hash of this architecture.\n\n    Uses deterministic JSON serialization to ensure same architecture\n    always produces the same hash, enabling caching and integrity checks.\n\n    Returns:\n        Hexadecimal SHA-256 hash string\n\n    Example:\n        &gt;&gt;&gt; arch = Architecture(components=[{\"id\": \"c1\"}])\n        &gt;&gt;&gt; hash1 = arch.hash()\n        &gt;&gt;&gt; hash2 = arch.hash()\n        &gt;&gt;&gt; hash1 == hash2  # Deterministic\n        True\n\n    References:\n    - SHA-256: Industry standard cryptographic hash\n    - Python hashlib: https://docs.python.org/3/library/hashlib.html\n    \"\"\"\n    arch_dict = self.to_dict()\n    json_str = json.dumps(arch_dict, sort_keys=True)\n    hash_obj = hashlib.sha256(json_str.encode('utf-8'))\n    return hash_obj.hexdigest()\n</code></pre>"},{"location":"api/core/architecture/#upir.core.architecture.Architecture.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Deserialize architecture from dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>Dictionary containing architecture fields</p> required <p>Returns:</p> Type Description <code>Architecture</code> <p>Architecture instance</p> Example <p>data = { ...     \"components\": [{\"name\": \"service\"}], ...     \"connections\": [], ...     \"deployment\": {}, ...     \"patterns\": [\"microservices\"] ... } arch = Architecture.from_dict(data) arch.patterns ['microservices']</p> Source code in <code>upir/core/architecture.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, Any]) -&gt; \"Architecture\":\n    \"\"\"\n    Deserialize architecture from dictionary.\n\n    Args:\n        data: Dictionary containing architecture fields\n\n    Returns:\n        Architecture instance\n\n    Example:\n        &gt;&gt;&gt; data = {\n        ...     \"components\": [{\"name\": \"service\"}],\n        ...     \"connections\": [],\n        ...     \"deployment\": {},\n        ...     \"patterns\": [\"microservices\"]\n        ... }\n        &gt;&gt;&gt; arch = Architecture.from_dict(data)\n        &gt;&gt;&gt; arch.patterns\n        ['microservices']\n    \"\"\"\n    return cls(\n        components=data.get(\"components\", []),\n        connections=data.get(\"connections\", []),\n        deployment=data.get(\"deployment\", {}),\n        patterns=data.get(\"patterns\", [])\n    )\n</code></pre>"},{"location":"api/core/architecture/#upir.core.architecture.Architecture.__str__","title":"<code>__str__()</code>","text":"<p>Human-readable string representation.</p> Source code in <code>upir/core/architecture.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Human-readable string representation.\"\"\"\n    parts = []\n    if self.components:\n        parts.append(f\"{len(self.components)} component(s)\")\n    if self.connections:\n        parts.append(f\"{len(self.connections)} connection(s)\")\n    if self.patterns:\n        parts.append(f\"{len(self.patterns)} pattern(s)\")\n\n    if not parts:\n        return \"Architecture(empty)\"\n\n    return f\"Architecture({', '.join(parts)})\"\n</code></pre>"},{"location":"api/core/architecture/#usage-example","title":"Usage Example","text":"<pre><code>from upir.core.architecture import Architecture\n\n# Define components\ncomponents = [\n    {\n        \"id\": \"api_gateway\",\n        \"name\": \"API Gateway\",\n        \"type\": \"api_gateway\",\n        \"latency_ms\": 10.0,\n        \"cost_monthly\": 300.0,\n        \"config\": {\n            \"max_connections\": 10000\n        }\n    },\n    {\n        \"id\": \"database\",\n        \"name\": \"PostgreSQL Database\",\n        \"type\": \"database\",\n        \"latency_ms\": 50.0,\n        \"cost_monthly\": 500.0,\n        \"config\": {\n            \"instance_type\": \"db.m5.large\"\n        }\n    }\n]\n\n# Define connections\nconnections = [\n    {\n        \"from\": \"api_gateway\",\n        \"to\": \"database\",\n        \"latency_ms\": 5.0\n    }\n]\n\n# Create architecture\narch = Architecture(\n    components=components,\n    connections=connections\n)\n\n# Access metrics\nprint(f\"Total latency: {arch.total_latency_ms}ms\")\nprint(f\"Total cost: ${arch.total_cost}/month\")\nprint(f\"Components: {len(arch.components)}\")\n\n# Serialize\narch_json = arch.to_json()\n</code></pre>"},{"location":"api/core/architecture/#component-schema","title":"Component Schema","text":"<p>Each component must have:</p> <ul> <li><code>id</code> (str): Unique identifier</li> <li><code>type</code> (str): Component type (e.g., \"database\", \"api_gateway\")</li> <li><code>latency_ms</code> (float, optional): Component latency in milliseconds</li> <li><code>cost_monthly</code> (float, optional): Monthly cost in USD</li> <li><code>name</code> (str, optional): Human-readable name</li> <li><code>config</code> (dict, optional): Component-specific configuration</li> </ul>"},{"location":"api/core/architecture/#connection-schema","title":"Connection Schema","text":"<p>Each connection must have:</p> <ul> <li><code>from</code> (str): Source component ID</li> <li><code>to</code> (str): Destination component ID</li> <li><code>latency_ms</code> (float, optional): Network latency in milliseconds</li> </ul>"},{"location":"api/core/architecture/#see-also","title":"See Also","text":"<ul> <li>UPIR - Main UPIR class</li> <li>Specification - Formal specifications</li> </ul>"},{"location":"api/core/evidence/","title":"Evidence","text":"<p>Evidence-based reasoning for architecture decisions.</p>"},{"location":"api/core/evidence/#overview","title":"Overview","text":"<p>The evidence system supports confidence-weighted reasoning about architectural properties.</p>"},{"location":"api/core/evidence/#class-documentation","title":"Class Documentation","text":""},{"location":"api/core/evidence/#upir.core.evidence.Evidence","title":"<code>upir.core.evidence.Evidence</code>  <code>dataclass</code>","text":"<p>A piece of evidence supporting or refuting an architectural decision.</p> <p>Evidence can come from various sources (benchmarks, tests, production data, formal proofs) and carries a Bayesian confidence level that can be updated as new observations arrive.</p> <p>Based on the TD Commons disclosure, UPIR tracks evidence with confidence levels that propagate through reasoning graphs. The Bayesian update implements a simple beta-binomial conjugate prior approach.</p> <p>Attributes:</p> Name Type Description <code>source</code> <code>str</code> <p>Where the evidence came from (e.g., \"load_test_2024-01\",     \"formal_verification\", \"production_metrics\")</p> <code>type</code> <code>str</code> <p>Type of evidence - one of: benchmark, test, production, formal_proof</p> <code>data</code> <code>Dict[str, Any]</code> <p>The actual evidence data (metrics, test results, proof artifacts)</p> <code>confidence</code> <code>float</code> <p>Bayesian confidence level in [0, 1]</p> <code>timestamp</code> <code>datetime</code> <p>When the evidence was collected (UTC)</p> Example <p>evidence = Evidence( ...     source=\"load_test_2024-01\", ...     type=\"benchmark\", ...     data={\"latency_p99\": 95, \"throughput\": 10000}, ...     confidence=0.8, ...     timestamp=datetime.utcnow() ... ) evidence.update_confidence(new_observation=True) evidence.confidence &gt; 0.8  # Confidence increased True</p> <p>References: - TD Commons: Evidence tracking structure - Bayesian inference: Beta-binomial conjugate prior</p> Source code in <code>upir/core/evidence.py</code> <pre><code>@dataclass\nclass Evidence:\n    \"\"\"\n    A piece of evidence supporting or refuting an architectural decision.\n\n    Evidence can come from various sources (benchmarks, tests, production data,\n    formal proofs) and carries a Bayesian confidence level that can be updated\n    as new observations arrive.\n\n    Based on the TD Commons disclosure, UPIR tracks evidence with confidence\n    levels that propagate through reasoning graphs. The Bayesian update\n    implements a simple beta-binomial conjugate prior approach.\n\n    Attributes:\n        source: Where the evidence came from (e.g., \"load_test_2024-01\",\n                \"formal_verification\", \"production_metrics\")\n        type: Type of evidence - one of: benchmark, test, production, formal_proof\n        data: The actual evidence data (metrics, test results, proof artifacts)\n        confidence: Bayesian confidence level in [0, 1]\n        timestamp: When the evidence was collected (UTC)\n\n    Example:\n        &gt;&gt;&gt; evidence = Evidence(\n        ...     source=\"load_test_2024-01\",\n        ...     type=\"benchmark\",\n        ...     data={\"latency_p99\": 95, \"throughput\": 10000},\n        ...     confidence=0.8,\n        ...     timestamp=datetime.utcnow()\n        ... )\n        &gt;&gt;&gt; evidence.update_confidence(new_observation=True)\n        &gt;&gt;&gt; evidence.confidence &gt; 0.8  # Confidence increased\n        True\n\n    References:\n    - TD Commons: Evidence tracking structure\n    - Bayesian inference: Beta-binomial conjugate prior\n    \"\"\"\n    source: str\n    type: str\n    data: Dict[str, Any]\n    confidence: float\n    timestamp: datetime = field(default_factory=datetime.utcnow)\n\n    def __post_init__(self):\n        \"\"\"Validate evidence fields.\"\"\"\n        valid_types = {\"benchmark\", \"test\", \"production\", \"formal_proof\"}\n        if self.type not in valid_types:\n            raise ValueError(\n                f\"Invalid evidence type '{self.type}'. \"\n                f\"Must be one of: {valid_types}\"\n            )\n\n        if not 0 &lt;= self.confidence &lt;= 1:\n            raise ValueError(\n                f\"Confidence must be in [0, 1], got {self.confidence}\"\n            )\n\n        if not self.source:\n            raise ValueError(\"Source cannot be empty\")\n\n    def update_confidence(\n        self,\n        new_observation: bool,\n        prior_weight: float = 0.1\n    ) -&gt; None:\n        \"\"\"\n        Update confidence using Bayesian update based on new observation.\n\n        This implements a simple Bayesian update using a beta-binomial conjugate\n        prior. The prior_weight controls how much the new observation affects\n        the current confidence.\n\n        Update rules:\n        - Positive observation: confidence += prior_weight * (1 - confidence)\n        - Negative observation: confidence *= (1 - prior_weight)\n\n        These rules ensure:\n        1. Confidence stays in [0, 1]\n        2. Positive observations increase confidence (asymptotically to 1)\n        3. Negative observations decrease confidence (multiplicatively)\n        4. Higher current confidence is harder to change (conservative)\n\n        Args:\n            new_observation: True if observation supports the evidence,\n                           False if it contradicts it\n            prior_weight: Weight of the prior in [0, 1]. Higher values mean\n                         new observations have more impact. Default 0.1.\n\n        Raises:\n            ValueError: If prior_weight not in [0, 1]\n\n        Example:\n            &gt;&gt;&gt; evidence = Evidence(\n            ...     source=\"test\",\n            ...     type=\"benchmark\",\n            ...     data={},\n            ...     confidence=0.5\n            ... )\n            &gt;&gt;&gt; evidence.update_confidence(new_observation=True)\n            &gt;&gt;&gt; evidence.confidence\n            0.55\n            &gt;&gt;&gt; evidence.update_confidence(new_observation=False)\n            &gt;&gt;&gt; evidence.confidence\n            0.495\n\n        References:\n        - Beta-binomial conjugate prior: Standard Bayesian approach for\n          binary observations\n        - Murphy (2006): Bayesian inference for Bernoulli distribution\n        \"\"\"\n        if not 0 &lt;= prior_weight &lt;= 1:\n            raise ValueError(\n                f\"prior_weight must be in [0, 1], got {prior_weight}\"\n            )\n\n        if new_observation:\n            # Positive observation: move confidence towards 1\n            # confidence += learning_rate * (target - current)\n            self.confidence = self.confidence + prior_weight * (1 - self.confidence)\n        else:\n            # Negative observation: decay confidence\n            # confidence *= (1 - decay_rate)\n            self.confidence = self.confidence * (1 - prior_weight)\n\n        # Ensure confidence stays in valid range (should be guaranteed by math)\n        self.confidence = max(0.0, min(1.0, self.confidence))\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Serialize evidence to JSON-compatible dictionary.\n\n        Returns:\n            Dictionary with all evidence fields\n\n        Example:\n            &gt;&gt;&gt; evidence = Evidence(\n            ...     source=\"test\",\n            ...     type=\"benchmark\",\n            ...     data={\"metric\": 100},\n            ...     confidence=0.8,\n            ...     timestamp=datetime(2024, 1, 1, 12, 0, 0)\n            ... )\n            &gt;&gt;&gt; d = evidence.to_dict()\n            &gt;&gt;&gt; d[\"source\"]\n            'test'\n        \"\"\"\n        return {\n            \"source\": self.source,\n            \"type\": self.type,\n            \"data\": self.data.copy(),\n            \"confidence\": self.confidence,\n            \"timestamp\": self.timestamp.isoformat()\n        }\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -&gt; \"Evidence\":\n        \"\"\"\n        Deserialize evidence from dictionary.\n\n        Args:\n            data: Dictionary containing evidence fields\n\n        Returns:\n            Evidence instance\n\n        Example:\n            &gt;&gt;&gt; data = {\n            ...     \"source\": \"test\",\n            ...     \"type\": \"benchmark\",\n            ...     \"data\": {\"metric\": 100},\n            ...     \"confidence\": 0.8,\n            ...     \"timestamp\": \"2024-01-01T12:00:00\"\n            ... }\n            &gt;&gt;&gt; evidence = Evidence.from_dict(data)\n            &gt;&gt;&gt; evidence.source\n            'test'\n        \"\"\"\n        return cls(\n            source=data[\"source\"],\n            type=data[\"type\"],\n            data=data[\"data\"],\n            confidence=data[\"confidence\"],\n            timestamp=datetime.fromisoformat(data[\"timestamp\"])\n        )\n\n    def __str__(self) -&gt; str:\n        \"\"\"Human-readable string representation.\"\"\"\n        return (\n            f\"Evidence({self.type} from {self.source}, \"\n            f\"confidence={self.confidence:.2f})\"\n        )\n</code></pre>"},{"location":"api/core/evidence/#upir.core.evidence.Evidence-functions","title":"Functions","text":""},{"location":"api/core/evidence/#upir.core.evidence.Evidence.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate evidence fields.</p> Source code in <code>upir/core/evidence.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate evidence fields.\"\"\"\n    valid_types = {\"benchmark\", \"test\", \"production\", \"formal_proof\"}\n    if self.type not in valid_types:\n        raise ValueError(\n            f\"Invalid evidence type '{self.type}'. \"\n            f\"Must be one of: {valid_types}\"\n        )\n\n    if not 0 &lt;= self.confidence &lt;= 1:\n        raise ValueError(\n            f\"Confidence must be in [0, 1], got {self.confidence}\"\n        )\n\n    if not self.source:\n        raise ValueError(\"Source cannot be empty\")\n</code></pre>"},{"location":"api/core/evidence/#upir.core.evidence.Evidence.update_confidence","title":"<code>update_confidence(new_observation, prior_weight=0.1)</code>","text":"<p>Update confidence using Bayesian update based on new observation.</p> <p>This implements a simple Bayesian update using a beta-binomial conjugate prior. The prior_weight controls how much the new observation affects the current confidence.</p> <p>Update rules: - Positive observation: confidence += prior_weight * (1 - confidence) - Negative observation: confidence *= (1 - prior_weight)</p> <p>These rules ensure: 1. Confidence stays in [0, 1] 2. Positive observations increase confidence (asymptotically to 1) 3. Negative observations decrease confidence (multiplicatively) 4. Higher current confidence is harder to change (conservative)</p> <p>Parameters:</p> Name Type Description Default <code>new_observation</code> <code>bool</code> <p>True if observation supports the evidence,            False if it contradicts it</p> required <code>prior_weight</code> <code>float</code> <p>Weight of the prior in [0, 1]. Higher values mean          new observations have more impact. Default 0.1.</p> <code>0.1</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If prior_weight not in [0, 1]</p> Example <p>evidence = Evidence( ...     source=\"test\", ...     type=\"benchmark\", ...     data={}, ...     confidence=0.5 ... ) evidence.update_confidence(new_observation=True) evidence.confidence 0.55 evidence.update_confidence(new_observation=False) evidence.confidence 0.495</p> <p>References: - Beta-binomial conjugate prior: Standard Bayesian approach for   binary observations - Murphy (2006): Bayesian inference for Bernoulli distribution</p> Source code in <code>upir/core/evidence.py</code> <pre><code>def update_confidence(\n    self,\n    new_observation: bool,\n    prior_weight: float = 0.1\n) -&gt; None:\n    \"\"\"\n    Update confidence using Bayesian update based on new observation.\n\n    This implements a simple Bayesian update using a beta-binomial conjugate\n    prior. The prior_weight controls how much the new observation affects\n    the current confidence.\n\n    Update rules:\n    - Positive observation: confidence += prior_weight * (1 - confidence)\n    - Negative observation: confidence *= (1 - prior_weight)\n\n    These rules ensure:\n    1. Confidence stays in [0, 1]\n    2. Positive observations increase confidence (asymptotically to 1)\n    3. Negative observations decrease confidence (multiplicatively)\n    4. Higher current confidence is harder to change (conservative)\n\n    Args:\n        new_observation: True if observation supports the evidence,\n                       False if it contradicts it\n        prior_weight: Weight of the prior in [0, 1]. Higher values mean\n                     new observations have more impact. Default 0.1.\n\n    Raises:\n        ValueError: If prior_weight not in [0, 1]\n\n    Example:\n        &gt;&gt;&gt; evidence = Evidence(\n        ...     source=\"test\",\n        ...     type=\"benchmark\",\n        ...     data={},\n        ...     confidence=0.5\n        ... )\n        &gt;&gt;&gt; evidence.update_confidence(new_observation=True)\n        &gt;&gt;&gt; evidence.confidence\n        0.55\n        &gt;&gt;&gt; evidence.update_confidence(new_observation=False)\n        &gt;&gt;&gt; evidence.confidence\n        0.495\n\n    References:\n    - Beta-binomial conjugate prior: Standard Bayesian approach for\n      binary observations\n    - Murphy (2006): Bayesian inference for Bernoulli distribution\n    \"\"\"\n    if not 0 &lt;= prior_weight &lt;= 1:\n        raise ValueError(\n            f\"prior_weight must be in [0, 1], got {prior_weight}\"\n        )\n\n    if new_observation:\n        # Positive observation: move confidence towards 1\n        # confidence += learning_rate * (target - current)\n        self.confidence = self.confidence + prior_weight * (1 - self.confidence)\n    else:\n        # Negative observation: decay confidence\n        # confidence *= (1 - decay_rate)\n        self.confidence = self.confidence * (1 - prior_weight)\n\n    # Ensure confidence stays in valid range (should be guaranteed by math)\n    self.confidence = max(0.0, min(1.0, self.confidence))\n</code></pre>"},{"location":"api/core/evidence/#upir.core.evidence.Evidence.to_dict","title":"<code>to_dict()</code>","text":"<p>Serialize evidence to JSON-compatible dictionary.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with all evidence fields</p> Example <p>evidence = Evidence( ...     source=\"test\", ...     type=\"benchmark\", ...     data={\"metric\": 100}, ...     confidence=0.8, ...     timestamp=datetime(2024, 1, 1, 12, 0, 0) ... ) d = evidence.to_dict() d[\"source\"] 'test'</p> Source code in <code>upir/core/evidence.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Serialize evidence to JSON-compatible dictionary.\n\n    Returns:\n        Dictionary with all evidence fields\n\n    Example:\n        &gt;&gt;&gt; evidence = Evidence(\n        ...     source=\"test\",\n        ...     type=\"benchmark\",\n        ...     data={\"metric\": 100},\n        ...     confidence=0.8,\n        ...     timestamp=datetime(2024, 1, 1, 12, 0, 0)\n        ... )\n        &gt;&gt;&gt; d = evidence.to_dict()\n        &gt;&gt;&gt; d[\"source\"]\n        'test'\n    \"\"\"\n    return {\n        \"source\": self.source,\n        \"type\": self.type,\n        \"data\": self.data.copy(),\n        \"confidence\": self.confidence,\n        \"timestamp\": self.timestamp.isoformat()\n    }\n</code></pre>"},{"location":"api/core/evidence/#upir.core.evidence.Evidence.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Deserialize evidence from dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>Dictionary containing evidence fields</p> required <p>Returns:</p> Type Description <code>Evidence</code> <p>Evidence instance</p> Example <p>data = { ...     \"source\": \"test\", ...     \"type\": \"benchmark\", ...     \"data\": {\"metric\": 100}, ...     \"confidence\": 0.8, ...     \"timestamp\": \"2024-01-01T12:00:00\" ... } evidence = Evidence.from_dict(data) evidence.source 'test'</p> Source code in <code>upir/core/evidence.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, Any]) -&gt; \"Evidence\":\n    \"\"\"\n    Deserialize evidence from dictionary.\n\n    Args:\n        data: Dictionary containing evidence fields\n\n    Returns:\n        Evidence instance\n\n    Example:\n        &gt;&gt;&gt; data = {\n        ...     \"source\": \"test\",\n        ...     \"type\": \"benchmark\",\n        ...     \"data\": {\"metric\": 100},\n        ...     \"confidence\": 0.8,\n        ...     \"timestamp\": \"2024-01-01T12:00:00\"\n        ... }\n        &gt;&gt;&gt; evidence = Evidence.from_dict(data)\n        &gt;&gt;&gt; evidence.source\n        'test'\n    \"\"\"\n    return cls(\n        source=data[\"source\"],\n        type=data[\"type\"],\n        data=data[\"data\"],\n        confidence=data[\"confidence\"],\n        timestamp=datetime.fromisoformat(data[\"timestamp\"])\n    )\n</code></pre>"},{"location":"api/core/evidence/#upir.core.evidence.Evidence.__str__","title":"<code>__str__()</code>","text":"<p>Human-readable string representation.</p> Source code in <code>upir/core/evidence.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Human-readable string representation.\"\"\"\n    return (\n        f\"Evidence({self.type} from {self.source}, \"\n        f\"confidence={self.confidence:.2f})\"\n    )\n</code></pre>"},{"location":"api/core/evidence/#upir.core.evidence.ReasoningNode","title":"<code>upir.core.evidence.ReasoningNode</code>  <code>dataclass</code>","text":"<p>A node in the reasoning graph representing an architectural decision.</p> <p>Reasoning nodes form a directed acyclic graph (DAG) where each node represents a decision or conclusion, supported by evidence and potentially dependent on other decisions (parent nodes).</p> <p>Based on the TD Commons disclosure, UPIR maintains a reasoning graph to track decision provenance and propagate confidence through the architecture.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier (UUID)</p> <code>decision</code> <code>str</code> <p>The decision or conclusion made</p> <code>rationale</code> <code>str</code> <p>Explanation of why this decision was made</p> <code>evidence_ids</code> <code>List[str]</code> <p>IDs of Evidence objects supporting this decision</p> <code>parent_ids</code> <code>List[str]</code> <p>IDs of other ReasoningNodes this depends on (for DAG)</p> <code>alternatives</code> <code>List[Dict[str, Any]]</code> <p>Other options that were considered but not chosen</p> <code>confidence</code> <code>float</code> <p>Computed confidence in this decision [0, 1]</p> Example <p>node = ReasoningNode( ...     id=str(uuid.uuid4()), ...     decision=\"Use PostgreSQL for primary database\", ...     rationale=\"Strong consistency needed for financial transactions\", ...     evidence_ids=[\"evidence-1\", \"evidence-2\"], ...     parent_ids=[\"node-consistency-requirement\"], ...     alternatives=[ ...         {\"option\": \"MongoDB\", \"rejected_because\": \"eventual consistency\"} ...     ], ...     confidence=0.0  # Will be computed from evidence ... )</p> <p>References: - TD Commons: Reasoning graph structure - DAG: Directed acyclic graph for decision dependencies</p> Source code in <code>upir/core/evidence.py</code> <pre><code>@dataclass\nclass ReasoningNode:\n    \"\"\"\n    A node in the reasoning graph representing an architectural decision.\n\n    Reasoning nodes form a directed acyclic graph (DAG) where each node\n    represents a decision or conclusion, supported by evidence and potentially\n    dependent on other decisions (parent nodes).\n\n    Based on the TD Commons disclosure, UPIR maintains a reasoning graph to\n    track decision provenance and propagate confidence through the architecture.\n\n    Attributes:\n        id: Unique identifier (UUID)\n        decision: The decision or conclusion made\n        rationale: Explanation of why this decision was made\n        evidence_ids: IDs of Evidence objects supporting this decision\n        parent_ids: IDs of other ReasoningNodes this depends on (for DAG)\n        alternatives: Other options that were considered but not chosen\n        confidence: Computed confidence in this decision [0, 1]\n\n    Example:\n        &gt;&gt;&gt; node = ReasoningNode(\n        ...     id=str(uuid.uuid4()),\n        ...     decision=\"Use PostgreSQL for primary database\",\n        ...     rationale=\"Strong consistency needed for financial transactions\",\n        ...     evidence_ids=[\"evidence-1\", \"evidence-2\"],\n        ...     parent_ids=[\"node-consistency-requirement\"],\n        ...     alternatives=[\n        ...         {\"option\": \"MongoDB\", \"rejected_because\": \"eventual consistency\"}\n        ...     ],\n        ...     confidence=0.0  # Will be computed from evidence\n        ... )\n\n    References:\n    - TD Commons: Reasoning graph structure\n    - DAG: Directed acyclic graph for decision dependencies\n    \"\"\"\n    id: str\n    decision: str\n    rationale: str\n    evidence_ids: List[str] = field(default_factory=list)\n    parent_ids: List[str] = field(default_factory=list)\n    alternatives: List[Dict[str, Any]] = field(default_factory=list)\n    confidence: float = 0.0\n\n    def __post_init__(self):\n        \"\"\"Validate reasoning node fields.\"\"\"\n        if not self.id:\n            raise ValueError(\"ID cannot be empty\")\n\n        if not self.decision:\n            raise ValueError(\"Decision cannot be empty\")\n\n        if not 0 &lt;= self.confidence &lt;= 1:\n            raise ValueError(\n                f\"Confidence must be in [0, 1], got {self.confidence}\"\n            )\n\n    @staticmethod\n    def generate_id() -&gt; str:\n        \"\"\"\n        Generate a unique ID for a reasoning node.\n\n        Returns:\n            UUID string\n\n        Example:\n            &gt;&gt;&gt; node_id = ReasoningNode.generate_id()\n            &gt;&gt;&gt; len(node_id) == 36  # UUID format\n            True\n        \"\"\"\n        return str(uuid.uuid4())\n\n    def compute_confidence(self, evidence_map: Dict[str, Evidence]) -&gt; float:\n        \"\"\"\n        Compute aggregate confidence from supporting evidence using geometric mean.\n\n        The geometric mean is more conservative than arithmetic mean - a single\n        piece of low-confidence evidence significantly reduces overall confidence.\n        This matches how engineers actually reason: one weak piece of evidence\n        can't be compensated by many strong pieces.\n\n        Formula: exp(mean(log(c_i))) for confidences c_i\n\n        If no evidence is available, returns 0.0 (no confidence).\n        If any evidence has confidence 0, returns 0.0 (geometric mean property).\n\n        Args:\n            evidence_map: Mapping from evidence IDs to Evidence objects\n\n        Returns:\n            Computed confidence in [0, 1]\n\n        Example:\n            &gt;&gt;&gt; evidence_map = {\n            ...     \"e1\": Evidence(\"src1\", \"test\", {}, 0.8, datetime.utcnow()),\n            ...     \"e2\": Evidence(\"src2\", \"test\", {}, 0.9, datetime.utcnow())\n            ... }\n            &gt;&gt;&gt; node = ReasoningNode(\n            ...     id=\"node-1\",\n            ...     decision=\"test\",\n            ...     rationale=\"test\",\n            ...     evidence_ids=[\"e1\", \"e2\"]\n            ... )\n            &gt;&gt;&gt; conf = node.compute_confidence(evidence_map)\n            &gt;&gt;&gt; 0.84 &lt; conf &lt; 0.85  # sqrt(0.8 * 0.9) \u2248 0.8485\n            True\n\n        References:\n        - Geometric mean: https://en.wikipedia.org/wiki/Geometric_mean\n        - More conservative than arithmetic mean for combining confidences\n        \"\"\"\n        # Filter evidence to only those referenced by this node\n        relevant_evidence = [\n            evidence_map[eid]\n            for eid in self.evidence_ids\n            if eid in evidence_map\n        ]\n\n        if not relevant_evidence:\n            return 0.0\n\n        # Check for any zero confidence (geometric mean would be 0)\n        confidences = [e.confidence for e in relevant_evidence]\n        if any(c == 0.0 for c in confidences):\n            return 0.0\n\n        # Compute geometric mean: exp(mean(log(c_i)))\n        log_sum = sum(math.log(c) for c in confidences)\n        log_mean = log_sum / len(confidences)\n        geometric_mean = math.exp(log_mean)\n\n        return geometric_mean\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Serialize reasoning node to JSON-compatible dictionary.\n\n        Returns:\n            Dictionary with all node fields\n\n        Example:\n            &gt;&gt;&gt; node = ReasoningNode(\n            ...     id=\"node-1\",\n            ...     decision=\"Use caching\",\n            ...     rationale=\"Reduce latency\",\n            ...     evidence_ids=[\"e1\"],\n            ...     parent_ids=[],\n            ...     alternatives=[{\"option\": \"No cache\"}],\n            ...     confidence=0.8\n            ... )\n            &gt;&gt;&gt; d = node.to_dict()\n            &gt;&gt;&gt; d[\"decision\"]\n            'Use caching'\n        \"\"\"\n        return {\n            \"id\": self.id,\n            \"decision\": self.decision,\n            \"rationale\": self.rationale,\n            \"evidence_ids\": self.evidence_ids.copy(),\n            \"parent_ids\": self.parent_ids.copy(),\n            \"alternatives\": [alt.copy() for alt in self.alternatives],\n            \"confidence\": self.confidence\n        }\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -&gt; \"ReasoningNode\":\n        \"\"\"\n        Deserialize reasoning node from dictionary.\n\n        Args:\n            data: Dictionary containing node fields\n\n        Returns:\n            ReasoningNode instance\n\n        Example:\n            &gt;&gt;&gt; data = {\n            ...     \"id\": \"node-1\",\n            ...     \"decision\": \"Use caching\",\n            ...     \"rationale\": \"Reduce latency\",\n            ...     \"evidence_ids\": [\"e1\"],\n            ...     \"parent_ids\": [],\n            ...     \"alternatives\": [],\n            ...     \"confidence\": 0.8\n            ... }\n            &gt;&gt;&gt; node = ReasoningNode.from_dict(data)\n            &gt;&gt;&gt; node.decision\n            'Use caching'\n        \"\"\"\n        return cls(\n            id=data[\"id\"],\n            decision=data[\"decision\"],\n            rationale=data[\"rationale\"],\n            evidence_ids=data.get(\"evidence_ids\", []),\n            parent_ids=data.get(\"parent_ids\", []),\n            alternatives=data.get(\"alternatives\", []),\n            confidence=data.get(\"confidence\", 0.0)\n        )\n\n    def __str__(self) -&gt; str:\n        \"\"\"Human-readable string representation.\"\"\"\n        return (\n            f\"ReasoningNode({self.decision}, \"\n            f\"confidence={self.confidence:.2f}, \"\n            f\"evidence={len(self.evidence_ids)})\"\n        )\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Developer-friendly representation.\"\"\"\n        return (\n            f\"ReasoningNode(id='{self.id}', \"\n            f\"decision='{self.decision}', \"\n            f\"confidence={self.confidence}, \"\n            f\"evidence_count={len(self.evidence_ids)}, \"\n            f\"parent_count={len(self.parent_ids)})\"\n        )\n</code></pre>"},{"location":"api/core/evidence/#upir.core.evidence.ReasoningNode-functions","title":"Functions","text":""},{"location":"api/core/evidence/#upir.core.evidence.ReasoningNode.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate reasoning node fields.</p> Source code in <code>upir/core/evidence.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate reasoning node fields.\"\"\"\n    if not self.id:\n        raise ValueError(\"ID cannot be empty\")\n\n    if not self.decision:\n        raise ValueError(\"Decision cannot be empty\")\n\n    if not 0 &lt;= self.confidence &lt;= 1:\n        raise ValueError(\n            f\"Confidence must be in [0, 1], got {self.confidence}\"\n        )\n</code></pre>"},{"location":"api/core/evidence/#upir.core.evidence.ReasoningNode.generate_id","title":"<code>generate_id()</code>  <code>staticmethod</code>","text":"<p>Generate a unique ID for a reasoning node.</p> <p>Returns:</p> Type Description <code>str</code> <p>UUID string</p> Example <p>node_id = ReasoningNode.generate_id() len(node_id) == 36  # UUID format True</p> Source code in <code>upir/core/evidence.py</code> <pre><code>@staticmethod\ndef generate_id() -&gt; str:\n    \"\"\"\n    Generate a unique ID for a reasoning node.\n\n    Returns:\n        UUID string\n\n    Example:\n        &gt;&gt;&gt; node_id = ReasoningNode.generate_id()\n        &gt;&gt;&gt; len(node_id) == 36  # UUID format\n        True\n    \"\"\"\n    return str(uuid.uuid4())\n</code></pre>"},{"location":"api/core/evidence/#upir.core.evidence.ReasoningNode.compute_confidence","title":"<code>compute_confidence(evidence_map)</code>","text":"<p>Compute aggregate confidence from supporting evidence using geometric mean.</p> <p>The geometric mean is more conservative than arithmetic mean - a single piece of low-confidence evidence significantly reduces overall confidence. This matches how engineers actually reason: one weak piece of evidence can't be compensated by many strong pieces.</p> <p>Formula: exp(mean(log(c_i))) for confidences c_i</p> <p>If no evidence is available, returns 0.0 (no confidence). If any evidence has confidence 0, returns 0.0 (geometric mean property).</p> <p>Parameters:</p> Name Type Description Default <code>evidence_map</code> <code>Dict[str, Evidence]</code> <p>Mapping from evidence IDs to Evidence objects</p> required <p>Returns:</p> Type Description <code>float</code> <p>Computed confidence in [0, 1]</p> Example <p>evidence_map = { ...     \"e1\": Evidence(\"src1\", \"test\", {}, 0.8, datetime.utcnow()), ...     \"e2\": Evidence(\"src2\", \"test\", {}, 0.9, datetime.utcnow()) ... } node = ReasoningNode( ...     id=\"node-1\", ...     decision=\"test\", ...     rationale=\"test\", ...     evidence_ids=[\"e1\", \"e2\"] ... ) conf = node.compute_confidence(evidence_map) 0.84 &lt; conf &lt; 0.85  # sqrt(0.8 * 0.9) \u2248 0.8485 True</p> <p>References: - Geometric mean: https://en.wikipedia.org/wiki/Geometric_mean - More conservative than arithmetic mean for combining confidences</p> Source code in <code>upir/core/evidence.py</code> <pre><code>def compute_confidence(self, evidence_map: Dict[str, Evidence]) -&gt; float:\n    \"\"\"\n    Compute aggregate confidence from supporting evidence using geometric mean.\n\n    The geometric mean is more conservative than arithmetic mean - a single\n    piece of low-confidence evidence significantly reduces overall confidence.\n    This matches how engineers actually reason: one weak piece of evidence\n    can't be compensated by many strong pieces.\n\n    Formula: exp(mean(log(c_i))) for confidences c_i\n\n    If no evidence is available, returns 0.0 (no confidence).\n    If any evidence has confidence 0, returns 0.0 (geometric mean property).\n\n    Args:\n        evidence_map: Mapping from evidence IDs to Evidence objects\n\n    Returns:\n        Computed confidence in [0, 1]\n\n    Example:\n        &gt;&gt;&gt; evidence_map = {\n        ...     \"e1\": Evidence(\"src1\", \"test\", {}, 0.8, datetime.utcnow()),\n        ...     \"e2\": Evidence(\"src2\", \"test\", {}, 0.9, datetime.utcnow())\n        ... }\n        &gt;&gt;&gt; node = ReasoningNode(\n        ...     id=\"node-1\",\n        ...     decision=\"test\",\n        ...     rationale=\"test\",\n        ...     evidence_ids=[\"e1\", \"e2\"]\n        ... )\n        &gt;&gt;&gt; conf = node.compute_confidence(evidence_map)\n        &gt;&gt;&gt; 0.84 &lt; conf &lt; 0.85  # sqrt(0.8 * 0.9) \u2248 0.8485\n        True\n\n    References:\n    - Geometric mean: https://en.wikipedia.org/wiki/Geometric_mean\n    - More conservative than arithmetic mean for combining confidences\n    \"\"\"\n    # Filter evidence to only those referenced by this node\n    relevant_evidence = [\n        evidence_map[eid]\n        for eid in self.evidence_ids\n        if eid in evidence_map\n    ]\n\n    if not relevant_evidence:\n        return 0.0\n\n    # Check for any zero confidence (geometric mean would be 0)\n    confidences = [e.confidence for e in relevant_evidence]\n    if any(c == 0.0 for c in confidences):\n        return 0.0\n\n    # Compute geometric mean: exp(mean(log(c_i)))\n    log_sum = sum(math.log(c) for c in confidences)\n    log_mean = log_sum / len(confidences)\n    geometric_mean = math.exp(log_mean)\n\n    return geometric_mean\n</code></pre>"},{"location":"api/core/evidence/#upir.core.evidence.ReasoningNode.to_dict","title":"<code>to_dict()</code>","text":"<p>Serialize reasoning node to JSON-compatible dictionary.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with all node fields</p> Example <p>node = ReasoningNode( ...     id=\"node-1\", ...     decision=\"Use caching\", ...     rationale=\"Reduce latency\", ...     evidence_ids=[\"e1\"], ...     parent_ids=[], ...     alternatives=[{\"option\": \"No cache\"}], ...     confidence=0.8 ... ) d = node.to_dict() d[\"decision\"] 'Use caching'</p> Source code in <code>upir/core/evidence.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Serialize reasoning node to JSON-compatible dictionary.\n\n    Returns:\n        Dictionary with all node fields\n\n    Example:\n        &gt;&gt;&gt; node = ReasoningNode(\n        ...     id=\"node-1\",\n        ...     decision=\"Use caching\",\n        ...     rationale=\"Reduce latency\",\n        ...     evidence_ids=[\"e1\"],\n        ...     parent_ids=[],\n        ...     alternatives=[{\"option\": \"No cache\"}],\n        ...     confidence=0.8\n        ... )\n        &gt;&gt;&gt; d = node.to_dict()\n        &gt;&gt;&gt; d[\"decision\"]\n        'Use caching'\n    \"\"\"\n    return {\n        \"id\": self.id,\n        \"decision\": self.decision,\n        \"rationale\": self.rationale,\n        \"evidence_ids\": self.evidence_ids.copy(),\n        \"parent_ids\": self.parent_ids.copy(),\n        \"alternatives\": [alt.copy() for alt in self.alternatives],\n        \"confidence\": self.confidence\n    }\n</code></pre>"},{"location":"api/core/evidence/#upir.core.evidence.ReasoningNode.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Deserialize reasoning node from dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>Dictionary containing node fields</p> required <p>Returns:</p> Type Description <code>ReasoningNode</code> <p>ReasoningNode instance</p> Example <p>data = { ...     \"id\": \"node-1\", ...     \"decision\": \"Use caching\", ...     \"rationale\": \"Reduce latency\", ...     \"evidence_ids\": [\"e1\"], ...     \"parent_ids\": [], ...     \"alternatives\": [], ...     \"confidence\": 0.8 ... } node = ReasoningNode.from_dict(data) node.decision 'Use caching'</p> Source code in <code>upir/core/evidence.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, Any]) -&gt; \"ReasoningNode\":\n    \"\"\"\n    Deserialize reasoning node from dictionary.\n\n    Args:\n        data: Dictionary containing node fields\n\n    Returns:\n        ReasoningNode instance\n\n    Example:\n        &gt;&gt;&gt; data = {\n        ...     \"id\": \"node-1\",\n        ...     \"decision\": \"Use caching\",\n        ...     \"rationale\": \"Reduce latency\",\n        ...     \"evidence_ids\": [\"e1\"],\n        ...     \"parent_ids\": [],\n        ...     \"alternatives\": [],\n        ...     \"confidence\": 0.8\n        ... }\n        &gt;&gt;&gt; node = ReasoningNode.from_dict(data)\n        &gt;&gt;&gt; node.decision\n        'Use caching'\n    \"\"\"\n    return cls(\n        id=data[\"id\"],\n        decision=data[\"decision\"],\n        rationale=data[\"rationale\"],\n        evidence_ids=data.get(\"evidence_ids\", []),\n        parent_ids=data.get(\"parent_ids\", []),\n        alternatives=data.get(\"alternatives\", []),\n        confidence=data.get(\"confidence\", 0.0)\n    )\n</code></pre>"},{"location":"api/core/evidence/#upir.core.evidence.ReasoningNode.__str__","title":"<code>__str__()</code>","text":"<p>Human-readable string representation.</p> Source code in <code>upir/core/evidence.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Human-readable string representation.\"\"\"\n    return (\n        f\"ReasoningNode({self.decision}, \"\n        f\"confidence={self.confidence:.2f}, \"\n        f\"evidence={len(self.evidence_ids)})\"\n    )\n</code></pre>"},{"location":"api/core/evidence/#upir.core.evidence.ReasoningNode.__repr__","title":"<code>__repr__()</code>","text":"<p>Developer-friendly representation.</p> Source code in <code>upir/core/evidence.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Developer-friendly representation.\"\"\"\n    return (\n        f\"ReasoningNode(id='{self.id}', \"\n        f\"decision='{self.decision}', \"\n        f\"confidence={self.confidence}, \"\n        f\"evidence_count={len(self.evidence_ids)}, \"\n        f\"parent_count={len(self.parent_ids)})\"\n    )\n</code></pre>"},{"location":"api/core/evidence/#see-also","title":"See Also","text":"<ul> <li>UPIR - Main UPIR class</li> </ul>"},{"location":"api/core/specification/","title":"FormalSpecification","text":"<p>Formal specification with temporal properties and constraints.</p>"},{"location":"api/core/specification/#overview","title":"Overview","text":"<p>The <code>FormalSpecification</code> class defines what a system must do:</p> <ul> <li>Properties: Temporal properties (liveness, safety)</li> <li>Invariants: Properties that must always hold</li> <li>Constraints: Hard constraints on resources and performance</li> </ul>"},{"location":"api/core/specification/#class-documentation","title":"Class Documentation","text":""},{"location":"api/core/specification/#upir.core.specification.FormalSpecification","title":"<code>upir.core.specification.FormalSpecification</code>  <code>dataclass</code>","text":"<p>A formal specification for distributed system architecture.</p> <p>Formal specifications capture all requirements and constraints that an architecture must satisfy, including temporal properties, resource constraints, and environmental assumptions.</p> <p>Based on the TD Commons disclosure, specifications include: - Invariants: Properties that MUST always hold (safety properties) - Properties: Desired properties that should hold (liveness, performance) - Constraints: Resource limits (latency, throughput, cost, etc.) - Assumptions: Environmental conditions assumed to hold</p> <p>Attributes:</p> Name Type Description <code>invariants</code> <code>List[TemporalProperty]</code> <p>List of temporal properties that must always hold.        Violations indicate architectural bugs.</p> <code>properties</code> <code>List[TemporalProperty]</code> <p>List of temporal properties that should hold.        These are optimization targets.</p> <code>constraints</code> <code>Dict[str, Dict[str, Any]]</code> <p>Resource constraints as nested dicts.         Format: {\"resource_name\": {\"min\": x, \"max\": y, \"equals\": z}}</p> <code>assumptions</code> <code>List[str]</code> <p>Environmental assumptions (e.g., \"network_reliable\",         \"nodes_fail_independently\")</p> Example <p>spec = FormalSpecification( ...     invariants=[ ...         TemporalProperty( ...             operator=TemporalOperator.ALWAYS, ...             predicate=\"data_consistent\" ...         ) ...     ], ...     constraints={ ...         \"latency\": {\"max\": 100}, ...         \"cost_per_month\": {\"max\": 10000} ...     }, ...     assumptions=[\"network_partitions_rare\"] ... )</p> <p>References: - TD Commons: Formal specification structure - Design by Contract: Invariants and preconditions</p> Source code in <code>upir/core/specification.py</code> <pre><code>@dataclass\nclass FormalSpecification:\n    \"\"\"\n    A formal specification for distributed system architecture.\n\n    Formal specifications capture all requirements and constraints that an\n    architecture must satisfy, including temporal properties, resource constraints,\n    and environmental assumptions.\n\n    Based on the TD Commons disclosure, specifications include:\n    - Invariants: Properties that MUST always hold (safety properties)\n    - Properties: Desired properties that should hold (liveness, performance)\n    - Constraints: Resource limits (latency, throughput, cost, etc.)\n    - Assumptions: Environmental conditions assumed to hold\n\n    Attributes:\n        invariants: List of temporal properties that must always hold.\n                   Violations indicate architectural bugs.\n        properties: List of temporal properties that should hold.\n                   These are optimization targets.\n        constraints: Resource constraints as nested dicts.\n                    Format: {\"resource_name\": {\"min\": x, \"max\": y, \"equals\": z}}\n        assumptions: Environmental assumptions (e.g., \"network_reliable\",\n                    \"nodes_fail_independently\")\n\n    Example:\n        &gt;&gt;&gt; spec = FormalSpecification(\n        ...     invariants=[\n        ...         TemporalProperty(\n        ...             operator=TemporalOperator.ALWAYS,\n        ...             predicate=\"data_consistent\"\n        ...         )\n        ...     ],\n        ...     constraints={\n        ...         \"latency\": {\"max\": 100},\n        ...         \"cost_per_month\": {\"max\": 10000}\n        ...     },\n        ...     assumptions=[\"network_partitions_rare\"]\n        ... )\n\n    References:\n    - TD Commons: Formal specification structure\n    - Design by Contract: Invariants and preconditions\n    \"\"\"\n    invariants: List[TemporalProperty] = field(default_factory=list)\n    properties: List[TemporalProperty] = field(default_factory=list)\n    constraints: Dict[str, Dict[str, Any]] = field(default_factory=dict)\n    assumptions: List[str] = field(default_factory=list)\n\n    def validate(self) -&gt; bool:\n        \"\"\"\n        Validate specification consistency and well-formedness.\n\n        Checks performed:\n        1. No duplicate invariants (same operator and predicate)\n        2. No duplicate properties\n        3. All constraints have valid fields (min, max, or equals)\n        4. Constraint values are valid (min &lt;= max if both present)\n        5. No empty predicates in temporal properties\n\n        Returns:\n            True if specification is valid\n\n        Raises:\n            ValueError: If specification has consistency issues\n\n        Example:\n            &gt;&gt;&gt; spec = FormalSpecification(\n            ...     invariants=[\n            ...         TemporalProperty(\n            ...             operator=TemporalOperator.ALWAYS,\n            ...             predicate=\"data_consistent\"\n            ...         )\n            ...     ]\n            ... )\n            &gt;&gt;&gt; spec.validate()\n            True\n        \"\"\"\n        # Check for duplicate invariants\n        seen_invariants = set()\n        for inv in self.invariants:\n            key = (inv.operator, inv.predicate)\n            if key in seen_invariants:\n                raise ValueError(\n                    f\"Duplicate invariant: {inv.operator.value}({inv.predicate})\"\n                )\n            seen_invariants.add(key)\n\n        # Check for duplicate properties\n        seen_properties = set()\n        for prop in self.properties:\n            key = (prop.operator, prop.predicate)\n            if key in seen_properties:\n                raise ValueError(\n                    f\"Duplicate property: {prop.operator.value}({prop.predicate})\"\n                )\n            seen_properties.add(key)\n\n        # Validate constraints\n        for resource_name, constraint in self.constraints.items():\n            if not isinstance(constraint, dict):\n                raise ValueError(\n                    f\"Constraint '{resource_name}' must be a dictionary\"\n                )\n\n            # Check that constraint has at least one valid field\n            valid_fields = {\"min\", \"max\", \"equals\"}\n            constraint_fields = set(constraint.keys())\n            if not constraint_fields.intersection(valid_fields):\n                raise ValueError(\n                    f\"Constraint '{resource_name}' must have at least one of: \"\n                    f\"min, max, equals. Got: {constraint_fields}\"\n                )\n\n            # Check that invalid fields are not present\n            invalid_fields = constraint_fields - valid_fields\n            if invalid_fields:\n                raise ValueError(\n                    f\"Constraint '{resource_name}' has invalid fields: {invalid_fields}. \"\n                    f\"Valid fields are: {valid_fields}\"\n                )\n\n            # Validate min &lt;= max if both present\n            if \"min\" in constraint and \"max\" in constraint:\n                min_val = constraint[\"min\"]\n                max_val = constraint[\"max\"]\n                if min_val &gt; max_val:\n                    raise ValueError(\n                        f\"Constraint '{resource_name}': min ({min_val}) &gt; max ({max_val})\"\n                    )\n\n            # If 'equals' is present, it should be the only field\n            if \"equals\" in constraint and len(constraint) &gt; 1:\n                raise ValueError(\n                    f\"Constraint '{resource_name}': 'equals' cannot be combined \"\n                    f\"with min/max\"\n                )\n\n        return True\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Serialize formal specification to JSON-compatible dictionary.\n\n        Uses deterministic ordering (sorted keys) to ensure consistent\n        serialization for hashing.\n\n        Returns:\n            Dictionary with all specification fields\n\n        Example:\n            &gt;&gt;&gt; spec = FormalSpecification(\n            ...     constraints={\"latency\": {\"max\": 100}}\n            ... )\n            &gt;&gt;&gt; d = spec.to_dict()\n            &gt;&gt;&gt; d[\"constraints\"][\"latency\"][\"max\"]\n            100\n        \"\"\"\n        return {\n            \"invariants\": [inv.to_dict() for inv in self.invariants],\n            \"properties\": [prop.to_dict() for prop in self.properties],\n            \"constraints\": {\n                k: dict(v) for k, v in sorted(self.constraints.items())\n            },\n            \"assumptions\": sorted(self.assumptions)\n        }\n\n    def to_json(self) -&gt; str:\n        \"\"\"\n        Serialize formal specification to JSON string.\n\n        Returns:\n            JSON string representation\n\n        Example:\n            &gt;&gt;&gt; spec = FormalSpecification(\n            ...     constraints={\"latency\": {\"max\": 100}}\n            ... )\n            &gt;&gt;&gt; json_str = spec.to_json()\n            &gt;&gt;&gt; isinstance(json_str, str)\n            True\n        \"\"\"\n        return json.dumps(self.to_dict(), indent=2)\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -&gt; \"FormalSpecification\":\n        \"\"\"\n        Deserialize formal specification from dictionary.\n\n        Args:\n            data: Dictionary containing specification fields\n\n        Returns:\n            FormalSpecification instance\n\n        Example:\n            &gt;&gt;&gt; data = {\n            ...     \"invariants\": [],\n            ...     \"properties\": [],\n            ...     \"constraints\": {\"latency\": {\"max\": 100}},\n            ...     \"assumptions\": [\"network_reliable\"]\n            ... }\n            &gt;&gt;&gt; spec = FormalSpecification.from_dict(data)\n            &gt;&gt;&gt; spec.constraints[\"latency\"][\"max\"]\n            100\n        \"\"\"\n        return cls(\n            invariants=[\n                TemporalProperty.from_dict(inv)\n                for inv in data.get(\"invariants\", [])\n            ],\n            properties=[\n                TemporalProperty.from_dict(prop)\n                for prop in data.get(\"properties\", [])\n            ],\n            constraints=data.get(\"constraints\", {}),\n            assumptions=data.get(\"assumptions\", [])\n        )\n\n    def hash(self) -&gt; str:\n        \"\"\"\n        Compute SHA-256 hash of specification for integrity verification.\n\n        Uses deterministic JSON serialization (sorted keys) to ensure\n        consistent hashes across runs and platforms.\n\n        Returns:\n            Hexadecimal string of SHA-256 hash\n\n        Example:\n            &gt;&gt;&gt; spec1 = FormalSpecification(\n            ...     constraints={\"latency\": {\"max\": 100}}\n            ... )\n            &gt;&gt;&gt; spec2 = FormalSpecification(\n            ...     constraints={\"latency\": {\"max\": 100}}\n            ... )\n            &gt;&gt;&gt; spec1.hash() == spec2.hash()\n            True\n\n        References:\n        - Python hashlib: https://docs.python.org/3/library/hashlib.html\n        - SHA-256: Industry standard cryptographic hash\n        \"\"\"\n        # Convert to JSON with sorted keys for deterministic serialization\n        spec_dict = self.to_dict()\n        json_str = json.dumps(spec_dict, sort_keys=True)\n\n        # Compute SHA-256 hash\n        hash_obj = hashlib.sha256(json_str.encode('utf-8'))\n        return hash_obj.hexdigest()\n\n    def __str__(self) -&gt; str:\n        \"\"\"Human-readable string representation.\"\"\"\n        parts = []\n        if self.invariants:\n            parts.append(f\"{len(self.invariants)} invariant(s)\")\n        if self.properties:\n            parts.append(f\"{len(self.properties)} propertie(s)\")\n        if self.constraints:\n            parts.append(f\"{len(self.constraints)} constraint(s)\")\n        if self.assumptions:\n            parts.append(f\"{len(self.assumptions)} assumption(s)\")\n\n        if not parts:\n            return \"FormalSpecification(empty)\"\n\n        return f\"FormalSpecification({', '.join(parts)})\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Developer-friendly representation.\"\"\"\n        return (\n            f\"FormalSpecification(\"\n            f\"invariants={len(self.invariants)}, \"\n            f\"properties={len(self.properties)}, \"\n            f\"constraints={len(self.constraints)}, \"\n            f\"assumptions={len(self.assumptions)})\"\n        )\n</code></pre>"},{"location":"api/core/specification/#upir.core.specification.FormalSpecification-functions","title":"Functions","text":""},{"location":"api/core/specification/#upir.core.specification.FormalSpecification.validate","title":"<code>validate()</code>","text":"<p>Validate specification consistency and well-formedness.</p> <p>Checks performed: 1. No duplicate invariants (same operator and predicate) 2. No duplicate properties 3. All constraints have valid fields (min, max, or equals) 4. Constraint values are valid (min &lt;= max if both present) 5. No empty predicates in temporal properties</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if specification is valid</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If specification has consistency issues</p> Example <p>spec = FormalSpecification( ...     invariants=[ ...         TemporalProperty( ...             operator=TemporalOperator.ALWAYS, ...             predicate=\"data_consistent\" ...         ) ...     ] ... ) spec.validate() True</p> Source code in <code>upir/core/specification.py</code> <pre><code>def validate(self) -&gt; bool:\n    \"\"\"\n    Validate specification consistency and well-formedness.\n\n    Checks performed:\n    1. No duplicate invariants (same operator and predicate)\n    2. No duplicate properties\n    3. All constraints have valid fields (min, max, or equals)\n    4. Constraint values are valid (min &lt;= max if both present)\n    5. No empty predicates in temporal properties\n\n    Returns:\n        True if specification is valid\n\n    Raises:\n        ValueError: If specification has consistency issues\n\n    Example:\n        &gt;&gt;&gt; spec = FormalSpecification(\n        ...     invariants=[\n        ...         TemporalProperty(\n        ...             operator=TemporalOperator.ALWAYS,\n        ...             predicate=\"data_consistent\"\n        ...         )\n        ...     ]\n        ... )\n        &gt;&gt;&gt; spec.validate()\n        True\n    \"\"\"\n    # Check for duplicate invariants\n    seen_invariants = set()\n    for inv in self.invariants:\n        key = (inv.operator, inv.predicate)\n        if key in seen_invariants:\n            raise ValueError(\n                f\"Duplicate invariant: {inv.operator.value}({inv.predicate})\"\n            )\n        seen_invariants.add(key)\n\n    # Check for duplicate properties\n    seen_properties = set()\n    for prop in self.properties:\n        key = (prop.operator, prop.predicate)\n        if key in seen_properties:\n            raise ValueError(\n                f\"Duplicate property: {prop.operator.value}({prop.predicate})\"\n            )\n        seen_properties.add(key)\n\n    # Validate constraints\n    for resource_name, constraint in self.constraints.items():\n        if not isinstance(constraint, dict):\n            raise ValueError(\n                f\"Constraint '{resource_name}' must be a dictionary\"\n            )\n\n        # Check that constraint has at least one valid field\n        valid_fields = {\"min\", \"max\", \"equals\"}\n        constraint_fields = set(constraint.keys())\n        if not constraint_fields.intersection(valid_fields):\n            raise ValueError(\n                f\"Constraint '{resource_name}' must have at least one of: \"\n                f\"min, max, equals. Got: {constraint_fields}\"\n            )\n\n        # Check that invalid fields are not present\n        invalid_fields = constraint_fields - valid_fields\n        if invalid_fields:\n            raise ValueError(\n                f\"Constraint '{resource_name}' has invalid fields: {invalid_fields}. \"\n                f\"Valid fields are: {valid_fields}\"\n            )\n\n        # Validate min &lt;= max if both present\n        if \"min\" in constraint and \"max\" in constraint:\n            min_val = constraint[\"min\"]\n            max_val = constraint[\"max\"]\n            if min_val &gt; max_val:\n                raise ValueError(\n                    f\"Constraint '{resource_name}': min ({min_val}) &gt; max ({max_val})\"\n                )\n\n        # If 'equals' is present, it should be the only field\n        if \"equals\" in constraint and len(constraint) &gt; 1:\n            raise ValueError(\n                f\"Constraint '{resource_name}': 'equals' cannot be combined \"\n                f\"with min/max\"\n            )\n\n    return True\n</code></pre>"},{"location":"api/core/specification/#upir.core.specification.FormalSpecification.to_dict","title":"<code>to_dict()</code>","text":"<p>Serialize formal specification to JSON-compatible dictionary.</p> <p>Uses deterministic ordering (sorted keys) to ensure consistent serialization for hashing.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with all specification fields</p> Example <p>spec = FormalSpecification( ...     constraints={\"latency\": {\"max\": 100}} ... ) d = spec.to_dict() d[\"constraints\"][\"latency\"][\"max\"] 100</p> Source code in <code>upir/core/specification.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Serialize formal specification to JSON-compatible dictionary.\n\n    Uses deterministic ordering (sorted keys) to ensure consistent\n    serialization for hashing.\n\n    Returns:\n        Dictionary with all specification fields\n\n    Example:\n        &gt;&gt;&gt; spec = FormalSpecification(\n        ...     constraints={\"latency\": {\"max\": 100}}\n        ... )\n        &gt;&gt;&gt; d = spec.to_dict()\n        &gt;&gt;&gt; d[\"constraints\"][\"latency\"][\"max\"]\n        100\n    \"\"\"\n    return {\n        \"invariants\": [inv.to_dict() for inv in self.invariants],\n        \"properties\": [prop.to_dict() for prop in self.properties],\n        \"constraints\": {\n            k: dict(v) for k, v in sorted(self.constraints.items())\n        },\n        \"assumptions\": sorted(self.assumptions)\n    }\n</code></pre>"},{"location":"api/core/specification/#upir.core.specification.FormalSpecification.to_json","title":"<code>to_json()</code>","text":"<p>Serialize formal specification to JSON string.</p> <p>Returns:</p> Type Description <code>str</code> <p>JSON string representation</p> Example <p>spec = FormalSpecification( ...     constraints={\"latency\": {\"max\": 100}} ... ) json_str = spec.to_json() isinstance(json_str, str) True</p> Source code in <code>upir/core/specification.py</code> <pre><code>def to_json(self) -&gt; str:\n    \"\"\"\n    Serialize formal specification to JSON string.\n\n    Returns:\n        JSON string representation\n\n    Example:\n        &gt;&gt;&gt; spec = FormalSpecification(\n        ...     constraints={\"latency\": {\"max\": 100}}\n        ... )\n        &gt;&gt;&gt; json_str = spec.to_json()\n        &gt;&gt;&gt; isinstance(json_str, str)\n        True\n    \"\"\"\n    return json.dumps(self.to_dict(), indent=2)\n</code></pre>"},{"location":"api/core/specification/#upir.core.specification.FormalSpecification.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Deserialize formal specification from dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>Dictionary containing specification fields</p> required <p>Returns:</p> Type Description <code>FormalSpecification</code> <p>FormalSpecification instance</p> Example <p>data = { ...     \"invariants\": [], ...     \"properties\": [], ...     \"constraints\": {\"latency\": {\"max\": 100}}, ...     \"assumptions\": [\"network_reliable\"] ... } spec = FormalSpecification.from_dict(data) spec.constraints[\"latency\"][\"max\"] 100</p> Source code in <code>upir/core/specification.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, Any]) -&gt; \"FormalSpecification\":\n    \"\"\"\n    Deserialize formal specification from dictionary.\n\n    Args:\n        data: Dictionary containing specification fields\n\n    Returns:\n        FormalSpecification instance\n\n    Example:\n        &gt;&gt;&gt; data = {\n        ...     \"invariants\": [],\n        ...     \"properties\": [],\n        ...     \"constraints\": {\"latency\": {\"max\": 100}},\n        ...     \"assumptions\": [\"network_reliable\"]\n        ... }\n        &gt;&gt;&gt; spec = FormalSpecification.from_dict(data)\n        &gt;&gt;&gt; spec.constraints[\"latency\"][\"max\"]\n        100\n    \"\"\"\n    return cls(\n        invariants=[\n            TemporalProperty.from_dict(inv)\n            for inv in data.get(\"invariants\", [])\n        ],\n        properties=[\n            TemporalProperty.from_dict(prop)\n            for prop in data.get(\"properties\", [])\n        ],\n        constraints=data.get(\"constraints\", {}),\n        assumptions=data.get(\"assumptions\", [])\n    )\n</code></pre>"},{"location":"api/core/specification/#upir.core.specification.FormalSpecification.hash","title":"<code>hash()</code>","text":"<p>Compute SHA-256 hash of specification for integrity verification.</p> <p>Uses deterministic JSON serialization (sorted keys) to ensure consistent hashes across runs and platforms.</p> <p>Returns:</p> Type Description <code>str</code> <p>Hexadecimal string of SHA-256 hash</p> Example <p>spec1 = FormalSpecification( ...     constraints={\"latency\": {\"max\": 100}} ... ) spec2 = FormalSpecification( ...     constraints={\"latency\": {\"max\": 100}} ... ) spec1.hash() == spec2.hash() True</p> <p>References: - Python hashlib: https://docs.python.org/3/library/hashlib.html - SHA-256: Industry standard cryptographic hash</p> Source code in <code>upir/core/specification.py</code> <pre><code>def hash(self) -&gt; str:\n    \"\"\"\n    Compute SHA-256 hash of specification for integrity verification.\n\n    Uses deterministic JSON serialization (sorted keys) to ensure\n    consistent hashes across runs and platforms.\n\n    Returns:\n        Hexadecimal string of SHA-256 hash\n\n    Example:\n        &gt;&gt;&gt; spec1 = FormalSpecification(\n        ...     constraints={\"latency\": {\"max\": 100}}\n        ... )\n        &gt;&gt;&gt; spec2 = FormalSpecification(\n        ...     constraints={\"latency\": {\"max\": 100}}\n        ... )\n        &gt;&gt;&gt; spec1.hash() == spec2.hash()\n        True\n\n    References:\n    - Python hashlib: https://docs.python.org/3/library/hashlib.html\n    - SHA-256: Industry standard cryptographic hash\n    \"\"\"\n    # Convert to JSON with sorted keys for deterministic serialization\n    spec_dict = self.to_dict()\n    json_str = json.dumps(spec_dict, sort_keys=True)\n\n    # Compute SHA-256 hash\n    hash_obj = hashlib.sha256(json_str.encode('utf-8'))\n    return hash_obj.hexdigest()\n</code></pre>"},{"location":"api/core/specification/#upir.core.specification.FormalSpecification.__str__","title":"<code>__str__()</code>","text":"<p>Human-readable string representation.</p> Source code in <code>upir/core/specification.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Human-readable string representation.\"\"\"\n    parts = []\n    if self.invariants:\n        parts.append(f\"{len(self.invariants)} invariant(s)\")\n    if self.properties:\n        parts.append(f\"{len(self.properties)} propertie(s)\")\n    if self.constraints:\n        parts.append(f\"{len(self.constraints)} constraint(s)\")\n    if self.assumptions:\n        parts.append(f\"{len(self.assumptions)} assumption(s)\")\n\n    if not parts:\n        return \"FormalSpecification(empty)\"\n\n    return f\"FormalSpecification({', '.join(parts)})\"\n</code></pre>"},{"location":"api/core/specification/#upir.core.specification.FormalSpecification.__repr__","title":"<code>__repr__()</code>","text":"<p>Developer-friendly representation.</p> Source code in <code>upir/core/specification.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Developer-friendly representation.\"\"\"\n    return (\n        f\"FormalSpecification(\"\n        f\"invariants={len(self.invariants)}, \"\n        f\"properties={len(self.properties)}, \"\n        f\"constraints={len(self.constraints)}, \"\n        f\"assumptions={len(self.assumptions)})\"\n    )\n</code></pre>"},{"location":"api/core/specification/#usage-example","title":"Usage Example","text":"<pre><code>from upir.core.specification import FormalSpecification\nfrom upir.core.temporal import TemporalProperty, TemporalOperator\n\n# Define temporal properties\nproperties = [\n    TemporalProperty(\n        operator=TemporalOperator.EVENTUALLY,\n        predicate=\"task_complete\",\n        time_bound=60000  # 60 seconds\n    ),\n    TemporalProperty(\n        operator=TemporalOperator.WITHIN,\n        predicate=\"respond\",\n        time_bound=100  # 100ms\n    )\n]\n\n# Define invariants\ninvariants = [\n    TemporalProperty(\n        operator=TemporalOperator.ALWAYS,\n        predicate=\"data_consistent\"\n    ),\n    TemporalProperty(\n        operator=TemporalOperator.ALWAYS,\n        predicate=\"no_data_loss\"\n    )\n]\n\n# Define constraints\nconstraints = {\n    \"latency_p99\": {\"max\": 100.0},\n    \"monthly_cost\": {\"max\": 5000.0},\n    \"throughput_qps\": {\"min\": 10000.0}\n}\n\n# Create specification\nspec = FormalSpecification(\n    properties=properties,\n    invariants=invariants,\n    constraints=constraints\n)\n\n# Serialize\nspec_json = spec.to_json()\n</code></pre>"},{"location":"api/core/specification/#constraint-schema","title":"Constraint Schema","text":"<p>Constraints are dictionaries with min/max bounds:</p> <pre><code>constraints = {\n    \"metric_name\": {\n        \"min\": 100.0,  # Minimum value (optional)\n        \"max\": 1000.0  # Maximum value (optional)\n    }\n}\n</code></pre>"},{"location":"api/core/specification/#see-also","title":"See Also","text":"<ul> <li>TemporalProperty - Temporal logic properties</li> <li>UPIR - Main UPIR class</li> <li>Verifier - Verify specifications</li> </ul>"},{"location":"api/core/temporal/","title":"Temporal Logic","text":"<p>Temporal properties using Linear Temporal Logic (LTL).</p>"},{"location":"api/core/temporal/#overview","title":"Overview","text":"<p>UPIR uses Linear Temporal Logic to express properties that must hold over time:</p> <ul> <li>TemporalOperator: Enum of LTL operators</li> <li>TemporalProperty: Property with operator and predicate</li> </ul>"},{"location":"api/core/temporal/#temporaloperator","title":"TemporalOperator","text":""},{"location":"api/core/temporal/#upir.core.temporal.TemporalOperator","title":"<code>upir.core.temporal.TemporalOperator</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Linear Temporal Logic (LTL) operators for expressing temporal properties.</p> <p>Based on Pnueli's temporal logic formalism, these operators allow expressing properties about all time points (ALWAYS), some future time point (EVENTUALLY), bounded time constraints (WITHIN), and sequential properties (UNTIL).</p> <p>References: - Pnueli (1977): Standard LTL operators - TD Commons disclosure: UPIR temporal property specification</p> Source code in <code>upir/core/temporal.py</code> <pre><code>class TemporalOperator(Enum):\n    \"\"\"\n    Linear Temporal Logic (LTL) operators for expressing temporal properties.\n\n    Based on Pnueli's temporal logic formalism, these operators allow\n    expressing properties about all time points (ALWAYS), some future time\n    point (EVENTUALLY), bounded time constraints (WITHIN), and sequential\n    properties (UNTIL).\n\n    References:\n    - Pnueli (1977): Standard LTL operators\n    - TD Commons disclosure: UPIR temporal property specification\n    \"\"\"\n    ALWAYS = \"ALWAYS\"  # \u25fbP or G(P) - Property holds at all time points\n    EVENTUALLY = \"EVENTUALLY\"  # \u25c7P or F(P) - Property holds at some future point\n    WITHIN = \"WITHIN\"  # Bounded eventually - Property holds within time bound\n    UNTIL = \"UNTIL\"  # P U Q - P holds until Q becomes true\n</code></pre>"},{"location":"api/core/temporal/#temporalproperty","title":"TemporalProperty","text":""},{"location":"api/core/temporal/#upir.core.temporal.TemporalProperty","title":"<code>upir.core.temporal.TemporalProperty</code>  <code>dataclass</code>","text":"<p>A temporal property with formal semantics for distributed system verification.</p> <p>Temporal properties express constraints that must hold over time, such as \"the system ALWAYS responds to requests\" or \"backups complete WITHIN 1 hour\".</p> <p>Based on Linear Temporal Logic (LTL) as defined in Pnueli (1977), extended with bounded operators for practical system verification per TD Commons.</p> <p>Attributes:</p> Name Type Description <code>operator</code> <code>TemporalOperator</code> <p>The temporal operator (ALWAYS, EVENTUALLY, WITHIN, UNTIL)</p> <code>predicate</code> <code>str</code> <p>String description of the property being asserted       (e.g., \"data_consistent\", \"request_processed\")</p> <code>time_bound</code> <code>Optional[float]</code> <p>Optional time bound in seconds for bounded operators (WITHIN)</p> <code>parameters</code> <code>Dict[str, Any]</code> <p>Additional parameters for property evaluation (e.g., thresholds)</p> Example <p>References: - TD Commons: Temporal property structure - Pnueli (1977): LTL semantics</p> Source code in <code>upir/core/temporal.py</code> <pre><code>@dataclass\nclass TemporalProperty:\n    \"\"\"\n    A temporal property with formal semantics for distributed system verification.\n\n    Temporal properties express constraints that must hold over time, such as\n    \"the system ALWAYS responds to requests\" or \"backups complete WITHIN 1 hour\".\n\n    Based on Linear Temporal Logic (LTL) as defined in Pnueli (1977), extended\n    with bounded operators for practical system verification per TD Commons.\n\n    Attributes:\n        operator: The temporal operator (ALWAYS, EVENTUALLY, WITHIN, UNTIL)\n        predicate: String description of the property being asserted\n                  (e.g., \"data_consistent\", \"request_processed\")\n        time_bound: Optional time bound in seconds for bounded operators (WITHIN)\n        parameters: Additional parameters for property evaluation (e.g., thresholds)\n\n    Example:\n        &gt;&gt;&gt; # System always responds within 100ms\n        &gt;&gt;&gt; prop = TemporalProperty(\n        ...     operator=TemporalOperator.WITHIN,\n        ...     predicate=\"response_received\",\n        ...     time_bound=0.1,\n        ...     parameters={\"max_latency_ms\": 100}\n        ... )\n\n    References:\n    - TD Commons: Temporal property structure\n    - Pnueli (1977): LTL semantics\n    \"\"\"\n    operator: TemporalOperator\n    predicate: str\n    time_bound: Optional[float] = None\n    parameters: Dict[str, Any] = field(default_factory=dict)\n\n    def __post_init__(self):\n        \"\"\"Validate temporal property constraints.\"\"\"\n        if self.operator == TemporalOperator.WITHIN and self.time_bound is None:\n            raise ValueError(\"WITHIN operator requires a time_bound\")\n\n        if self.time_bound is not None and self.time_bound &lt;= 0:\n            raise ValueError(\"time_bound must be positive\")\n\n        if not self.predicate:\n            raise ValueError(\"predicate cannot be empty\")\n\n    def to_smt(self) -&gt; str:\n        \"\"\"\n        Convert temporal property to SMT-LIB format for Z3 solver.\n\n        Encoding follows standard temporal logic to first-order logic translation:\n        - ALWAYS P: \u2200t. P(t) - Universal quantification over time\n        - EVENTUALLY P: \u2203t. P(t) - Existential quantification over time\n        - WITHIN P (bound b): \u2203t. (t \u2264 b) \u2227 P(t) - Bounded existential\n        - P UNTIL Q: \u2203t. Q(t) \u2227 \u2200s. (s &lt; t) \u2192 P(s) - Q eventually holds, P until then\n\n        Returns:\n            SMT-LIB formatted string suitable for Z3\n\n        References:\n        - Z3 tutorial: https://microsoft.github.io/z3guide/\n        - Temporal logic encoding: Standard translation to FOL\n\n        Example:\n            &gt;&gt;&gt; prop = TemporalProperty(\n            ...     operator=TemporalOperator.ALWAYS,\n            ...     predicate=\"data_consistent\"\n            ... )\n            &gt;&gt;&gt; smt = prop.to_smt()\n            &gt;&gt;&gt; \"forall\" in smt\n            True\n        \"\"\"\n        pred_name = self.predicate\n\n        if self.operator == TemporalOperator.ALWAYS:\n            # \u2200t. P(t) - Property holds at all time points\n            return f\"(forall ((t Real)) ({pred_name} t))\"\n\n        elif self.operator == TemporalOperator.EVENTUALLY:\n            # \u2203t. P(t) - Property holds at some future time point\n            return f\"(exists ((t Real)) ({pred_name} t))\"\n\n        elif self.operator == TemporalOperator.WITHIN:\n            # \u2203t. (t \u2264 bound) \u2227 P(t) - Property holds within time bound\n            bound = self.time_bound\n            return f\"(exists ((t Real)) (and (&lt;= t {bound}) ({pred_name} t)))\"\n\n        elif self.operator == TemporalOperator.UNTIL:\n            # P UNTIL Q encoding: \u2203t. Q(t) \u2227 \u2200s. (s &lt; t) \u2192 P(s)\n            # Note: For UNTIL, predicate should contain both P and Q predicates\n            # This is a simplified encoding; full UNTIL may need parameters\n            return (\n                f\"(exists ((t Real)) \"\n                f\"(and ({pred_name}_q t) \"\n                f\"(forall ((s Real)) (=&gt; (&lt; s t) ({pred_name}_p s)))))\"\n            )\n\n        else:\n            raise ValueError(f\"Unknown temporal operator: {self.operator}\")\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Serialize temporal property to JSON-compatible dictionary.\n\n        Returns:\n            Dictionary with all property fields in serializable format\n\n        Example:\n            &gt;&gt;&gt; prop = TemporalProperty(\n            ...     operator=TemporalOperator.WITHIN,\n            ...     predicate=\"backup_complete\",\n            ...     time_bound=3600.0,\n            ...     parameters={\"backup_type\": \"full\"}\n            ... )\n            &gt;&gt;&gt; d = prop.to_dict()\n            &gt;&gt;&gt; d[\"operator\"]\n            'WITHIN'\n        \"\"\"\n        return {\n            \"operator\": self.operator.value,\n            \"predicate\": self.predicate,\n            \"time_bound\": self.time_bound,\n            \"parameters\": self.parameters.copy()\n        }\n\n    def hash(self) -&gt; str:\n        \"\"\"\n        Generate SHA-256 hash of this temporal property.\n\n        Uses deterministic JSON serialization to ensure same property\n        always produces the same hash, enabling caching and integrity checks.\n\n        Returns:\n            Hexadecimal SHA-256 hash string\n\n        Example:\n            &gt;&gt;&gt; prop = TemporalProperty(\n            ...     operator=TemporalOperator.ALWAYS,\n            ...     predicate=\"test\"\n            ... )\n            &gt;&gt;&gt; hash1 = prop.hash()\n            &gt;&gt;&gt; hash2 = prop.hash()\n            &gt;&gt;&gt; hash1 == hash2  # Deterministic\n            True\n\n        References:\n        - SHA-256: Industry standard cryptographic hash\n        - Python hashlib: https://docs.python.org/3/library/hashlib.html\n        \"\"\"\n        prop_dict = self.to_dict()\n        json_str = json.dumps(prop_dict, sort_keys=True)\n        hash_obj = hashlib.sha256(json_str.encode('utf-8'))\n        return hash_obj.hexdigest()\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -&gt; \"TemporalProperty\":\n        \"\"\"\n        Deserialize temporal property from dictionary.\n\n        Args:\n            data: Dictionary containing property fields\n\n        Returns:\n            TemporalProperty instance\n\n        Raises:\n            ValueError: If operator is invalid or required fields missing\n\n        Example:\n            &gt;&gt;&gt; data = {\n            ...     \"operator\": \"ALWAYS\",\n            ...     \"predicate\": \"data_consistent\",\n            ...     \"time_bound\": None,\n            ...     \"parameters\": {}\n            ... }\n            &gt;&gt;&gt; prop = TemporalProperty.from_dict(data)\n            &gt;&gt;&gt; prop.operator == TemporalOperator.ALWAYS\n            True\n        \"\"\"\n        return cls(\n            operator=TemporalOperator(data[\"operator\"]),\n            predicate=data[\"predicate\"],\n            time_bound=data.get(\"time_bound\"),\n            parameters=data.get(\"parameters\", {})\n        )\n\n    def __str__(self) -&gt; str:\n        \"\"\"Human-readable string representation.\"\"\"\n        if self.time_bound is not None:\n            return f\"{self.operator.value}[{self.time_bound}s]({self.predicate})\"\n        return f\"{self.operator.value}({self.predicate})\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Developer-friendly representation.\"\"\"\n        return (\n            f\"TemporalProperty(operator={self.operator}, \"\n            f\"predicate='{self.predicate}', \"\n            f\"time_bound={self.time_bound}, \"\n            f\"parameters={self.parameters})\"\n        )\n</code></pre>"},{"location":"api/core/temporal/#upir.core.temporal.TemporalProperty--system-always-responds-within-100ms","title":"System always responds within 100ms","text":"<p>prop = TemporalProperty( ...     operator=TemporalOperator.WITHIN, ...     predicate=\"response_received\", ...     time_bound=0.1, ...     parameters={\"max_latency_ms\": 100} ... )</p>"},{"location":"api/core/temporal/#upir.core.temporal.TemporalProperty-functions","title":"Functions","text":""},{"location":"api/core/temporal/#upir.core.temporal.TemporalProperty.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate temporal property constraints.</p> Source code in <code>upir/core/temporal.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate temporal property constraints.\"\"\"\n    if self.operator == TemporalOperator.WITHIN and self.time_bound is None:\n        raise ValueError(\"WITHIN operator requires a time_bound\")\n\n    if self.time_bound is not None and self.time_bound &lt;= 0:\n        raise ValueError(\"time_bound must be positive\")\n\n    if not self.predicate:\n        raise ValueError(\"predicate cannot be empty\")\n</code></pre>"},{"location":"api/core/temporal/#upir.core.temporal.TemporalProperty.to_smt","title":"<code>to_smt()</code>","text":"<p>Convert temporal property to SMT-LIB format for Z3 solver.</p> <p>Encoding follows standard temporal logic to first-order logic translation: - ALWAYS P: \u2200t. P(t) - Universal quantification over time - EVENTUALLY P: \u2203t. P(t) - Existential quantification over time - WITHIN P (bound b): \u2203t. (t \u2264 b) \u2227 P(t) - Bounded existential - P UNTIL Q: \u2203t. Q(t) \u2227 \u2200s. (s &lt; t) \u2192 P(s) - Q eventually holds, P until then</p> <p>Returns:</p> Type Description <code>str</code> <p>SMT-LIB formatted string suitable for Z3</p> <p>References: - Z3 tutorial: https://microsoft.github.io/z3guide/ - Temporal logic encoding: Standard translation to FOL</p> Example <p>prop = TemporalProperty( ...     operator=TemporalOperator.ALWAYS, ...     predicate=\"data_consistent\" ... ) smt = prop.to_smt() \"forall\" in smt True</p> Source code in <code>upir/core/temporal.py</code> <pre><code>def to_smt(self) -&gt; str:\n    \"\"\"\n    Convert temporal property to SMT-LIB format for Z3 solver.\n\n    Encoding follows standard temporal logic to first-order logic translation:\n    - ALWAYS P: \u2200t. P(t) - Universal quantification over time\n    - EVENTUALLY P: \u2203t. P(t) - Existential quantification over time\n    - WITHIN P (bound b): \u2203t. (t \u2264 b) \u2227 P(t) - Bounded existential\n    - P UNTIL Q: \u2203t. Q(t) \u2227 \u2200s. (s &lt; t) \u2192 P(s) - Q eventually holds, P until then\n\n    Returns:\n        SMT-LIB formatted string suitable for Z3\n\n    References:\n    - Z3 tutorial: https://microsoft.github.io/z3guide/\n    - Temporal logic encoding: Standard translation to FOL\n\n    Example:\n        &gt;&gt;&gt; prop = TemporalProperty(\n        ...     operator=TemporalOperator.ALWAYS,\n        ...     predicate=\"data_consistent\"\n        ... )\n        &gt;&gt;&gt; smt = prop.to_smt()\n        &gt;&gt;&gt; \"forall\" in smt\n        True\n    \"\"\"\n    pred_name = self.predicate\n\n    if self.operator == TemporalOperator.ALWAYS:\n        # \u2200t. P(t) - Property holds at all time points\n        return f\"(forall ((t Real)) ({pred_name} t))\"\n\n    elif self.operator == TemporalOperator.EVENTUALLY:\n        # \u2203t. P(t) - Property holds at some future time point\n        return f\"(exists ((t Real)) ({pred_name} t))\"\n\n    elif self.operator == TemporalOperator.WITHIN:\n        # \u2203t. (t \u2264 bound) \u2227 P(t) - Property holds within time bound\n        bound = self.time_bound\n        return f\"(exists ((t Real)) (and (&lt;= t {bound}) ({pred_name} t)))\"\n\n    elif self.operator == TemporalOperator.UNTIL:\n        # P UNTIL Q encoding: \u2203t. Q(t) \u2227 \u2200s. (s &lt; t) \u2192 P(s)\n        # Note: For UNTIL, predicate should contain both P and Q predicates\n        # This is a simplified encoding; full UNTIL may need parameters\n        return (\n            f\"(exists ((t Real)) \"\n            f\"(and ({pred_name}_q t) \"\n            f\"(forall ((s Real)) (=&gt; (&lt; s t) ({pred_name}_p s)))))\"\n        )\n\n    else:\n        raise ValueError(f\"Unknown temporal operator: {self.operator}\")\n</code></pre>"},{"location":"api/core/temporal/#upir.core.temporal.TemporalProperty.to_dict","title":"<code>to_dict()</code>","text":"<p>Serialize temporal property to JSON-compatible dictionary.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with all property fields in serializable format</p> Example <p>prop = TemporalProperty( ...     operator=TemporalOperator.WITHIN, ...     predicate=\"backup_complete\", ...     time_bound=3600.0, ...     parameters={\"backup_type\": \"full\"} ... ) d = prop.to_dict() d[\"operator\"] 'WITHIN'</p> Source code in <code>upir/core/temporal.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Serialize temporal property to JSON-compatible dictionary.\n\n    Returns:\n        Dictionary with all property fields in serializable format\n\n    Example:\n        &gt;&gt;&gt; prop = TemporalProperty(\n        ...     operator=TemporalOperator.WITHIN,\n        ...     predicate=\"backup_complete\",\n        ...     time_bound=3600.0,\n        ...     parameters={\"backup_type\": \"full\"}\n        ... )\n        &gt;&gt;&gt; d = prop.to_dict()\n        &gt;&gt;&gt; d[\"operator\"]\n        'WITHIN'\n    \"\"\"\n    return {\n        \"operator\": self.operator.value,\n        \"predicate\": self.predicate,\n        \"time_bound\": self.time_bound,\n        \"parameters\": self.parameters.copy()\n    }\n</code></pre>"},{"location":"api/core/temporal/#upir.core.temporal.TemporalProperty.hash","title":"<code>hash()</code>","text":"<p>Generate SHA-256 hash of this temporal property.</p> <p>Uses deterministic JSON serialization to ensure same property always produces the same hash, enabling caching and integrity checks.</p> <p>Returns:</p> Type Description <code>str</code> <p>Hexadecimal SHA-256 hash string</p> Example <p>prop = TemporalProperty( ...     operator=TemporalOperator.ALWAYS, ...     predicate=\"test\" ... ) hash1 = prop.hash() hash2 = prop.hash() hash1 == hash2  # Deterministic True</p> <p>References: - SHA-256: Industry standard cryptographic hash - Python hashlib: https://docs.python.org/3/library/hashlib.html</p> Source code in <code>upir/core/temporal.py</code> <pre><code>def hash(self) -&gt; str:\n    \"\"\"\n    Generate SHA-256 hash of this temporal property.\n\n    Uses deterministic JSON serialization to ensure same property\n    always produces the same hash, enabling caching and integrity checks.\n\n    Returns:\n        Hexadecimal SHA-256 hash string\n\n    Example:\n        &gt;&gt;&gt; prop = TemporalProperty(\n        ...     operator=TemporalOperator.ALWAYS,\n        ...     predicate=\"test\"\n        ... )\n        &gt;&gt;&gt; hash1 = prop.hash()\n        &gt;&gt;&gt; hash2 = prop.hash()\n        &gt;&gt;&gt; hash1 == hash2  # Deterministic\n        True\n\n    References:\n    - SHA-256: Industry standard cryptographic hash\n    - Python hashlib: https://docs.python.org/3/library/hashlib.html\n    \"\"\"\n    prop_dict = self.to_dict()\n    json_str = json.dumps(prop_dict, sort_keys=True)\n    hash_obj = hashlib.sha256(json_str.encode('utf-8'))\n    return hash_obj.hexdigest()\n</code></pre>"},{"location":"api/core/temporal/#upir.core.temporal.TemporalProperty.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Deserialize temporal property from dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>Dictionary containing property fields</p> required <p>Returns:</p> Type Description <code>TemporalProperty</code> <p>TemporalProperty instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If operator is invalid or required fields missing</p> Example <p>data = { ...     \"operator\": \"ALWAYS\", ...     \"predicate\": \"data_consistent\", ...     \"time_bound\": None, ...     \"parameters\": {} ... } prop = TemporalProperty.from_dict(data) prop.operator == TemporalOperator.ALWAYS True</p> Source code in <code>upir/core/temporal.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, Any]) -&gt; \"TemporalProperty\":\n    \"\"\"\n    Deserialize temporal property from dictionary.\n\n    Args:\n        data: Dictionary containing property fields\n\n    Returns:\n        TemporalProperty instance\n\n    Raises:\n        ValueError: If operator is invalid or required fields missing\n\n    Example:\n        &gt;&gt;&gt; data = {\n        ...     \"operator\": \"ALWAYS\",\n        ...     \"predicate\": \"data_consistent\",\n        ...     \"time_bound\": None,\n        ...     \"parameters\": {}\n        ... }\n        &gt;&gt;&gt; prop = TemporalProperty.from_dict(data)\n        &gt;&gt;&gt; prop.operator == TemporalOperator.ALWAYS\n        True\n    \"\"\"\n    return cls(\n        operator=TemporalOperator(data[\"operator\"]),\n        predicate=data[\"predicate\"],\n        time_bound=data.get(\"time_bound\"),\n        parameters=data.get(\"parameters\", {})\n    )\n</code></pre>"},{"location":"api/core/temporal/#upir.core.temporal.TemporalProperty.__str__","title":"<code>__str__()</code>","text":"<p>Human-readable string representation.</p> Source code in <code>upir/core/temporal.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Human-readable string representation.\"\"\"\n    if self.time_bound is not None:\n        return f\"{self.operator.value}[{self.time_bound}s]({self.predicate})\"\n    return f\"{self.operator.value}({self.predicate})\"\n</code></pre>"},{"location":"api/core/temporal/#upir.core.temporal.TemporalProperty.__repr__","title":"<code>__repr__()</code>","text":"<p>Developer-friendly representation.</p> Source code in <code>upir/core/temporal.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Developer-friendly representation.\"\"\"\n    return (\n        f\"TemporalProperty(operator={self.operator}, \"\n        f\"predicate='{self.predicate}', \"\n        f\"time_bound={self.time_bound}, \"\n        f\"parameters={self.parameters})\"\n    )\n</code></pre>"},{"location":"api/core/temporal/#usage-examples","title":"Usage Examples","text":""},{"location":"api/core/temporal/#always-operator","title":"ALWAYS Operator","text":"<p>Property must hold at all times:</p> <pre><code>from upir.core.temporal import TemporalOperator, TemporalProperty\n\nalways_consistent = TemporalProperty(\n    operator=TemporalOperator.ALWAYS,\n    predicate=\"data_consistent\"\n)\n\n# SMT encoding: \u2200t. data_consistent(t)\n</code></pre>"},{"location":"api/core/temporal/#eventually-operator","title":"EVENTUALLY Operator","text":"<p>Property must eventually hold:</p> <pre><code>eventually_complete = TemporalProperty(\n    operator=TemporalOperator.EVENTUALLY,\n    predicate=\"all_tasks_complete\",\n    time_bound=60000  # within 60 seconds\n)\n\n# SMT encoding: \u2203t. (t \u2264 60000) \u2227 all_tasks_complete(t)\n</code></pre>"},{"location":"api/core/temporal/#within-operator","title":"WITHIN Operator","text":"<p>Property must occur within time bound:</p> <pre><code>within_100ms = TemporalProperty(\n    operator=TemporalOperator.WITHIN,\n    predicate=\"respond\",\n    time_bound=100\n)\n\n# SMT encoding: \u2203t. (t \u2264 100) \u2227 respond(t)\n</code></pre>"},{"location":"api/core/temporal/#until-operator","title":"UNTIL Operator","text":"<p>Property P holds until Q becomes true:</p> <pre><code>until_complete = TemporalProperty(\n    operator=TemporalOperator.UNTIL,\n    predicate=\"processing\",\n    time_bound=30000  # max 30 seconds\n)\n\n# SMT encoding: \u2203t. (t \u2264 30000) \u2227 (\u2200s &lt; t. processing(s)) \u2227 complete(t)\n</code></pre>"},{"location":"api/core/temporal/#smt-encoding","title":"SMT Encoding","text":"<p>Convert temporal properties to SMT formulas for verification:</p> <pre><code>property = TemporalProperty(\n    operator=TemporalOperator.ALWAYS,\n    predicate=\"data_consistent\"\n)\n\n# Get SMT encoding\nsmt_formula = property.to_smt()\nprint(smt_formula)\n# Output: \"(forall ((t Int)) (&gt;= t 0) (data_consistent t))\"\n</code></pre>"},{"location":"api/core/temporal/#see-also","title":"See Also","text":"<ul> <li>FormalSpecification - Combine properties into specs</li> <li>Verifier - Verify temporal properties</li> </ul>"},{"location":"api/core/upir/","title":"UPIR","text":"<p>The main <code>UPIR</code> class that combines specifications and architectures.</p>"},{"location":"api/core/upir/#overview","title":"Overview","text":"<p>The <code>UPIR</code> class is the core abstraction that ties together:</p> <ul> <li>Formal specifications (temporal properties and constraints)</li> <li>System architecture (components and connections)</li> <li>Metadata and configuration</li> </ul>"},{"location":"api/core/upir/#class-documentation","title":"Class Documentation","text":""},{"location":"api/core/upir/#upir.core.upir.UPIR","title":"<code>upir.core.upir.UPIR</code>  <code>dataclass</code>","text":"<p>Universal Plan Intermediate Representation for distributed systems.</p> <p>UPIR integrates formal specifications, architectural designs, evidence from various sources, and reasoning graphs to enable automated synthesis, verification, and continuous optimization of distributed systems.</p> <p>Based on TD Commons disclosure, UPIR maintains: - Formal specifications (invariants, properties, constraints) - Architecture representation (components, connections, deployment) - Evidence tracking (benchmarks, tests, production data, proofs) - Reasoning graph (decisions, rationale, confidence propagation)</p> <p>Attributes:</p> Name Type Description <code>specification</code> <code>Optional[FormalSpecification]</code> <p>Formal specification with invariants and properties</p> <code>architecture</code> <code>Optional[Architecture]</code> <p>Architecture representation (components, connections)</p> <code>id</code> <code>str</code> <p>Unique identifier (auto-generated UUID if not provided)</p> <code>name</code> <code>str</code> <p>Human-readable name for this UPIR instance (default: \"\")</p> <code>description</code> <code>str</code> <p>Description of the system being represented (default: \"\")</p> <code>evidence</code> <code>Dict[str, Evidence]</code> <p>Dictionary of evidence keyed by evidence ID</p> <code>reasoning</code> <code>Dict[str, ReasoningNode]</code> <p>Dictionary of reasoning nodes keyed by node ID</p> <code>metadata</code> <code>Dict[str, Any]</code> <p>Additional metadata (tags, owner, project, etc.)</p> <code>created_at</code> <code>datetime</code> <p>When this UPIR was created (UTC)</p> <code>updated_at</code> <code>datetime</code> <p>When this UPIR was last modified (UTC)</p> Example <p>References: - TD Commons: UPIR structure and components</p> Source code in <code>upir/core/upir.py</code> <pre><code>@dataclass\nclass UPIR:\n    \"\"\"\n    Universal Plan Intermediate Representation for distributed systems.\n\n    UPIR integrates formal specifications, architectural designs, evidence\n    from various sources, and reasoning graphs to enable automated synthesis,\n    verification, and continuous optimization of distributed systems.\n\n    Based on TD Commons disclosure, UPIR maintains:\n    - Formal specifications (invariants, properties, constraints)\n    - Architecture representation (components, connections, deployment)\n    - Evidence tracking (benchmarks, tests, production data, proofs)\n    - Reasoning graph (decisions, rationale, confidence propagation)\n\n    Attributes:\n        specification: Formal specification with invariants and properties\n        architecture: Architecture representation (components, connections)\n        id: Unique identifier (auto-generated UUID if not provided)\n        name: Human-readable name for this UPIR instance (default: \"\")\n        description: Description of the system being represented (default: \"\")\n        evidence: Dictionary of evidence keyed by evidence ID\n        reasoning: Dictionary of reasoning nodes keyed by node ID\n        metadata: Additional metadata (tags, owner, project, etc.)\n        created_at: When this UPIR was created (UTC)\n        updated_at: When this UPIR was last modified (UTC)\n\n    Example:\n        &gt;&gt;&gt; # Simple usage with defaults\n        &gt;&gt;&gt; upir = UPIR(\n        ...     specification=FormalSpecification(...),\n        ...     architecture=Architecture(...)\n        ... )\n        &gt;&gt;&gt; # Or with explicit metadata\n        &gt;&gt;&gt; upir = UPIR(\n        ...     specification=spec,\n        ...     architecture=arch,\n        ...     name=\"E-commerce Platform\",\n        ...     description=\"High-throughput e-commerce system\"\n        ... )\n        &gt;&gt;&gt; upir.validate()\n        True\n\n    References:\n    - TD Commons: UPIR structure and components\n    \"\"\"\n    specification: Optional[FormalSpecification] = None\n    architecture: Optional[Architecture] = None\n    id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    name: str = \"\"\n    description: str = \"\"\n    evidence: Dict[str, Evidence] = field(default_factory=dict)\n    reasoning: Dict[str, ReasoningNode] = field(default_factory=dict)\n    metadata: Dict[str, Any] = field(default_factory=dict)\n    created_at: datetime = field(default_factory=datetime.utcnow)\n    updated_at: datetime = field(default_factory=datetime.utcnow)\n\n    def __post_init__(self):\n        \"\"\"Validate UPIR fields.\"\"\"\n        if not self.id:\n            raise ValueError(\"ID cannot be empty\")\n\n    @staticmethod\n    def generate_id() -&gt; str:\n        \"\"\"\n        Generate a unique ID for a UPIR instance.\n\n        Returns:\n            UUID string\n\n        Example:\n            &gt;&gt;&gt; upir_id = UPIR.generate_id()\n            &gt;&gt;&gt; len(upir_id) == 36  # UUID format\n            True\n        \"\"\"\n        return str(uuid.uuid4())\n\n    def add_evidence(self, evidence: Evidence) -&gt; str:\n        \"\"\"\n        Add evidence to this UPIR instance.\n\n        Generates a unique ID for the evidence and adds it to the evidence\n        dictionary. Updates the updated_at timestamp.\n\n        Args:\n            evidence: Evidence instance to add\n\n        Returns:\n            Generated evidence ID\n\n        Example:\n            &gt;&gt;&gt; upir = UPIR(id=\"upir-1\", name=\"test\", description=\"test\")\n            &gt;&gt;&gt; evidence = Evidence(\"test\", \"benchmark\", {}, 0.8)\n            &gt;&gt;&gt; evidence_id = upir.add_evidence(evidence)\n            &gt;&gt;&gt; len(evidence_id) == 36  # UUID\n            True\n            &gt;&gt;&gt; evidence_id in upir.evidence\n            True\n        \"\"\"\n        evidence_id = str(uuid.uuid4())\n        self.evidence[evidence_id] = evidence\n        self.updated_at = datetime.utcnow()\n        return evidence_id\n\n    def add_reasoning(self, node: ReasoningNode) -&gt; str:\n        \"\"\"\n        Add reasoning node to this UPIR instance.\n\n        Adds the node to the reasoning dictionary using its ID. Updates the\n        updated_at timestamp.\n\n        Args:\n            node: ReasoningNode instance to add\n\n        Returns:\n            The node's ID (from node.id)\n\n        Example:\n            &gt;&gt;&gt; upir = UPIR(id=\"upir-1\", name=\"test\", description=\"test\")\n            &gt;&gt;&gt; node = ReasoningNode(\n            ...     id=ReasoningNode.generate_id(),\n            ...     decision=\"Use PostgreSQL\",\n            ...     rationale=\"Strong consistency needed\"\n            ... )\n            &gt;&gt;&gt; node_id = upir.add_reasoning(node)\n            &gt;&gt;&gt; node_id in upir.reasoning\n            True\n        \"\"\"\n        self.reasoning[node.id] = node\n        self.updated_at = datetime.utcnow()\n        return node.id\n\n    def compute_overall_confidence(self) -&gt; float:\n        \"\"\"\n        Compute overall confidence using harmonic mean of leaf node confidences.\n\n        Leaf nodes are reasoning nodes that are not referenced as parents by\n        any other nodes - they represent final decisions or conclusions.\n\n        The harmonic mean is more conservative than arithmetic or geometric mean,\n        heavily penalizing low confidences. Formula: n / sum(1/c_i)\n\n        Returns:\n            Overall confidence in [0, 1], or 0.0 if no leaf nodes\n\n        Example:\n            &gt;&gt;&gt; upir = UPIR(id=\"upir-1\", name=\"test\", description=\"test\")\n            &gt;&gt;&gt; node1 = ReasoningNode(\"n1\", \"decision1\", \"rationale\", confidence=0.8)\n            &gt;&gt;&gt; node2 = ReasoningNode(\"n2\", \"decision2\", \"rationale\", confidence=0.9)\n            &gt;&gt;&gt; upir.add_reasoning(node1)\n            'n1'\n            &gt;&gt;&gt; upir.add_reasoning(node2)\n            'n2'\n            &gt;&gt;&gt; conf = upir.compute_overall_confidence()\n            &gt;&gt;&gt; 0.84 &lt; conf &lt; 0.85  # Harmonic mean of 0.8 and 0.9\n            True\n\n        References:\n        - Harmonic mean: More conservative aggregation for confidences\n        - Leaf nodes: Final conclusions in reasoning DAG\n        \"\"\"\n        if not self.reasoning:\n            return 0.0\n\n        # Find all nodes referenced as parents\n        referenced_nodes: Set[str] = set()\n        for node in self.reasoning.values():\n            referenced_nodes.update(node.parent_ids)\n\n        # Leaf nodes are those NOT referenced as parents\n        leaf_nodes = [\n            node for node_id, node in self.reasoning.items()\n            if node_id not in referenced_nodes\n        ]\n\n        if not leaf_nodes:\n            return 0.0\n\n        # Filter out zero confidences (would cause division by zero)\n        confidences = [node.confidence for node in leaf_nodes]\n        if any(c == 0.0 for c in confidences):\n            return 0.0\n\n        # Compute harmonic mean: n / sum(1/c_i)\n        n = len(confidences)\n        reciprocal_sum = sum(1.0 / c for c in confidences)\n        harmonic_mean = n / reciprocal_sum\n\n        return harmonic_mean\n\n    def validate(self) -&gt; bool:\n        \"\"\"\n        Validate the UPIR instance for consistency.\n\n        Performs multiple validation checks:\n        1. Specification validation (if present)\n        2. Reasoning DAG cycle detection\n        3. Evidence reference validation (all referenced evidence exists)\n\n        Returns:\n            True if all validations pass\n\n        Raises:\n            ValueError: If any validation check fails\n\n        Example:\n            &gt;&gt;&gt; upir = UPIR(\n            ...     id=\"upir-1\",\n            ...     name=\"test\",\n            ...     description=\"test\",\n            ...     specification=FormalSpecification()\n            ... )\n            &gt;&gt;&gt; upir.validate()\n            True\n\n        References:\n        - Cycle detection: DFS with recursion stack\n        - Standard graph algorithm for DAG validation\n        \"\"\"\n        # Validate specification if present\n        if self.specification is not None:\n            self.specification.validate()\n\n        # Check for cycles in reasoning DAG\n        self._check_dag_cycles()\n\n        # Validate evidence references\n        for node in self.reasoning.values():\n            for evidence_id in node.evidence_ids:\n                if evidence_id not in self.evidence:\n                    raise ValueError(\n                        f\"Reasoning node '{node.id}' references non-existent \"\n                        f\"evidence '{evidence_id}'\"\n                    )\n\n        return True\n\n    def _check_dag_cycles(self) -&gt; None:\n        \"\"\"\n        Check for cycles in the reasoning DAG using DFS.\n\n        Uses depth-first search with a recursion stack to detect cycles.\n        If a node is encountered that's already in the recursion stack,\n        a cycle exists.\n\n        Raises:\n            ValueError: If a cycle is detected\n\n        Algorithm:\n        - visited: Set of all nodes we've completely processed\n        - rec_stack: Set of nodes in current DFS path (recursion stack)\n        - For each unvisited node, do DFS\n        - If we visit a node already in rec_stack -&gt; cycle\n\n        References:\n        - Standard DFS cycle detection for directed graphs\n        - Cormen et al., \"Introduction to Algorithms\"\n        \"\"\"\n        visited: Set[str] = set()\n        rec_stack: Set[str] = set()\n\n        def dfs(node_id: str) -&gt; None:\n            \"\"\"DFS helper to detect cycles.\"\"\"\n            visited.add(node_id)\n            rec_stack.add(node_id)\n\n            # Visit all parent nodes\n            if node_id in self.reasoning:\n                node = self.reasoning[node_id]\n                for parent_id in node.parent_ids:\n                    if parent_id not in visited:\n                        # Recursively visit parent\n                        dfs(parent_id)\n                    elif parent_id in rec_stack:\n                        # Found a back edge -&gt; cycle\n                        raise ValueError(\n                            f\"Cycle detected in reasoning graph involving \"\n                            f\"node '{parent_id}'\"\n                        )\n\n            # Remove from recursion stack when done\n            rec_stack.remove(node_id)\n\n        # Check all nodes\n        for node_id in self.reasoning.keys():\n            if node_id not in visited:\n                dfs(node_id)\n\n    def generate_signature(self) -&gt; str:\n        \"\"\"\n        Generate cryptographic signature (SHA-256) of this UPIR.\n\n        Creates a deterministic hash of the entire UPIR state including\n        specification, architecture, evidence, and reasoning. Useful for\n        integrity verification and change detection.\n\n        Returns:\n            Hexadecimal SHA-256 hash string\n\n        Example:\n            &gt;&gt;&gt; upir = UPIR(id=\"upir-1\", name=\"test\", description=\"test\")\n            &gt;&gt;&gt; sig1 = upir.generate_signature()\n            &gt;&gt;&gt; sig2 = upir.generate_signature()\n            &gt;&gt;&gt; sig1 == sig2  # Deterministic\n            True\n            &gt;&gt;&gt; len(sig1) == 64  # SHA-256\n            True\n\n        References:\n        - SHA-256: Industry standard cryptographic hash\n        - Python hashlib: https://docs.python.org/3/library/hashlib.html\n        \"\"\"\n        # Convert to dict representation\n        upir_dict = {\n            \"id\": self.id,\n            \"name\": self.name,\n            \"description\": self.description,\n            \"specification\": (\n                self.specification.to_dict() if self.specification else None\n            ),\n            \"architecture\": (\n                self.architecture.to_dict() if self.architecture else None\n            ),\n            \"evidence\": {\n                eid: ev.to_dict() for eid, ev in sorted(self.evidence.items())\n            },\n            \"reasoning\": {\n                nid: node.to_dict() for nid, node in sorted(self.reasoning.items())\n            },\n            \"metadata\": dict(sorted(self.metadata.items()))\n        }\n\n        # Create deterministic JSON\n        json_str = json.dumps(upir_dict, sort_keys=True)\n\n        # Compute SHA-256\n        hash_obj = hashlib.sha256(json_str.encode('utf-8'))\n        return hash_obj.hexdigest()\n\n    def to_json(self) -&gt; str:\n        \"\"\"\n        Serialize UPIR to JSON string.\n\n        Returns:\n            JSON string representation\n\n        Example:\n            &gt;&gt;&gt; upir = UPIR(id=\"upir-1\", name=\"test\", description=\"desc\")\n            &gt;&gt;&gt; json_str = upir.to_json()\n            &gt;&gt;&gt; \"upir-1\" in json_str\n            True\n        \"\"\"\n        upir_dict = {\n            \"id\": self.id,\n            \"name\": self.name,\n            \"description\": self.description,\n            \"specification\": (\n                self.specification.to_dict() if self.specification else None\n            ),\n            \"architecture\": (\n                self.architecture.to_dict() if self.architecture else None\n            ),\n            \"evidence\": {\n                eid: ev.to_dict() for eid, ev in self.evidence.items()\n            },\n            \"reasoning\": {\n                nid: node.to_dict() for nid, node in self.reasoning.items()\n            },\n            \"metadata\": self.metadata,\n            \"created_at\": self.created_at.isoformat(),\n            \"updated_at\": self.updated_at.isoformat()\n        }\n        return json.dumps(upir_dict, indent=2)\n\n    @classmethod\n    def from_json(cls, json_str: str) -&gt; \"UPIR\":\n        \"\"\"\n        Deserialize UPIR from JSON string.\n\n        Args:\n            json_str: JSON string representation\n\n        Returns:\n            UPIR instance\n\n        Example:\n            &gt;&gt;&gt; original = UPIR(id=\"upir-1\", name=\"test\", description=\"desc\")\n            &gt;&gt;&gt; json_str = original.to_json()\n            &gt;&gt;&gt; restored = UPIR.from_json(json_str)\n            &gt;&gt;&gt; restored.id == original.id\n            True\n        \"\"\"\n        data = json.loads(json_str)\n\n        return cls(\n            id=data[\"id\"],\n            name=data[\"name\"],\n            description=data[\"description\"],\n            specification=(\n                FormalSpecification.from_dict(data[\"specification\"])\n                if data.get(\"specification\") else None\n            ),\n            architecture=(\n                Architecture.from_dict(data[\"architecture\"])\n                if data.get(\"architecture\") else None\n            ),\n            evidence={\n                eid: Evidence.from_dict(ev_data)\n                for eid, ev_data in data.get(\"evidence\", {}).items()\n            },\n            reasoning={\n                nid: ReasoningNode.from_dict(node_data)\n                for nid, node_data in data.get(\"reasoning\", {}).items()\n            },\n            metadata=data.get(\"metadata\", {}),\n            created_at=datetime.fromisoformat(data[\"created_at\"]),\n            updated_at=datetime.fromisoformat(data[\"updated_at\"])\n        )\n\n    def __str__(self) -&gt; str:\n        \"\"\"Human-readable string representation.\"\"\"\n        parts = [f\"'{self.name}'\"]\n        if self.specification:\n            parts.append(\"with spec\")\n        if self.architecture:\n            parts.append(\"with arch\")\n        if self.evidence:\n            parts.append(f\"{len(self.evidence)} evidence\")\n        if self.reasoning:\n            parts.append(f\"{len(self.reasoning)} reasoning nodes\")\n\n        return f\"UPIR({', '.join(parts)})\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Developer-friendly representation.\"\"\"\n        return (\n            f\"UPIR(id='{self.id}', name='{self.name}', \"\n            f\"spec={self.specification is not None}, \"\n            f\"arch={self.architecture is not None}, \"\n            f\"evidence={len(self.evidence)}, \"\n            f\"reasoning={len(self.reasoning)})\"\n        )\n</code></pre>"},{"location":"api/core/upir/#upir.core.upir.UPIR--simple-usage-with-defaults","title":"Simple usage with defaults","text":"<p>upir = UPIR( ...     specification=FormalSpecification(...), ...     architecture=Architecture(...) ... )</p>"},{"location":"api/core/upir/#upir.core.upir.UPIR--or-with-explicit-metadata","title":"Or with explicit metadata","text":"<p>upir = UPIR( ...     specification=spec, ...     architecture=arch, ...     name=\"E-commerce Platform\", ...     description=\"High-throughput e-commerce system\" ... ) upir.validate() True</p>"},{"location":"api/core/upir/#upir.core.upir.UPIR-functions","title":"Functions","text":""},{"location":"api/core/upir/#upir.core.upir.UPIR.__init__","title":"<code>__init__(specification=None, architecture=None, id=(lambda: str(uuid.uuid4()))(), name='', description='', evidence=dict(), reasoning=dict(), metadata=dict(), created_at=datetime.utcnow(), updated_at=datetime.utcnow())</code>","text":""},{"location":"api/core/upir/#upir.core.upir.UPIR.validate","title":"<code>validate()</code>","text":"<p>Validate the UPIR instance for consistency.</p> <p>Performs multiple validation checks: 1. Specification validation (if present) 2. Reasoning DAG cycle detection 3. Evidence reference validation (all referenced evidence exists)</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if all validations pass</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any validation check fails</p> Example <p>upir = UPIR( ...     id=\"upir-1\", ...     name=\"test\", ...     description=\"test\", ...     specification=FormalSpecification() ... ) upir.validate() True</p> <p>References: - Cycle detection: DFS with recursion stack - Standard graph algorithm for DAG validation</p> Source code in <code>upir/core/upir.py</code> <pre><code>def validate(self) -&gt; bool:\n    \"\"\"\n    Validate the UPIR instance for consistency.\n\n    Performs multiple validation checks:\n    1. Specification validation (if present)\n    2. Reasoning DAG cycle detection\n    3. Evidence reference validation (all referenced evidence exists)\n\n    Returns:\n        True if all validations pass\n\n    Raises:\n        ValueError: If any validation check fails\n\n    Example:\n        &gt;&gt;&gt; upir = UPIR(\n        ...     id=\"upir-1\",\n        ...     name=\"test\",\n        ...     description=\"test\",\n        ...     specification=FormalSpecification()\n        ... )\n        &gt;&gt;&gt; upir.validate()\n        True\n\n    References:\n    - Cycle detection: DFS with recursion stack\n    - Standard graph algorithm for DAG validation\n    \"\"\"\n    # Validate specification if present\n    if self.specification is not None:\n        self.specification.validate()\n\n    # Check for cycles in reasoning DAG\n    self._check_dag_cycles()\n\n    # Validate evidence references\n    for node in self.reasoning.values():\n        for evidence_id in node.evidence_ids:\n            if evidence_id not in self.evidence:\n                raise ValueError(\n                    f\"Reasoning node '{node.id}' references non-existent \"\n                    f\"evidence '{evidence_id}'\"\n                )\n\n    return True\n</code></pre>"},{"location":"api/core/upir/#upir.core.upir.UPIR.to_json","title":"<code>to_json()</code>","text":"<p>Serialize UPIR to JSON string.</p> <p>Returns:</p> Type Description <code>str</code> <p>JSON string representation</p> Example <p>upir = UPIR(id=\"upir-1\", name=\"test\", description=\"desc\") json_str = upir.to_json() \"upir-1\" in json_str True</p> Source code in <code>upir/core/upir.py</code> <pre><code>def to_json(self) -&gt; str:\n    \"\"\"\n    Serialize UPIR to JSON string.\n\n    Returns:\n        JSON string representation\n\n    Example:\n        &gt;&gt;&gt; upir = UPIR(id=\"upir-1\", name=\"test\", description=\"desc\")\n        &gt;&gt;&gt; json_str = upir.to_json()\n        &gt;&gt;&gt; \"upir-1\" in json_str\n        True\n    \"\"\"\n    upir_dict = {\n        \"id\": self.id,\n        \"name\": self.name,\n        \"description\": self.description,\n        \"specification\": (\n            self.specification.to_dict() if self.specification else None\n        ),\n        \"architecture\": (\n            self.architecture.to_dict() if self.architecture else None\n        ),\n        \"evidence\": {\n            eid: ev.to_dict() for eid, ev in self.evidence.items()\n        },\n        \"reasoning\": {\n            nid: node.to_dict() for nid, node in self.reasoning.items()\n        },\n        \"metadata\": self.metadata,\n        \"created_at\": self.created_at.isoformat(),\n        \"updated_at\": self.updated_at.isoformat()\n    }\n    return json.dumps(upir_dict, indent=2)\n</code></pre>"},{"location":"api/core/upir/#upir.core.upir.UPIR.from_json","title":"<code>from_json(json_str)</code>  <code>classmethod</code>","text":"<p>Deserialize UPIR from JSON string.</p> <p>Parameters:</p> Name Type Description Default <code>json_str</code> <code>str</code> <p>JSON string representation</p> required <p>Returns:</p> Type Description <code>UPIR</code> <p>UPIR instance</p> Example <p>original = UPIR(id=\"upir-1\", name=\"test\", description=\"desc\") json_str = original.to_json() restored = UPIR.from_json(json_str) restored.id == original.id True</p> Source code in <code>upir/core/upir.py</code> <pre><code>@classmethod\ndef from_json(cls, json_str: str) -&gt; \"UPIR\":\n    \"\"\"\n    Deserialize UPIR from JSON string.\n\n    Args:\n        json_str: JSON string representation\n\n    Returns:\n        UPIR instance\n\n    Example:\n        &gt;&gt;&gt; original = UPIR(id=\"upir-1\", name=\"test\", description=\"desc\")\n        &gt;&gt;&gt; json_str = original.to_json()\n        &gt;&gt;&gt; restored = UPIR.from_json(json_str)\n        &gt;&gt;&gt; restored.id == original.id\n        True\n    \"\"\"\n    data = json.loads(json_str)\n\n    return cls(\n        id=data[\"id\"],\n        name=data[\"name\"],\n        description=data[\"description\"],\n        specification=(\n            FormalSpecification.from_dict(data[\"specification\"])\n            if data.get(\"specification\") else None\n        ),\n        architecture=(\n            Architecture.from_dict(data[\"architecture\"])\n            if data.get(\"architecture\") else None\n        ),\n        evidence={\n            eid: Evidence.from_dict(ev_data)\n            for eid, ev_data in data.get(\"evidence\", {}).items()\n        },\n        reasoning={\n            nid: ReasoningNode.from_dict(node_data)\n            for nid, node_data in data.get(\"reasoning\", {}).items()\n        },\n        metadata=data.get(\"metadata\", {}),\n        created_at=datetime.fromisoformat(data[\"created_at\"]),\n        updated_at=datetime.fromisoformat(data[\"updated_at\"])\n    )\n</code></pre>"},{"location":"api/core/upir/#usage-example","title":"Usage Example","text":"<pre><code>from upir import UPIR, FormalSpecification, Architecture\nfrom upir.core.temporal import TemporalProperty, TemporalOperator\n\n# Create specification\nspec = FormalSpecification(\n    properties=[\n        TemporalProperty(\n            operator=TemporalOperator.ALWAYS,\n            predicate=\"data_consistent\"\n        )\n    ],\n    constraints={\"latency_p99\": {\"max\": 100.0}}\n)\n\n# Create architecture\narch = Architecture(\n    components=[\n        {\"id\": \"api\", \"type\": \"api_gateway\", \"latency_ms\": 10}\n    ],\n    connections=[]\n)\n\n# Create UPIR instance\nupir = UPIR(specification=spec, architecture=arch)\n\n# Validate\nis_valid = upir.validate()\n\n# Serialize\nupir_json = upir.to_json()\n</code></pre>"},{"location":"api/core/upir/#see-also","title":"See Also","text":"<ul> <li>Architecture - Architecture representation</li> <li>FormalSpecification - Specification representation</li> <li>Verifier - Verify UPIR instances</li> </ul>"},{"location":"api/learning/learner/","title":"RL Optimizer","text":"<p>Reinforcement learning-based architecture optimization.</p>"},{"location":"api/learning/learner/#overview","title":"Overview","text":"<p>The <code>ArchitectureLearner</code> uses PPO (Proximal Policy Optimization) to optimize architectures from production metrics.</p>"},{"location":"api/learning/learner/#class-documentation","title":"Class Documentation","text":""},{"location":"api/learning/learner/#upir.learning.learner.ArchitectureLearner","title":"<code>upir.learning.learner.ArchitectureLearner</code>","text":"<p>Learn optimal architectures using PPO reinforcement learning.</p> <p>The learner encodes UPIR architectures as state vectors, uses PPO to select architectural modifications (actions), and learns from performance metrics (rewards) to optimize the architecture over time.</p> <p>State encoding extracts features like component count, latency, throughput. Actions represent architectural changes (adjust parallelism, change types, etc.). Rewards combine constraint satisfaction and performance improvements.</p> <p>Attributes:</p> Name Type Description <code>state_dim</code> <p>Fixed dimension of state vectors</p> <code>action_dim</code> <p>Number of possible actions</p> <code>ppo</code> <p>PPO agent for policy learning</p> <code>experience_buffer</code> <code>Deque[Experience]</code> <p>Replay buffer for experiences</p> <code>config</code> <p>PPO configuration</p> Example <p>learner = ArchitectureLearner(state_dim=64, action_dim=40)</p> <p>References: - PPO: Policy gradient method with clipped objective - TD Commons: Architecture optimization approach - OpenAI Spinning Up: RL best practices</p> Source code in <code>upir/learning/learner.py</code> <pre><code>class ArchitectureLearner:\n    \"\"\"\n    Learn optimal architectures using PPO reinforcement learning.\n\n    The learner encodes UPIR architectures as state vectors, uses PPO to\n    select architectural modifications (actions), and learns from performance\n    metrics (rewards) to optimize the architecture over time.\n\n    State encoding extracts features like component count, latency, throughput.\n    Actions represent architectural changes (adjust parallelism, change types, etc.).\n    Rewards combine constraint satisfaction and performance improvements.\n\n    Attributes:\n        state_dim: Fixed dimension of state vectors\n        action_dim: Number of possible actions\n        ppo: PPO agent for policy learning\n        experience_buffer: Replay buffer for experiences\n        config: PPO configuration\n\n    Example:\n        &gt;&gt;&gt; learner = ArchitectureLearner(state_dim=64, action_dim=40)\n        &gt;&gt;&gt; # After deploying architecture and collecting metrics:\n        &gt;&gt;&gt; optimized_upir = learner.learn_from_metrics(upir, metrics)\n\n    References:\n    - PPO: Policy gradient method with clipped objective\n    - TD Commons: Architecture optimization approach\n    - OpenAI Spinning Up: RL best practices\n    \"\"\"\n\n    def __init__(\n        self,\n        state_dim: int = 64,\n        action_dim: int = 40,\n        config: PPOConfig = None,\n        buffer_size: int = 1000\n    ):\n        \"\"\"\n        Initialize architecture learner.\n\n        Args:\n            state_dim: Dimension of state encoding (default 64)\n            action_dim: Number of possible actions (default 40)\n            config: PPO configuration (uses defaults if None)\n            buffer_size: Maximum size of experience buffer\n        \"\"\"\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.config = config or PPOConfig()\n\n        # Initialize PPO agent\n        self.ppo = PPO(state_dim=state_dim, action_dim=action_dim, config=self.config)\n\n        # Experience buffer for training\n        self.experience_buffer: Deque[Experience] = deque(maxlen=buffer_size)\n\n        # Feature normalization stats (updated online)\n        self.feature_stats = {\n            \"num_components_max\": 100.0,\n            \"num_connections_max\": 200.0,\n            \"avg_latency_max\": 10000.0,  # 10 seconds in ms\n            \"total_throughput_max\": 100000.0,  # 100k QPS\n            \"complexity_max\": 1000.0,\n        }\n\n        logger.info(\n            f\"Initialized ArchitectureLearner: state_dim={state_dim}, \"\n            f\"action_dim={action_dim}, buffer_size={buffer_size}\"\n        )\n\n    def encode_state(self, upir: UPIR) -&gt; np.ndarray:\n        \"\"\"\n        Encode UPIR architecture as fixed-size state vector.\n\n        Extracts architectural features and normalizes to [0, 1] range:\n        - Number of components\n        - Number of connections\n        - Average component latency\n        - Total throughput capacity\n        - Deployment complexity score\n\n        The state is padded to fixed size (state_dim) for consistent input.\n\n        Args:\n            upir: UPIR to encode\n\n        Returns:\n            State vector (state_dim,) with values in [0, 1]\n\n        Example:\n            &gt;&gt;&gt; learner = ArchitectureLearner()\n            &gt;&gt;&gt; upir = UPIR(id=\"test\", name=\"Test\", description=\"Test\")\n            &gt;&gt;&gt; state = learner.encode_state(upir)\n            &gt;&gt;&gt; state.shape\n            (64,)\n            &gt;&gt;&gt; assert np.all((state &gt;= 0) &amp; (state &lt;= 1))\n        \"\"\"\n        if upir.architecture is None:\n            # No architecture - return zero state\n            return np.zeros(self.state_dim)\n\n        arch = upir.architecture\n\n        # Extract basic features\n        num_components = len(arch.components)\n        num_connections = len(arch.connections)\n\n        # Compute average latency (if components have latency info)\n        total_latency = 0.0\n        latency_count = 0\n        for comp in arch.components:\n            if isinstance(comp, dict) and \"latency_ms\" in comp:\n                total_latency += comp[\"latency_ms\"]\n                latency_count += 1\n        avg_latency = total_latency / latency_count if latency_count &gt; 0 else 0.0\n\n        # Compute total throughput (if components have throughput info)\n        total_throughput = 0.0\n        for comp in arch.components:\n            if isinstance(comp, dict) and \"throughput_qps\" in comp:\n                total_throughput += comp[\"throughput_qps\"]\n\n        # Compute deployment complexity (simple heuristic)\n        # Based on number of components and connections\n        complexity = num_components * 10 + num_connections * 5\n\n        # Normalize features to [0, 1]\n        features = np.array([\n            num_components / self.feature_stats[\"num_components_max\"],\n            num_connections / self.feature_stats[\"num_connections_max\"],\n            avg_latency / self.feature_stats[\"avg_latency_max\"],\n            total_throughput / self.feature_stats[\"total_throughput_max\"],\n            complexity / self.feature_stats[\"complexity_max\"],\n        ])\n\n        # Clip to [0, 1] range\n        features = np.clip(features, 0.0, 1.0)\n\n        # Add per-component features (latency, throughput, parallelism)\n        component_features = []\n        for comp in arch.components[:10]:  # Max 10 components\n            if isinstance(comp, dict):\n                comp_latency = comp.get(\"latency_ms\", 0.0)\n                comp_throughput = comp.get(\"throughput_qps\", 0.0)\n                comp_parallelism = comp.get(\"parallelism\", 1)\n\n                component_features.extend([\n                    comp_latency / self.feature_stats[\"avg_latency_max\"],\n                    comp_throughput / self.feature_stats[\"total_throughput_max\"],\n                    comp_parallelism / 100.0,  # Normalize to [0, 1] assuming max 100\n                ])\n\n        # Pad to fixed size\n        all_features = np.concatenate([features, component_features])\n\n        # Clip all features to [0, 1] range\n        all_features = np.clip(all_features, 0.0, 1.0)\n        if len(all_features) &lt; self.state_dim:\n            # Pad with zeros\n            all_features = np.pad(\n                all_features,\n                (0, self.state_dim - len(all_features)),\n                mode=\"constant\"\n            )\n        else:\n            # Truncate if too long\n            all_features = all_features[:self.state_dim]\n\n        return all_features.astype(np.float32)\n\n    def decode_action(self, action: int, upir: UPIR) -&gt; UPIR:\n        \"\"\"\n        Decode action and apply architectural modification.\n\n        Actions represent different architectural changes:\n        - 0-9: Increase parallelism of component i\n        - 10-19: Decrease parallelism of component i\n        - 20-29: Change component type (e.g., batch -&gt; streaming)\n        - 30-39: Modify connection (add/remove)\n\n        Args:\n            action: Action index (0 to action_dim-1)\n            upir: Current UPIR\n\n        Returns:\n            Modified UPIR with architectural change applied\n\n        Example:\n            &gt;&gt;&gt; learner = ArchitectureLearner()\n            &gt;&gt;&gt; upir = UPIR(id=\"test\", name=\"Test\", description=\"Test\")\n            &gt;&gt;&gt; modified = learner.decode_action(0, upir)\n        \"\"\"\n        # Create copy to avoid modifying original\n        modified_upir = copy.deepcopy(upir)\n\n        if modified_upir.architecture is None:\n            logger.warning(\"No architecture to modify\")\n            return modified_upir\n\n        arch = modified_upir.architecture\n        num_components = len(arch.components)\n\n        if num_components == 0:\n            logger.warning(\"No components to modify\")\n            return modified_upir\n\n        # Decode action type and target\n        if action &lt; 10:\n            # Increase parallelism of component (action % num_components)\n            component_idx = action % num_components\n            comp = arch.components[component_idx]\n            current_parallelism = comp.get(\"parallelism\", 1)\n            new_parallelism = min(current_parallelism + 1, 100)\n            comp[\"parallelism\"] = new_parallelism\n            logger.debug(\n                f\"Action {action}: Increase parallelism of {comp.get('name', f'comp_{component_idx}')} \"\n                f\"from {current_parallelism} to {new_parallelism}\"\n            )\n\n        elif action &lt; 20:\n            # Decrease parallelism of component\n            component_idx = (action - 10) % num_components\n            comp = arch.components[component_idx]\n            current_parallelism = comp.get(\"parallelism\", 1)\n            new_parallelism = max(current_parallelism - 1, 1)\n            comp[\"parallelism\"] = new_parallelism\n            logger.debug(\n                f\"Action {action}: Decrease parallelism of {comp.get('name', f'comp_{component_idx}')} \"\n                f\"from {current_parallelism} to {new_parallelism}\"\n            )\n\n        elif action &lt; 30:\n            # Change component type (simplified: toggle between batch/streaming)\n            component_idx = (action - 20) % num_components\n            comp = arch.components[component_idx]\n            current_type = comp.get(\"type\", \"processor\")\n            if \"streaming\" in current_type.lower():\n                new_type = \"batch_processor\"\n            else:\n                new_type = \"streaming_processor\"\n            comp[\"type\"] = new_type\n            logger.debug(\n                f\"Action {action}: Change type of {comp.get('name', f'comp_{component_idx}')} \"\n                f\"from {current_type} to {new_type}\"\n            )\n\n        else:\n            # Modify connection (simplified: toggle connection property)\n            if len(arch.connections) &gt; 0:\n                connection_idx = (action - 30) % len(arch.connections)\n                conn = arch.connections[connection_idx]\n                # Toggle \"batched\" property\n                current_batched = conn.get(\"batched\", False)\n                conn[\"batched\"] = not current_batched\n                logger.debug(\n                    f\"Action {action}: Toggle batched for connection \"\n                    f\"{conn.get('from', '?')}-&gt;{conn.get('to', '?')} to {not current_batched}\"\n                )\n\n        return modified_upir\n\n    def compute_reward(\n        self,\n        metrics: Dict[str, float],\n        spec: FormalSpecification,\n        previous_metrics: Dict[str, float] = None\n    ) -&gt; float:\n        \"\"\"\n        Compute reward from performance metrics and specification.\n\n        Reward structure:\n        - Base reward: 1.0\n        - Constraint satisfaction: +0.1 per met constraint, -0.5 per violation\n        - Performance improvement: +delta/target for improvements\n\n        Rewards are clipped to [-1, 1] range for stability.\n\n        Args:\n            metrics: Current performance metrics\n                - latency_p99: 99th percentile latency (ms)\n                - throughput_qps: Queries per second\n                - error_rate: Error rate (0-1)\n                - cost: Deployment cost\n            spec: Formal specification with constraints\n            previous_metrics: Previous metrics for computing deltas\n\n        Returns:\n            Reward in [-1, 1]\n\n        Example:\n            &gt;&gt;&gt; learner = ArchitectureLearner()\n            &gt;&gt;&gt; metrics = {\"latency_p99\": 100, \"throughput_qps\": 1000}\n            &gt;&gt;&gt; spec = FormalSpecification()\n            &gt;&gt;&gt; reward = learner.compute_reward(metrics, spec)\n        \"\"\"\n        reward = 1.0  # Base reward\n\n        # Check constraint satisfaction\n        constraints_met = 0\n        constraints_violated = 0\n\n        # Check temporal properties for latency constraints\n        for prop in spec.properties + spec.invariants:\n            if prop.time_bound is not None:\n                # Latency constraint\n                target_latency = prop.time_bound\n                actual_latency = metrics.get(\"latency_p99\", float(\"inf\"))\n\n                if actual_latency &lt;= target_latency:\n                    constraints_met += 1\n                    reward += 0.1\n                else:\n                    constraints_violated += 1\n                    reward -= 0.5\n\n        # Check throughput requirements (heuristic from predicates)\n        has_throughput_req = any(\n            \"throughput\" in prop.predicate.lower()\n            for prop in spec.properties + spec.invariants\n        )\n        if has_throughput_req:\n            # Assume target throughput of 1000 QPS (could be extracted from spec)\n            target_throughput = 1000.0\n            actual_throughput = metrics.get(\"throughput_qps\", 0.0)\n\n            if actual_throughput &gt;= target_throughput:\n                constraints_met += 1\n                reward += 0.1\n            else:\n                constraints_violated += 1\n                reward -= 0.5\n\n        # Reward performance improvements over previous metrics\n        if previous_metrics is not None:\n            # Latency reduction (lower is better)\n            prev_latency = previous_metrics.get(\"latency_p99\", 0.0)\n            curr_latency = metrics.get(\"latency_p99\", 0.0)\n            if prev_latency &gt; 0:\n                latency_delta = (prev_latency - curr_latency) / prev_latency\n                reward += latency_delta  # Positive if latency reduced\n\n            # Throughput increase (higher is better)\n            prev_throughput = previous_metrics.get(\"throughput_qps\", 0.0)\n            curr_throughput = metrics.get(\"throughput_qps\", 0.0)\n            if prev_throughput &gt; 0:\n                throughput_delta = (curr_throughput - prev_throughput) / prev_throughput\n                reward += throughput_delta  # Positive if throughput increased\n\n        # Penalty for high error rate\n        error_rate = metrics.get(\"error_rate\", 0.0)\n        if error_rate &gt; 0.01:  # More than 1% errors\n            reward -= error_rate * 10  # Heavy penalty\n\n        # Clip reward to [-1, 1]\n        reward = np.clip(reward, -1.0, 1.0)\n\n        logger.debug(\n            f\"Computed reward: {reward:.3f} \"\n            f\"(constraints_met={constraints_met}, violated={constraints_violated})\"\n        )\n\n        return reward\n\n    def learn_from_metrics(\n        self,\n        upir: UPIR,\n        metrics: Dict[str, float],\n        previous_metrics: Dict[str, float] = None\n    ) -&gt; UPIR:\n        \"\"\"\n        Learn from performance metrics and return optimized architecture.\n\n        Main entry point for architecture optimization. This method:\n        1. Encodes current architecture as state\n        2. Selects action using PPO policy\n        3. Decodes action to modify architecture\n        4. Computes reward from metrics\n        5. Stores experience for learning\n        6. Updates PPO policy when buffer is full\n        7. Returns optimized architecture\n\n        Args:\n            upir: Current UPIR\n            metrics: Performance metrics from deployment\n            previous_metrics: Previous metrics for delta computation\n\n        Returns:\n            Optimized UPIR with modified architecture\n\n        Example:\n            &gt;&gt;&gt; learner = ArchitectureLearner()\n            &gt;&gt;&gt; upir = UPIR(id=\"test\", name=\"Test\", description=\"Test\")\n            &gt;&gt;&gt; metrics = {\"latency_p99\": 100, \"throughput_qps\": 1000}\n            &gt;&gt;&gt; optimized = learner.learn_from_metrics(upir, metrics)\n\n        References:\n        - PPO: Policy gradient with experience replay\n        - TD Commons: Continuous optimization loop\n        \"\"\"\n        # 1. Encode current architecture as state\n        state = self.encode_state(upir)\n\n        # 2. Select action using PPO policy\n        action, log_prob, value = self.ppo.select_action(state)\n\n        # 3. Decode action to modify architecture\n        optimized_upir = self.decode_action(action, upir)\n\n        # 4. Compute reward from metrics\n        reward = self.compute_reward(\n            metrics,\n            upir.specification if upir.specification else FormalSpecification(),\n            previous_metrics\n        )\n\n        # 5. Store experience\n        # Check if this is terminal (could be based on convergence criteria)\n        done = False  # For now, never terminal (continuous learning)\n\n        experience = Experience(\n            state=state,\n            action=action,\n            reward=reward,\n            log_prob=log_prob,\n            value=value,\n            done=done\n        )\n        self.experience_buffer.append(experience)\n\n        # 6. Update policy if we have enough experiences\n        if len(self.experience_buffer) &gt;= self.config.batch_size:\n            self._update_policy()\n\n        logger.info(\n            f\"Learning step: action={action}, reward={reward:.3f}, \"\n            f\"buffer_size={len(self.experience_buffer)}\"\n        )\n\n        return optimized_upir\n\n    def _update_policy(self):\n        \"\"\"\n        Update PPO policy using experiences from buffer.\n\n        Extracts states, actions, rewards from buffer, computes advantages\n        using GAE, and performs PPO update.\n        \"\"\"\n        # Extract experiences\n        states = np.array([exp.state for exp in self.experience_buffer])\n        actions = np.array([exp.action for exp in self.experience_buffer])\n        old_log_probs = np.array([exp.log_prob for exp in self.experience_buffer])\n        rewards = np.array([exp.reward for exp in self.experience_buffer])\n        values = np.array([exp.value for exp in self.experience_buffer])\n        dones = np.array([exp.done for exp in self.experience_buffer], dtype=np.float32)\n\n        # Compute advantages using GAE\n        advantages, returns = self.ppo.compute_gae(rewards, values, dones)\n\n        # Update PPO policy\n        metrics = self.ppo.update(states, actions, old_log_probs, returns, advantages)\n\n        logger.info(\n            f\"Policy update: policy_loss={metrics['policy_loss']:.4f}, \"\n            f\"value_loss={metrics['value_loss']:.4f}, \"\n            f\"entropy={metrics['entropy']:.4f}\"\n        )\n\n        # Clear buffer after update\n        self.experience_buffer.clear()\n\n    def __str__(self) -&gt; str:\n        \"\"\"String representation.\"\"\"\n        return (\n            f\"ArchitectureLearner(state_dim={self.state_dim}, \"\n            f\"action_dim={self.action_dim}, \"\n            f\"buffer={len(self.experience_buffer)}/{self.experience_buffer.maxlen})\"\n        )\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Developer-friendly representation.\"\"\"\n        return (\n            f\"ArchitectureLearner(state_dim={self.state_dim}, \"\n            f\"action_dim={self.action_dim}, \"\n            f\"config={self.config})\"\n        )\n</code></pre>"},{"location":"api/learning/learner/#upir.learning.learner.ArchitectureLearner--after-deploying-architecture-and-collecting-metrics","title":"After deploying architecture and collecting metrics:","text":"<p>optimized_upir = learner.learn_from_metrics(upir, metrics)</p>"},{"location":"api/learning/learner/#upir.learning.learner.ArchitectureLearner-functions","title":"Functions","text":""},{"location":"api/learning/learner/#upir.learning.learner.ArchitectureLearner.__init__","title":"<code>__init__(state_dim=64, action_dim=40, config=None, buffer_size=1000)</code>","text":"<p>Initialize architecture learner.</p> <p>Parameters:</p> Name Type Description Default <code>state_dim</code> <code>int</code> <p>Dimension of state encoding (default 64)</p> <code>64</code> <code>action_dim</code> <code>int</code> <p>Number of possible actions (default 40)</p> <code>40</code> <code>config</code> <code>PPOConfig</code> <p>PPO configuration (uses defaults if None)</p> <code>None</code> <code>buffer_size</code> <code>int</code> <p>Maximum size of experience buffer</p> <code>1000</code> Source code in <code>upir/learning/learner.py</code> <pre><code>def __init__(\n    self,\n    state_dim: int = 64,\n    action_dim: int = 40,\n    config: PPOConfig = None,\n    buffer_size: int = 1000\n):\n    \"\"\"\n    Initialize architecture learner.\n\n    Args:\n        state_dim: Dimension of state encoding (default 64)\n        action_dim: Number of possible actions (default 40)\n        config: PPO configuration (uses defaults if None)\n        buffer_size: Maximum size of experience buffer\n    \"\"\"\n    self.state_dim = state_dim\n    self.action_dim = action_dim\n    self.config = config or PPOConfig()\n\n    # Initialize PPO agent\n    self.ppo = PPO(state_dim=state_dim, action_dim=action_dim, config=self.config)\n\n    # Experience buffer for training\n    self.experience_buffer: Deque[Experience] = deque(maxlen=buffer_size)\n\n    # Feature normalization stats (updated online)\n    self.feature_stats = {\n        \"num_components_max\": 100.0,\n        \"num_connections_max\": 200.0,\n        \"avg_latency_max\": 10000.0,  # 10 seconds in ms\n        \"total_throughput_max\": 100000.0,  # 100k QPS\n        \"complexity_max\": 1000.0,\n    }\n\n    logger.info(\n        f\"Initialized ArchitectureLearner: state_dim={state_dim}, \"\n        f\"action_dim={action_dim}, buffer_size={buffer_size}\"\n    )\n</code></pre>"},{"location":"api/learning/learner/#upir.learning.learner.ArchitectureLearner.encode_state","title":"<code>encode_state(upir)</code>","text":"<p>Encode UPIR architecture as fixed-size state vector.</p> <p>Extracts architectural features and normalizes to [0, 1] range: - Number of components - Number of connections - Average component latency - Total throughput capacity - Deployment complexity score</p> <p>The state is padded to fixed size (state_dim) for consistent input.</p> <p>Parameters:</p> Name Type Description Default <code>upir</code> <code>UPIR</code> <p>UPIR to encode</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>State vector (state_dim,) with values in [0, 1]</p> Example <p>learner = ArchitectureLearner() upir = UPIR(id=\"test\", name=\"Test\", description=\"Test\") state = learner.encode_state(upir) state.shape (64,) assert np.all((state &gt;= 0) &amp; (state &lt;= 1))</p> Source code in <code>upir/learning/learner.py</code> <pre><code>def encode_state(self, upir: UPIR) -&gt; np.ndarray:\n    \"\"\"\n    Encode UPIR architecture as fixed-size state vector.\n\n    Extracts architectural features and normalizes to [0, 1] range:\n    - Number of components\n    - Number of connections\n    - Average component latency\n    - Total throughput capacity\n    - Deployment complexity score\n\n    The state is padded to fixed size (state_dim) for consistent input.\n\n    Args:\n        upir: UPIR to encode\n\n    Returns:\n        State vector (state_dim,) with values in [0, 1]\n\n    Example:\n        &gt;&gt;&gt; learner = ArchitectureLearner()\n        &gt;&gt;&gt; upir = UPIR(id=\"test\", name=\"Test\", description=\"Test\")\n        &gt;&gt;&gt; state = learner.encode_state(upir)\n        &gt;&gt;&gt; state.shape\n        (64,)\n        &gt;&gt;&gt; assert np.all((state &gt;= 0) &amp; (state &lt;= 1))\n    \"\"\"\n    if upir.architecture is None:\n        # No architecture - return zero state\n        return np.zeros(self.state_dim)\n\n    arch = upir.architecture\n\n    # Extract basic features\n    num_components = len(arch.components)\n    num_connections = len(arch.connections)\n\n    # Compute average latency (if components have latency info)\n    total_latency = 0.0\n    latency_count = 0\n    for comp in arch.components:\n        if isinstance(comp, dict) and \"latency_ms\" in comp:\n            total_latency += comp[\"latency_ms\"]\n            latency_count += 1\n    avg_latency = total_latency / latency_count if latency_count &gt; 0 else 0.0\n\n    # Compute total throughput (if components have throughput info)\n    total_throughput = 0.0\n    for comp in arch.components:\n        if isinstance(comp, dict) and \"throughput_qps\" in comp:\n            total_throughput += comp[\"throughput_qps\"]\n\n    # Compute deployment complexity (simple heuristic)\n    # Based on number of components and connections\n    complexity = num_components * 10 + num_connections * 5\n\n    # Normalize features to [0, 1]\n    features = np.array([\n        num_components / self.feature_stats[\"num_components_max\"],\n        num_connections / self.feature_stats[\"num_connections_max\"],\n        avg_latency / self.feature_stats[\"avg_latency_max\"],\n        total_throughput / self.feature_stats[\"total_throughput_max\"],\n        complexity / self.feature_stats[\"complexity_max\"],\n    ])\n\n    # Clip to [0, 1] range\n    features = np.clip(features, 0.0, 1.0)\n\n    # Add per-component features (latency, throughput, parallelism)\n    component_features = []\n    for comp in arch.components[:10]:  # Max 10 components\n        if isinstance(comp, dict):\n            comp_latency = comp.get(\"latency_ms\", 0.0)\n            comp_throughput = comp.get(\"throughput_qps\", 0.0)\n            comp_parallelism = comp.get(\"parallelism\", 1)\n\n            component_features.extend([\n                comp_latency / self.feature_stats[\"avg_latency_max\"],\n                comp_throughput / self.feature_stats[\"total_throughput_max\"],\n                comp_parallelism / 100.0,  # Normalize to [0, 1] assuming max 100\n            ])\n\n    # Pad to fixed size\n    all_features = np.concatenate([features, component_features])\n\n    # Clip all features to [0, 1] range\n    all_features = np.clip(all_features, 0.0, 1.0)\n    if len(all_features) &lt; self.state_dim:\n        # Pad with zeros\n        all_features = np.pad(\n            all_features,\n            (0, self.state_dim - len(all_features)),\n            mode=\"constant\"\n        )\n    else:\n        # Truncate if too long\n        all_features = all_features[:self.state_dim]\n\n    return all_features.astype(np.float32)\n</code></pre>"},{"location":"api/learning/learner/#upir.learning.learner.ArchitectureLearner.decode_action","title":"<code>decode_action(action, upir)</code>","text":"<p>Decode action and apply architectural modification.</p> <p>Actions represent different architectural changes: - 0-9: Increase parallelism of component i - 10-19: Decrease parallelism of component i - 20-29: Change component type (e.g., batch -&gt; streaming) - 30-39: Modify connection (add/remove)</p> <p>Parameters:</p> Name Type Description Default <code>action</code> <code>int</code> <p>Action index (0 to action_dim-1)</p> required <code>upir</code> <code>UPIR</code> <p>Current UPIR</p> required <p>Returns:</p> Type Description <code>UPIR</code> <p>Modified UPIR with architectural change applied</p> Example <p>learner = ArchitectureLearner() upir = UPIR(id=\"test\", name=\"Test\", description=\"Test\") modified = learner.decode_action(0, upir)</p> Source code in <code>upir/learning/learner.py</code> <pre><code>def decode_action(self, action: int, upir: UPIR) -&gt; UPIR:\n    \"\"\"\n    Decode action and apply architectural modification.\n\n    Actions represent different architectural changes:\n    - 0-9: Increase parallelism of component i\n    - 10-19: Decrease parallelism of component i\n    - 20-29: Change component type (e.g., batch -&gt; streaming)\n    - 30-39: Modify connection (add/remove)\n\n    Args:\n        action: Action index (0 to action_dim-1)\n        upir: Current UPIR\n\n    Returns:\n        Modified UPIR with architectural change applied\n\n    Example:\n        &gt;&gt;&gt; learner = ArchitectureLearner()\n        &gt;&gt;&gt; upir = UPIR(id=\"test\", name=\"Test\", description=\"Test\")\n        &gt;&gt;&gt; modified = learner.decode_action(0, upir)\n    \"\"\"\n    # Create copy to avoid modifying original\n    modified_upir = copy.deepcopy(upir)\n\n    if modified_upir.architecture is None:\n        logger.warning(\"No architecture to modify\")\n        return modified_upir\n\n    arch = modified_upir.architecture\n    num_components = len(arch.components)\n\n    if num_components == 0:\n        logger.warning(\"No components to modify\")\n        return modified_upir\n\n    # Decode action type and target\n    if action &lt; 10:\n        # Increase parallelism of component (action % num_components)\n        component_idx = action % num_components\n        comp = arch.components[component_idx]\n        current_parallelism = comp.get(\"parallelism\", 1)\n        new_parallelism = min(current_parallelism + 1, 100)\n        comp[\"parallelism\"] = new_parallelism\n        logger.debug(\n            f\"Action {action}: Increase parallelism of {comp.get('name', f'comp_{component_idx}')} \"\n            f\"from {current_parallelism} to {new_parallelism}\"\n        )\n\n    elif action &lt; 20:\n        # Decrease parallelism of component\n        component_idx = (action - 10) % num_components\n        comp = arch.components[component_idx]\n        current_parallelism = comp.get(\"parallelism\", 1)\n        new_parallelism = max(current_parallelism - 1, 1)\n        comp[\"parallelism\"] = new_parallelism\n        logger.debug(\n            f\"Action {action}: Decrease parallelism of {comp.get('name', f'comp_{component_idx}')} \"\n            f\"from {current_parallelism} to {new_parallelism}\"\n        )\n\n    elif action &lt; 30:\n        # Change component type (simplified: toggle between batch/streaming)\n        component_idx = (action - 20) % num_components\n        comp = arch.components[component_idx]\n        current_type = comp.get(\"type\", \"processor\")\n        if \"streaming\" in current_type.lower():\n            new_type = \"batch_processor\"\n        else:\n            new_type = \"streaming_processor\"\n        comp[\"type\"] = new_type\n        logger.debug(\n            f\"Action {action}: Change type of {comp.get('name', f'comp_{component_idx}')} \"\n            f\"from {current_type} to {new_type}\"\n        )\n\n    else:\n        # Modify connection (simplified: toggle connection property)\n        if len(arch.connections) &gt; 0:\n            connection_idx = (action - 30) % len(arch.connections)\n            conn = arch.connections[connection_idx]\n            # Toggle \"batched\" property\n            current_batched = conn.get(\"batched\", False)\n            conn[\"batched\"] = not current_batched\n            logger.debug(\n                f\"Action {action}: Toggle batched for connection \"\n                f\"{conn.get('from', '?')}-&gt;{conn.get('to', '?')} to {not current_batched}\"\n            )\n\n    return modified_upir\n</code></pre>"},{"location":"api/learning/learner/#upir.learning.learner.ArchitectureLearner.compute_reward","title":"<code>compute_reward(metrics, spec, previous_metrics=None)</code>","text":"<p>Compute reward from performance metrics and specification.</p> <p>Reward structure: - Base reward: 1.0 - Constraint satisfaction: +0.1 per met constraint, -0.5 per violation - Performance improvement: +delta/target for improvements</p> <p>Rewards are clipped to [-1, 1] range for stability.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>Dict[str, float]</code> <p>Current performance metrics - latency_p99: 99th percentile latency (ms) - throughput_qps: Queries per second - error_rate: Error rate (0-1) - cost: Deployment cost</p> required <code>spec</code> <code>FormalSpecification</code> <p>Formal specification with constraints</p> required <code>previous_metrics</code> <code>Dict[str, float]</code> <p>Previous metrics for computing deltas</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>Reward in [-1, 1]</p> Example <p>learner = ArchitectureLearner() metrics = {\"latency_p99\": 100, \"throughput_qps\": 1000} spec = FormalSpecification() reward = learner.compute_reward(metrics, spec)</p> Source code in <code>upir/learning/learner.py</code> <pre><code>def compute_reward(\n    self,\n    metrics: Dict[str, float],\n    spec: FormalSpecification,\n    previous_metrics: Dict[str, float] = None\n) -&gt; float:\n    \"\"\"\n    Compute reward from performance metrics and specification.\n\n    Reward structure:\n    - Base reward: 1.0\n    - Constraint satisfaction: +0.1 per met constraint, -0.5 per violation\n    - Performance improvement: +delta/target for improvements\n\n    Rewards are clipped to [-1, 1] range for stability.\n\n    Args:\n        metrics: Current performance metrics\n            - latency_p99: 99th percentile latency (ms)\n            - throughput_qps: Queries per second\n            - error_rate: Error rate (0-1)\n            - cost: Deployment cost\n        spec: Formal specification with constraints\n        previous_metrics: Previous metrics for computing deltas\n\n    Returns:\n        Reward in [-1, 1]\n\n    Example:\n        &gt;&gt;&gt; learner = ArchitectureLearner()\n        &gt;&gt;&gt; metrics = {\"latency_p99\": 100, \"throughput_qps\": 1000}\n        &gt;&gt;&gt; spec = FormalSpecification()\n        &gt;&gt;&gt; reward = learner.compute_reward(metrics, spec)\n    \"\"\"\n    reward = 1.0  # Base reward\n\n    # Check constraint satisfaction\n    constraints_met = 0\n    constraints_violated = 0\n\n    # Check temporal properties for latency constraints\n    for prop in spec.properties + spec.invariants:\n        if prop.time_bound is not None:\n            # Latency constraint\n            target_latency = prop.time_bound\n            actual_latency = metrics.get(\"latency_p99\", float(\"inf\"))\n\n            if actual_latency &lt;= target_latency:\n                constraints_met += 1\n                reward += 0.1\n            else:\n                constraints_violated += 1\n                reward -= 0.5\n\n    # Check throughput requirements (heuristic from predicates)\n    has_throughput_req = any(\n        \"throughput\" in prop.predicate.lower()\n        for prop in spec.properties + spec.invariants\n    )\n    if has_throughput_req:\n        # Assume target throughput of 1000 QPS (could be extracted from spec)\n        target_throughput = 1000.0\n        actual_throughput = metrics.get(\"throughput_qps\", 0.0)\n\n        if actual_throughput &gt;= target_throughput:\n            constraints_met += 1\n            reward += 0.1\n        else:\n            constraints_violated += 1\n            reward -= 0.5\n\n    # Reward performance improvements over previous metrics\n    if previous_metrics is not None:\n        # Latency reduction (lower is better)\n        prev_latency = previous_metrics.get(\"latency_p99\", 0.0)\n        curr_latency = metrics.get(\"latency_p99\", 0.0)\n        if prev_latency &gt; 0:\n            latency_delta = (prev_latency - curr_latency) / prev_latency\n            reward += latency_delta  # Positive if latency reduced\n\n        # Throughput increase (higher is better)\n        prev_throughput = previous_metrics.get(\"throughput_qps\", 0.0)\n        curr_throughput = metrics.get(\"throughput_qps\", 0.0)\n        if prev_throughput &gt; 0:\n            throughput_delta = (curr_throughput - prev_throughput) / prev_throughput\n            reward += throughput_delta  # Positive if throughput increased\n\n    # Penalty for high error rate\n    error_rate = metrics.get(\"error_rate\", 0.0)\n    if error_rate &gt; 0.01:  # More than 1% errors\n        reward -= error_rate * 10  # Heavy penalty\n\n    # Clip reward to [-1, 1]\n    reward = np.clip(reward, -1.0, 1.0)\n\n    logger.debug(\n        f\"Computed reward: {reward:.3f} \"\n        f\"(constraints_met={constraints_met}, violated={constraints_violated})\"\n    )\n\n    return reward\n</code></pre>"},{"location":"api/learning/learner/#upir.learning.learner.ArchitectureLearner.learn_from_metrics","title":"<code>learn_from_metrics(upir, metrics, previous_metrics=None)</code>","text":"<p>Learn from performance metrics and return optimized architecture.</p> <p>Main entry point for architecture optimization. This method: 1. Encodes current architecture as state 2. Selects action using PPO policy 3. Decodes action to modify architecture 4. Computes reward from metrics 5. Stores experience for learning 6. Updates PPO policy when buffer is full 7. Returns optimized architecture</p> <p>Parameters:</p> Name Type Description Default <code>upir</code> <code>UPIR</code> <p>Current UPIR</p> required <code>metrics</code> <code>Dict[str, float]</code> <p>Performance metrics from deployment</p> required <code>previous_metrics</code> <code>Dict[str, float]</code> <p>Previous metrics for delta computation</p> <code>None</code> <p>Returns:</p> Type Description <code>UPIR</code> <p>Optimized UPIR with modified architecture</p> Example <p>learner = ArchitectureLearner() upir = UPIR(id=\"test\", name=\"Test\", description=\"Test\") metrics = {\"latency_p99\": 100, \"throughput_qps\": 1000} optimized = learner.learn_from_metrics(upir, metrics)</p> <p>References: - PPO: Policy gradient with experience replay - TD Commons: Continuous optimization loop</p> Source code in <code>upir/learning/learner.py</code> <pre><code>def learn_from_metrics(\n    self,\n    upir: UPIR,\n    metrics: Dict[str, float],\n    previous_metrics: Dict[str, float] = None\n) -&gt; UPIR:\n    \"\"\"\n    Learn from performance metrics and return optimized architecture.\n\n    Main entry point for architecture optimization. This method:\n    1. Encodes current architecture as state\n    2. Selects action using PPO policy\n    3. Decodes action to modify architecture\n    4. Computes reward from metrics\n    5. Stores experience for learning\n    6. Updates PPO policy when buffer is full\n    7. Returns optimized architecture\n\n    Args:\n        upir: Current UPIR\n        metrics: Performance metrics from deployment\n        previous_metrics: Previous metrics for delta computation\n\n    Returns:\n        Optimized UPIR with modified architecture\n\n    Example:\n        &gt;&gt;&gt; learner = ArchitectureLearner()\n        &gt;&gt;&gt; upir = UPIR(id=\"test\", name=\"Test\", description=\"Test\")\n        &gt;&gt;&gt; metrics = {\"latency_p99\": 100, \"throughput_qps\": 1000}\n        &gt;&gt;&gt; optimized = learner.learn_from_metrics(upir, metrics)\n\n    References:\n    - PPO: Policy gradient with experience replay\n    - TD Commons: Continuous optimization loop\n    \"\"\"\n    # 1. Encode current architecture as state\n    state = self.encode_state(upir)\n\n    # 2. Select action using PPO policy\n    action, log_prob, value = self.ppo.select_action(state)\n\n    # 3. Decode action to modify architecture\n    optimized_upir = self.decode_action(action, upir)\n\n    # 4. Compute reward from metrics\n    reward = self.compute_reward(\n        metrics,\n        upir.specification if upir.specification else FormalSpecification(),\n        previous_metrics\n    )\n\n    # 5. Store experience\n    # Check if this is terminal (could be based on convergence criteria)\n    done = False  # For now, never terminal (continuous learning)\n\n    experience = Experience(\n        state=state,\n        action=action,\n        reward=reward,\n        log_prob=log_prob,\n        value=value,\n        done=done\n    )\n    self.experience_buffer.append(experience)\n\n    # 6. Update policy if we have enough experiences\n    if len(self.experience_buffer) &gt;= self.config.batch_size:\n        self._update_policy()\n\n    logger.info(\n        f\"Learning step: action={action}, reward={reward:.3f}, \"\n        f\"buffer_size={len(self.experience_buffer)}\"\n    )\n\n    return optimized_upir\n</code></pre>"},{"location":"api/learning/learner/#upir.learning.learner.ArchitectureLearner.__str__","title":"<code>__str__()</code>","text":"<p>String representation.</p> Source code in <code>upir/learning/learner.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"String representation.\"\"\"\n    return (\n        f\"ArchitectureLearner(state_dim={self.state_dim}, \"\n        f\"action_dim={self.action_dim}, \"\n        f\"buffer={len(self.experience_buffer)}/{self.experience_buffer.maxlen})\"\n    )\n</code></pre>"},{"location":"api/learning/learner/#upir.learning.learner.ArchitectureLearner.__repr__","title":"<code>__repr__()</code>","text":"<p>Developer-friendly representation.</p> Source code in <code>upir/learning/learner.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Developer-friendly representation.\"\"\"\n    return (\n        f\"ArchitectureLearner(state_dim={self.state_dim}, \"\n        f\"action_dim={self.action_dim}, \"\n        f\"config={self.config})\"\n    )\n</code></pre>"},{"location":"api/learning/learner/#usage-example","title":"Usage Example","text":"<pre><code>from upir import UPIR\nfrom upir.learning.learner import ArchitectureLearner\n\n# Create learner\nlearner = ArchitectureLearner(\n    upir,\n    learning_rate=0.0003,\n    gamma=0.99\n)\n\n# Simulate production metrics\nmetrics = {\n    \"latency_p99\": 85.0,\n    \"monthly_cost\": 4500.0,\n    \"throughput_qps\": 12000.0\n}\n\n# Learn to optimize\noptimized_upir = learner.learn(\n    metrics,\n    episodes=100,\n    steps_per_episode=50\n)\n\n# Compare\nprint(f\"Original cost: ${upir.architecture.total_cost}\")\nprint(f\"Optimized cost: ${optimized_upir.architecture.total_cost}\")\n</code></pre>"},{"location":"api/learning/learner/#see-also","title":"See Also","text":"<ul> <li>PPO - PPO implementation</li> </ul>"},{"location":"api/learning/ppo/","title":"PPO","text":"<p>Proximal Policy Optimization algorithm.</p>"},{"location":"api/learning/ppo/#overview","title":"Overview","text":"<p>PPO (Proximal Policy Optimization) is a state-of-the-art reinforcement learning algorithm used to optimize architectures.</p>"},{"location":"api/learning/ppo/#class-documentation","title":"Class Documentation","text":""},{"location":"api/learning/ppo/#upir.learning.ppo.PPO","title":"<code>upir.learning.ppo.PPO</code>","text":"<p>Proximal Policy Optimization (PPO) algorithm.</p> <p>PPO is a policy gradient method that uses a clipped objective to ensure stable, conservative policy updates. It's one of the most popular RL algorithms due to its simplicity and effectiveness.</p> <p>The key innovation is the clipped surrogate objective: L^CLIP(\u03b8) = E[min(r(\u03b8)A, clip(r(\u03b8), 1-\u03b5, 1+\u03b5)A)]</p> <p>where r(\u03b8) = \u03c0_\u03b8(a|s) / \u03c0_\u03b8_old(a|s) is the probability ratio.</p> <p>Attributes:</p> Name Type Description <code>policy</code> <p>PolicyNetwork for action selection and value estimation</p> <code>config</code> <p>PPO hyperparameters</p> <code>optimizer_state</code> <p>State for optimization (momentum, etc.)</p> <p>References: - PPO paper: https://arxiv.org/abs/1707.06347 - OpenAI Spinning Up: https://spinningup.openai.com/en/latest/algorithms/ppo.html - TD Commons: Architecture optimization using PPO</p> Source code in <code>upir/learning/ppo.py</code> <pre><code>class PPO:\n    \"\"\"\n    Proximal Policy Optimization (PPO) algorithm.\n\n    PPO is a policy gradient method that uses a clipped objective to ensure\n    stable, conservative policy updates. It's one of the most popular\n    RL algorithms due to its simplicity and effectiveness.\n\n    The key innovation is the clipped surrogate objective:\n    L^CLIP(\u03b8) = E[min(r(\u03b8)A, clip(r(\u03b8), 1-\u03b5, 1+\u03b5)A)]\n\n    where r(\u03b8) = \u03c0_\u03b8(a|s) / \u03c0_\u03b8_old(a|s) is the probability ratio.\n\n    Attributes:\n        policy: PolicyNetwork for action selection and value estimation\n        config: PPO hyperparameters\n        optimizer_state: State for optimization (momentum, etc.)\n\n    References:\n    - PPO paper: https://arxiv.org/abs/1707.06347\n    - OpenAI Spinning Up: https://spinningup.openai.com/en/latest/algorithms/ppo.html\n    - TD Commons: Architecture optimization using PPO\n    \"\"\"\n\n    def __init__(self, state_dim: int, action_dim: int, config: PPOConfig = None):\n        \"\"\"\n        Initialize PPO agent.\n\n        Args:\n            state_dim: Dimension of state space\n            action_dim: Dimension of action space\n            config: PPO configuration (uses defaults if None)\n        \"\"\"\n        self.config = config or PPOConfig()\n        self.policy = PolicyNetwork(state_dim, action_dim)\n\n        # Optimizer state (simple momentum-based)\n        self.optimizer_state = {\n            name: {\"velocity\": np.zeros_like(param)}\n            for name, param in self.policy.weights.items()\n        }\n\n        logger.info(\n            f\"Initialized PPO: state_dim={state_dim}, action_dim={action_dim}, \"\n            f\"lr={self.config.learning_rate}, epsilon={self.config.epsilon}\"\n        )\n\n    def select_action(self, state: np.ndarray) -&gt; Tuple[int, float, float]:\n        \"\"\"\n        Select action using current policy.\n\n        Args:\n            state: Current state vector\n\n        Returns:\n            Tuple of (action, log_prob, value)\n        \"\"\"\n        return self.policy.get_action(state)\n\n    def compute_gae(\n        self,\n        rewards: np.ndarray,\n        values: np.ndarray,\n        dones: np.ndarray\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Compute Generalized Advantage Estimation (GAE).\n\n        GAE uses an exponentially-weighted average of n-step advantages to\n        reduce variance while maintaining low bias. It interpolates between\n        Monte Carlo (high variance, low bias) and TD (low variance, high bias).\n\n        Formula:\n        \u03b4_t = r_t + \u03b3V(s_{t+1})(1 - done_t) - V(s_t)\n        A_t = \u03b4_t + (\u03b3\u03bb)\u03b4_{t+1} + (\u03b3\u03bb)\u00b2\u03b4_{t+2} + ...\n\n        Args:\n            rewards: Rewards received (T,)\n            values: Value estimates V(s_t) (T,)\n            dones: Episode termination flags (T,)\n\n        Returns:\n            Tuple of (advantages, returns):\n            - advantages: Advantage estimates A_t (T,)\n            - returns: Discounted returns (T,)\n\n        References:\n        - GAE paper: https://arxiv.org/abs/1506.02438\n        - OpenAI Spinning Up: GAE explanation\n        \"\"\"\n        T = len(rewards)\n        advantages = np.zeros(T, dtype=np.float32)\n        returns = np.zeros(T, dtype=np.float32)\n\n        # Compute TD errors (deltas)\n        deltas = np.zeros(T, dtype=np.float32)\n        for t in range(T):\n            # \u03b4_t = r_t + \u03b3V(s_{t+1})(1 - done_t) - V(s_t)\n            next_value = values[t + 1] if t + 1 &lt; T else 0.0\n            deltas[t] = rewards[t] + self.config.gamma * next_value * (1 - dones[t]) - values[t]\n\n        # Compute GAE advantages (backward pass)\n        gae = 0\n        for t in reversed(range(T)):\n            # A_t = \u03b4_t + (\u03b3\u03bb)A_{t+1}(1 - done_t)\n            gae = deltas[t] + self.config.gamma * self.config.lambda_gae * gae * (1 - dones[t])\n            advantages[t] = gae\n\n        # Compute returns: R_t = A_t + V(s_t)\n        returns = advantages + values[:T]\n\n        return advantages, returns\n\n    def update(\n        self,\n        states: np.ndarray,\n        actions: np.ndarray,\n        old_log_probs: np.ndarray,\n        returns: np.ndarray,\n        advantages: np.ndarray\n    ) -&gt; Dict[str, float]:\n        \"\"\"\n        Update policy using PPO clipped objective.\n\n        Performs multiple epochs of minibatch updates using the PPO loss:\n        L = L^CLIP - c_1 * L^VF + c_2 * H\n\n        where:\n        - L^CLIP: Clipped surrogate objective\n        - L^VF: Value function loss (MSE)\n        - H: Entropy bonus\n\n        Args:\n            states: Batch of states (batch_size, state_dim)\n            actions: Batch of actions (batch_size,)\n            old_log_probs: Old log probabilities (batch_size,)\n            returns: Discounted returns (batch_size,)\n            advantages: Advantage estimates (batch_size,)\n\n        Returns:\n            Dictionary with training metrics:\n            - policy_loss: Policy loss\n            - value_loss: Value function loss\n            - entropy: Policy entropy\n            - total_loss: Combined loss\n\n        Example:\n            &gt;&gt;&gt; ppo = PPO(state_dim=10, action_dim=4)\n            &gt;&gt;&gt; # Collect trajectories...\n            &gt;&gt;&gt; metrics = ppo.update(states, actions, old_log_probs, returns, advantages)\n\n        References:\n        - PPO paper: Section 3 (PPO-Clip algorithm)\n        - Clipped objective prevents large policy updates\n        \"\"\"\n        # Normalize advantages (reduces variance)\n        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n\n        batch_size = states.shape[0]\n        total_metrics = {\n            \"policy_loss\": 0.0,\n            \"value_loss\": 0.0,\n            \"entropy\": 0.0,\n            \"total_loss\": 0.0,\n        }\n\n        # Multiple epochs of updates\n        for epoch in range(self.config.num_epochs):\n            # Shuffle data\n            indices = np.random.permutation(batch_size)\n\n            # Minibatch updates\n            for start in range(0, batch_size, self.config.batch_size):\n                end = min(start + self.config.batch_size, batch_size)\n                batch_indices = indices[start:end]\n\n                # Get minibatch\n                batch_states = states[batch_indices]\n                batch_actions = actions[batch_indices]\n                batch_old_log_probs = old_log_probs[batch_indices]\n                batch_returns = returns[batch_indices]\n                batch_advantages = advantages[batch_indices]\n\n                # Evaluate actions under current policy\n                new_log_probs, values, entropy = self.policy.evaluate_actions(\n                    batch_states, batch_actions\n                )\n\n                # Compute probability ratio: r(\u03b8) = \u03c0_\u03b8(a|s) / \u03c0_\u03b8_old(a|s)\n                ratio = np.exp(new_log_probs - batch_old_log_probs)\n\n                # Compute clipped objective\n                # L^CLIP = E[min(r*A, clip(r, 1-\u03b5, 1+\u03b5)*A)]\n                surr1 = ratio * batch_advantages\n                surr2 = np.clip(ratio, 1 - self.config.epsilon, 1 + self.config.epsilon) * batch_advantages\n                policy_loss = -np.minimum(surr1, surr2).mean()\n\n                # Value function loss: MSE\n                value_loss = ((values - batch_returns) ** 2).mean()\n\n                # Total loss\n                total_loss = (\n                    policy_loss\n                    + self.config.value_coef * value_loss\n                    - self.config.entropy_coef * entropy\n                )\n\n                # Gradient descent (simplified - in practice, use automatic differentiation)\n                # This is a placeholder - real implementation would compute gradients\n                # and update weights using backpropagation\n                self._simple_update(total_loss, batch_states, batch_actions)\n\n                # Track metrics\n                total_metrics[\"policy_loss\"] += policy_loss\n                total_metrics[\"value_loss\"] += value_loss\n                total_metrics[\"entropy\"] += entropy\n                total_metrics[\"total_loss\"] += total_loss\n\n        # Average metrics\n        num_updates = self.config.num_epochs * max(1, (batch_size // self.config.batch_size))\n        for key in total_metrics:\n            total_metrics[key] /= num_updates\n\n        logger.debug(\n            f\"PPO update: policy_loss={total_metrics['policy_loss']:.4f}, \"\n            f\"value_loss={total_metrics['value_loss']:.4f}, \"\n            f\"entropy={total_metrics['entropy']:.4f}\"\n        )\n\n        return total_metrics\n\n    def _simple_update(self, loss: float, states: np.ndarray, actions: np.ndarray):\n        \"\"\"\n        Simplified weight update (placeholder for gradient descent).\n\n        In a full implementation, this would:\n        1. Compute gradients via backpropagation\n        2. Update weights using optimizer (Adam, SGD, etc.)\n\n        For now, this is a minimal placeholder. Upgrade to PyTorch for\n        automatic differentiation and proper optimization.\n\n        TODO: Implement full backpropagation or migrate to PyTorch\n\n        Args:\n            loss: Scalar loss value\n            states: Batch of states\n            actions: Batch of actions\n        \"\"\"\n        # Placeholder: Random small updates (not real gradient descent)\n        # This maintains the interface but should be replaced with proper backprop\n        for name, param in self.policy.weights.items():\n            # Very small random perturbation (NOT a real gradient update)\n            gradient = np.random.randn(*param.shape) * 1e-6\n            param -= self.config.learning_rate * gradient\n\n    def __str__(self) -&gt; str:\n        \"\"\"String representation.\"\"\"\n        return (\n            f\"PPO(lr={self.config.learning_rate}, \"\n            f\"gamma={self.config.gamma}, \"\n            f\"epsilon={self.config.epsilon})\"\n        )\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Developer-friendly representation.\"\"\"\n        return (\n            f\"PPO(state_dim={self.policy.state_dim}, \"\n            f\"action_dim={self.policy.action_dim}, \"\n            f\"config={self.config})\"\n        )\n</code></pre>"},{"location":"api/learning/ppo/#upir.learning.ppo.PPO-functions","title":"Functions","text":""},{"location":"api/learning/ppo/#upir.learning.ppo.PPO.__init__","title":"<code>__init__(state_dim, action_dim, config=None)</code>","text":"<p>Initialize PPO agent.</p> <p>Parameters:</p> Name Type Description Default <code>state_dim</code> <code>int</code> <p>Dimension of state space</p> required <code>action_dim</code> <code>int</code> <p>Dimension of action space</p> required <code>config</code> <code>PPOConfig</code> <p>PPO configuration (uses defaults if None)</p> <code>None</code> Source code in <code>upir/learning/ppo.py</code> <pre><code>def __init__(self, state_dim: int, action_dim: int, config: PPOConfig = None):\n    \"\"\"\n    Initialize PPO agent.\n\n    Args:\n        state_dim: Dimension of state space\n        action_dim: Dimension of action space\n        config: PPO configuration (uses defaults if None)\n    \"\"\"\n    self.config = config or PPOConfig()\n    self.policy = PolicyNetwork(state_dim, action_dim)\n\n    # Optimizer state (simple momentum-based)\n    self.optimizer_state = {\n        name: {\"velocity\": np.zeros_like(param)}\n        for name, param in self.policy.weights.items()\n    }\n\n    logger.info(\n        f\"Initialized PPO: state_dim={state_dim}, action_dim={action_dim}, \"\n        f\"lr={self.config.learning_rate}, epsilon={self.config.epsilon}\"\n    )\n</code></pre>"},{"location":"api/learning/ppo/#upir.learning.ppo.PPO.select_action","title":"<code>select_action(state)</code>","text":"<p>Select action using current policy.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>ndarray</code> <p>Current state vector</p> required <p>Returns:</p> Type Description <code>Tuple[int, float, float]</code> <p>Tuple of (action, log_prob, value)</p> Source code in <code>upir/learning/ppo.py</code> <pre><code>def select_action(self, state: np.ndarray) -&gt; Tuple[int, float, float]:\n    \"\"\"\n    Select action using current policy.\n\n    Args:\n        state: Current state vector\n\n    Returns:\n        Tuple of (action, log_prob, value)\n    \"\"\"\n    return self.policy.get_action(state)\n</code></pre>"},{"location":"api/learning/ppo/#upir.learning.ppo.PPO.compute_gae","title":"<code>compute_gae(rewards, values, dones)</code>","text":"<p>Compute Generalized Advantage Estimation (GAE).</p> <p>GAE uses an exponentially-weighted average of n-step advantages to reduce variance while maintaining low bias. It interpolates between Monte Carlo (high variance, low bias) and TD (low variance, high bias).</p> <p>Formula: \u03b4_t = r_t + \u03b3V(s_{t+1})(1 - done_t) - V(s_t) A_t = \u03b4_t + (\u03b3\u03bb)\u03b4_{t+1} + (\u03b3\u03bb)\u00b2\u03b4_{t+2} + ...</p> <p>Parameters:</p> Name Type Description Default <code>rewards</code> <code>ndarray</code> <p>Rewards received (T,)</p> required <code>values</code> <code>ndarray</code> <p>Value estimates V(s_t) (T,)</p> required <code>dones</code> <code>ndarray</code> <p>Episode termination flags (T,)</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Tuple of (advantages, returns):</p> <code>ndarray</code> <ul> <li>advantages: Advantage estimates A_t (T,)</li> </ul> <code>Tuple[ndarray, ndarray]</code> <ul> <li>returns: Discounted returns (T,)</li> </ul> <p>References: - GAE paper: https://arxiv.org/abs/1506.02438 - OpenAI Spinning Up: GAE explanation</p> Source code in <code>upir/learning/ppo.py</code> <pre><code>def compute_gae(\n    self,\n    rewards: np.ndarray,\n    values: np.ndarray,\n    dones: np.ndarray\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute Generalized Advantage Estimation (GAE).\n\n    GAE uses an exponentially-weighted average of n-step advantages to\n    reduce variance while maintaining low bias. It interpolates between\n    Monte Carlo (high variance, low bias) and TD (low variance, high bias).\n\n    Formula:\n    \u03b4_t = r_t + \u03b3V(s_{t+1})(1 - done_t) - V(s_t)\n    A_t = \u03b4_t + (\u03b3\u03bb)\u03b4_{t+1} + (\u03b3\u03bb)\u00b2\u03b4_{t+2} + ...\n\n    Args:\n        rewards: Rewards received (T,)\n        values: Value estimates V(s_t) (T,)\n        dones: Episode termination flags (T,)\n\n    Returns:\n        Tuple of (advantages, returns):\n        - advantages: Advantage estimates A_t (T,)\n        - returns: Discounted returns (T,)\n\n    References:\n    - GAE paper: https://arxiv.org/abs/1506.02438\n    - OpenAI Spinning Up: GAE explanation\n    \"\"\"\n    T = len(rewards)\n    advantages = np.zeros(T, dtype=np.float32)\n    returns = np.zeros(T, dtype=np.float32)\n\n    # Compute TD errors (deltas)\n    deltas = np.zeros(T, dtype=np.float32)\n    for t in range(T):\n        # \u03b4_t = r_t + \u03b3V(s_{t+1})(1 - done_t) - V(s_t)\n        next_value = values[t + 1] if t + 1 &lt; T else 0.0\n        deltas[t] = rewards[t] + self.config.gamma * next_value * (1 - dones[t]) - values[t]\n\n    # Compute GAE advantages (backward pass)\n    gae = 0\n    for t in reversed(range(T)):\n        # A_t = \u03b4_t + (\u03b3\u03bb)A_{t+1}(1 - done_t)\n        gae = deltas[t] + self.config.gamma * self.config.lambda_gae * gae * (1 - dones[t])\n        advantages[t] = gae\n\n    # Compute returns: R_t = A_t + V(s_t)\n    returns = advantages + values[:T]\n\n    return advantages, returns\n</code></pre>"},{"location":"api/learning/ppo/#upir.learning.ppo.PPO.update","title":"<code>update(states, actions, old_log_probs, returns, advantages)</code>","text":"<p>Update policy using PPO clipped objective.</p> <p>Performs multiple epochs of minibatch updates using the PPO loss: L = L^CLIP - c_1 * L^VF + c_2 * H</p> <p>where: - L^CLIP: Clipped surrogate objective - L^VF: Value function loss (MSE) - H: Entropy bonus</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>ndarray</code> <p>Batch of states (batch_size, state_dim)</p> required <code>actions</code> <code>ndarray</code> <p>Batch of actions (batch_size,)</p> required <code>old_log_probs</code> <code>ndarray</code> <p>Old log probabilities (batch_size,)</p> required <code>returns</code> <code>ndarray</code> <p>Discounted returns (batch_size,)</p> required <code>advantages</code> <code>ndarray</code> <p>Advantage estimates (batch_size,)</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary with training metrics:</p> <code>Dict[str, float]</code> <ul> <li>policy_loss: Policy loss</li> </ul> <code>Dict[str, float]</code> <ul> <li>value_loss: Value function loss</li> </ul> <code>Dict[str, float]</code> <ul> <li>entropy: Policy entropy</li> </ul> <code>Dict[str, float]</code> <ul> <li>total_loss: Combined loss</li> </ul> Example <p>ppo = PPO(state_dim=10, action_dim=4)</p> <p>References: - PPO paper: Section 3 (PPO-Clip algorithm) - Clipped objective prevents large policy updates</p> Source code in <code>upir/learning/ppo.py</code> <pre><code>def update(\n    self,\n    states: np.ndarray,\n    actions: np.ndarray,\n    old_log_probs: np.ndarray,\n    returns: np.ndarray,\n    advantages: np.ndarray\n) -&gt; Dict[str, float]:\n    \"\"\"\n    Update policy using PPO clipped objective.\n\n    Performs multiple epochs of minibatch updates using the PPO loss:\n    L = L^CLIP - c_1 * L^VF + c_2 * H\n\n    where:\n    - L^CLIP: Clipped surrogate objective\n    - L^VF: Value function loss (MSE)\n    - H: Entropy bonus\n\n    Args:\n        states: Batch of states (batch_size, state_dim)\n        actions: Batch of actions (batch_size,)\n        old_log_probs: Old log probabilities (batch_size,)\n        returns: Discounted returns (batch_size,)\n        advantages: Advantage estimates (batch_size,)\n\n    Returns:\n        Dictionary with training metrics:\n        - policy_loss: Policy loss\n        - value_loss: Value function loss\n        - entropy: Policy entropy\n        - total_loss: Combined loss\n\n    Example:\n        &gt;&gt;&gt; ppo = PPO(state_dim=10, action_dim=4)\n        &gt;&gt;&gt; # Collect trajectories...\n        &gt;&gt;&gt; metrics = ppo.update(states, actions, old_log_probs, returns, advantages)\n\n    References:\n    - PPO paper: Section 3 (PPO-Clip algorithm)\n    - Clipped objective prevents large policy updates\n    \"\"\"\n    # Normalize advantages (reduces variance)\n    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n\n    batch_size = states.shape[0]\n    total_metrics = {\n        \"policy_loss\": 0.0,\n        \"value_loss\": 0.0,\n        \"entropy\": 0.0,\n        \"total_loss\": 0.0,\n    }\n\n    # Multiple epochs of updates\n    for epoch in range(self.config.num_epochs):\n        # Shuffle data\n        indices = np.random.permutation(batch_size)\n\n        # Minibatch updates\n        for start in range(0, batch_size, self.config.batch_size):\n            end = min(start + self.config.batch_size, batch_size)\n            batch_indices = indices[start:end]\n\n            # Get minibatch\n            batch_states = states[batch_indices]\n            batch_actions = actions[batch_indices]\n            batch_old_log_probs = old_log_probs[batch_indices]\n            batch_returns = returns[batch_indices]\n            batch_advantages = advantages[batch_indices]\n\n            # Evaluate actions under current policy\n            new_log_probs, values, entropy = self.policy.evaluate_actions(\n                batch_states, batch_actions\n            )\n\n            # Compute probability ratio: r(\u03b8) = \u03c0_\u03b8(a|s) / \u03c0_\u03b8_old(a|s)\n            ratio = np.exp(new_log_probs - batch_old_log_probs)\n\n            # Compute clipped objective\n            # L^CLIP = E[min(r*A, clip(r, 1-\u03b5, 1+\u03b5)*A)]\n            surr1 = ratio * batch_advantages\n            surr2 = np.clip(ratio, 1 - self.config.epsilon, 1 + self.config.epsilon) * batch_advantages\n            policy_loss = -np.minimum(surr1, surr2).mean()\n\n            # Value function loss: MSE\n            value_loss = ((values - batch_returns) ** 2).mean()\n\n            # Total loss\n            total_loss = (\n                policy_loss\n                + self.config.value_coef * value_loss\n                - self.config.entropy_coef * entropy\n            )\n\n            # Gradient descent (simplified - in practice, use automatic differentiation)\n            # This is a placeholder - real implementation would compute gradients\n            # and update weights using backpropagation\n            self._simple_update(total_loss, batch_states, batch_actions)\n\n            # Track metrics\n            total_metrics[\"policy_loss\"] += policy_loss\n            total_metrics[\"value_loss\"] += value_loss\n            total_metrics[\"entropy\"] += entropy\n            total_metrics[\"total_loss\"] += total_loss\n\n    # Average metrics\n    num_updates = self.config.num_epochs * max(1, (batch_size // self.config.batch_size))\n    for key in total_metrics:\n        total_metrics[key] /= num_updates\n\n    logger.debug(\n        f\"PPO update: policy_loss={total_metrics['policy_loss']:.4f}, \"\n        f\"value_loss={total_metrics['value_loss']:.4f}, \"\n        f\"entropy={total_metrics['entropy']:.4f}\"\n    )\n\n    return total_metrics\n</code></pre>"},{"location":"api/learning/ppo/#upir.learning.ppo.PPO.update--collect-trajectories","title":"Collect trajectories...","text":"<p>metrics = ppo.update(states, actions, old_log_probs, returns, advantages)</p>"},{"location":"api/learning/ppo/#upir.learning.ppo.PPO.__str__","title":"<code>__str__()</code>","text":"<p>String representation.</p> Source code in <code>upir/learning/ppo.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"String representation.\"\"\"\n    return (\n        f\"PPO(lr={self.config.learning_rate}, \"\n        f\"gamma={self.config.gamma}, \"\n        f\"epsilon={self.config.epsilon})\"\n    )\n</code></pre>"},{"location":"api/learning/ppo/#upir.learning.ppo.PPO.__repr__","title":"<code>__repr__()</code>","text":"<p>Developer-friendly representation.</p> Source code in <code>upir/learning/ppo.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Developer-friendly representation.\"\"\"\n    return (\n        f\"PPO(state_dim={self.policy.state_dim}, \"\n        f\"action_dim={self.policy.action_dim}, \"\n        f\"config={self.config})\"\n    )\n</code></pre>"},{"location":"api/learning/ppo/#see-also","title":"See Also","text":"<ul> <li>RL Optimizer - High-level RL API</li> </ul>"},{"location":"api/patterns/extractor/","title":"Pattern Extractor","text":"<p>Extract architectural patterns from UPIR instances.</p>"},{"location":"api/patterns/extractor/#overview","title":"Overview","text":"<p>The <code>PatternExtractor</code> extracts reusable patterns from verified architectures using feature extraction and clustering.</p>"},{"location":"api/patterns/extractor/#class-documentation","title":"Class Documentation","text":""},{"location":"api/patterns/extractor/#upir.patterns.extractor.PatternExtractor","title":"<code>upir.patterns.extractor.PatternExtractor</code>","text":"<p>Extract architectural patterns from UPIR instances using clustering.</p> <p>Discovers common architectural patterns by: 1. Extracting features from each UPIR architecture 2. Clustering similar architectures using KMeans 3. Analyzing each cluster to identify common structure 4. Creating pattern templates from clusters</p> <p>The extractor can identify patterns like \"streaming ETL\", \"API gateway\", \"batch processing\", etc., based on structural similarities.</p> <p>Attributes:</p> Name Type Description <code>n_clusters</code> <p>Number of clusters for KMeans (default 10)</p> <code>scaler</code> <p>StandardScaler for feature normalization</p> <code>kmeans</code> <p>KMeans clustering model</p> <code>feature_dim</code> <p>Dimension of feature vectors</p> Example <p>extractor = PatternExtractor(n_clusters=5) upirs = [...]  # List of UPIR instances patterns = extractor.discover_patterns(upirs) for pattern in patterns: ...     print(f\"{pattern.name}: {len(pattern.instances)} instances\")</p> <p>References: - KMeans: Partitional clustering algorithm - TD Commons: Pattern extraction for architecture optimization - Feature engineering for architectural analysis</p> Source code in <code>upir/patterns/extractor.py</code> <pre><code>class PatternExtractor:\n    \"\"\"\n    Extract architectural patterns from UPIR instances using clustering.\n\n    Discovers common architectural patterns by:\n    1. Extracting features from each UPIR architecture\n    2. Clustering similar architectures using KMeans\n    3. Analyzing each cluster to identify common structure\n    4. Creating pattern templates from clusters\n\n    The extractor can identify patterns like \"streaming ETL\", \"API gateway\",\n    \"batch processing\", etc., based on structural similarities.\n\n    Attributes:\n        n_clusters: Number of clusters for KMeans (default 10)\n        scaler: StandardScaler for feature normalization\n        kmeans: KMeans clustering model\n        feature_dim: Dimension of feature vectors\n\n    Example:\n        &gt;&gt;&gt; extractor = PatternExtractor(n_clusters=5)\n        &gt;&gt;&gt; upirs = [...]  # List of UPIR instances\n        &gt;&gt;&gt; patterns = extractor.discover_patterns(upirs)\n        &gt;&gt;&gt; for pattern in patterns:\n        ...     print(f\"{pattern.name}: {len(pattern.instances)} instances\")\n\n    References:\n    - KMeans: Partitional clustering algorithm\n    - TD Commons: Pattern extraction for architecture optimization\n    - Feature engineering for architectural analysis\n    \"\"\"\n\n    def __init__(self, n_clusters: int = 10, feature_dim: int = 32):\n        \"\"\"\n        Initialize pattern extractor.\n\n        Args:\n            n_clusters: Number of patterns to discover\n            feature_dim: Dimension of feature vectors (default 32)\n        \"\"\"\n        self.n_clusters = n_clusters\n        self.feature_dim = feature_dim\n        self.scaler = StandardScaler()\n        self.kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n\n        logger.info(\n            f\"Initialized PatternExtractor: n_clusters={n_clusters}, \"\n            f\"feature_dim={feature_dim}\"\n        )\n\n    def extract_features(self, upir: UPIR) -&gt; np.ndarray:\n        \"\"\"\n        Extract feature vector from UPIR architecture.\n\n        Extracts architectural features and creates a fixed-size normalized\n        feature vector suitable for clustering. Features include:\n        - Component count\n        - Connection density (connections / components)\n        - Deployment type (one-hot encoding)\n        - Component type distribution\n        - Constraint profile (latency, throughput requirements)\n\n        Args:\n            upir: UPIR instance to extract features from\n\n        Returns:\n            Feature vector (feature_dim,) normalized to [0, 1]\n\n        Example:\n            &gt;&gt;&gt; extractor = PatternExtractor()\n            &gt;&gt;&gt; upir = UPIR(id=\"test\", name=\"Test\", description=\"Test\")\n            &gt;&gt;&gt; features = extractor.extract_features(upir)\n            &gt;&gt;&gt; features.shape\n            (32,)\n        \"\"\"\n        features = []\n\n        if upir.architecture is None:\n            # No architecture - return zero vector\n            return np.zeros(self.feature_dim)\n\n        arch = upir.architecture\n\n        # Basic structural features\n        num_components = len(arch.components)\n        num_connections = len(arch.connections)\n\n        features.append(num_components)\n        features.append(num_connections)\n\n        # Connection density\n        connection_density = num_connections / max(num_components, 1)\n        features.append(connection_density)\n\n        # Component type distribution (one-hot for common types)\n        component_types = Counter()\n        for comp in arch.components:\n            if isinstance(comp, dict):\n                comp_type = comp.get(\"type\", \"unknown\")\n            else:\n                comp_type = \"unknown\"\n            component_types[comp_type] += 1\n\n        # Top component types (streaming, batch, api, database, cache)\n        common_types = [\"streaming\", \"batch\", \"api\", \"database\", \"cache\"]\n        for comp_type in common_types:\n            count = sum(component_types[k] for k in component_types if comp_type in k.lower())\n            features.append(count / max(num_components, 1))\n\n        # Deployment pattern (based on deployment config)\n        deployment_types = [\"single_region\", \"multi_region\", \"distributed\", \"serverless\"]\n        deployment = arch.deployment if hasattr(arch, 'deployment') else {}\n        if isinstance(deployment, dict):\n            deployment_type = deployment.get(\"type\", \"unknown\")\n        else:\n            deployment_type = \"unknown\"\n\n        for dep_type in deployment_types:\n            features.append(1.0 if dep_type in deployment_type.lower() else 0.0)\n\n        # Constraint profile from specification\n        if upir.specification:\n            spec = upir.specification\n\n            # Latency constraints\n            has_latency = any(\n                prop.time_bound is not None\n                for prop in spec.properties + spec.invariants\n            )\n            features.append(1.0 if has_latency else 0.0)\n\n            # Min latency requirement (normalized)\n            min_latency = min(\n                (prop.time_bound for prop in spec.properties + spec.invariants\n                 if prop.time_bound is not None),\n                default=0\n            )\n            features.append(min_latency / 10000.0)  # Normalize to [0, 1]\n\n            # Throughput requirements (heuristic from predicates)\n            has_throughput = any(\n                \"throughput\" in prop.predicate.lower() or \"qps\" in prop.predicate.lower()\n                for prop in spec.properties + spec.invariants\n            )\n            features.append(1.0 if has_throughput else 0.0)\n\n            # Number of constraints\n            num_constraints = len(spec.properties) + len(spec.invariants)\n            features.append(num_constraints / 10.0)  # Normalize\n        else:\n            # No specification - add zeros\n            features.extend([0.0] * 4)\n\n        # Pad or truncate to fixed size\n        features = np.array(features)\n        if len(features) &lt; self.feature_dim:\n            features = np.pad(features, (0, self.feature_dim - len(features)), mode='constant')\n        else:\n            features = features[:self.feature_dim]\n\n        # Clip to [0, 1] for numerical stability\n        features = np.clip(features, 0.0, 1.0)\n\n        return features.astype(np.float32)\n\n    def cluster_architectures(self, upirs: List[UPIR]) -&gt; Dict[int, List[UPIR]]:\n        \"\"\"\n        Cluster similar architectures using KMeans.\n\n        Groups UPIRs with similar architectural features into clusters.\n        Each cluster represents a potential pattern.\n\n        Args:\n            upirs: List of UPIR instances to cluster\n\n        Returns:\n            Dictionary mapping cluster_id -&gt; list of UPIRs in that cluster\n\n        Example:\n            &gt;&gt;&gt; extractor = PatternExtractor(n_clusters=3)\n            &gt;&gt;&gt; upirs = [...]  # List of UPIRs\n            &gt;&gt;&gt; clusters = extractor.cluster_architectures(upirs)\n            &gt;&gt;&gt; for cluster_id, cluster_upirs in clusters.items():\n            ...     print(f\"Cluster {cluster_id}: {len(cluster_upirs)} UPIRs\")\n\n        References:\n        - KMeans: Minimizes within-cluster sum of squares\n        - Feature normalization improves clustering quality\n        \"\"\"\n        if not upirs:\n            logger.warning(\"Empty UPIR list provided for clustering\")\n            return {}\n\n        if len(upirs) &lt; self.n_clusters:\n            logger.warning(\n                f\"Only {len(upirs)} UPIRs but {self.n_clusters} clusters requested. \"\n                f\"Using {len(upirs)} clusters instead.\"\n            )\n            self.n_clusters = len(upirs)\n            self.kmeans = KMeans(n_clusters=self.n_clusters, random_state=42, n_init=10)\n\n        # Extract features from all UPIRs\n        feature_matrix = np.array([self.extract_features(upir) for upir in upirs])\n\n        # Normalize features\n        feature_matrix_normalized = self.scaler.fit_transform(feature_matrix)\n\n        # Cluster\n        labels = self.kmeans.fit_predict(feature_matrix_normalized)\n\n        # Group UPIRs by cluster\n        clusters = {}\n        for cluster_id in range(self.n_clusters):\n            clusters[cluster_id] = []\n\n        for upir, label in zip(upirs, labels):\n            clusters[label].append(upir)\n\n        logger.info(\n            f\"Clustered {len(upirs)} UPIRs into {self.n_clusters} clusters. \"\n            f\"Sizes: {[len(clusters[i]) for i in range(self.n_clusters)]}\"\n        )\n\n        return clusters\n\n    def extract_pattern(self, cluster: List[UPIR], cluster_id: int) -&gt; Pattern:\n        \"\"\"\n        Extract common pattern from a cluster of similar architectures.\n\n        Analyzes UPIRs in the cluster to identify common structure and\n        creates a reusable pattern template.\n\n        Args:\n            cluster: List of UPIRs in the cluster\n            cluster_id: Cluster identifier\n\n        Returns:\n            Pattern extracted from the cluster\n\n        Example:\n            &gt;&gt;&gt; extractor = PatternExtractor()\n            &gt;&gt;&gt; cluster_upirs = [...]  # UPIRs from same cluster\n            &gt;&gt;&gt; pattern = extractor.extract_pattern(cluster_upirs, cluster_id=0)\n            &gt;&gt;&gt; print(pattern.name)\n\n        References:\n        - Mode for categorical features\n        - Average for numerical features\n        - Template parameterization\n        \"\"\"\n        if not cluster:\n            # Empty cluster - return empty pattern\n            return Pattern(\n                id=f\"pattern-{cluster_id}\",\n                name=f\"Pattern {cluster_id}\",\n                description=\"Empty pattern\",\n                template={}\n            )\n\n        # Collect statistics from cluster\n        component_types = Counter()\n        connection_counts = []\n        component_counts = []\n        has_specifications = 0\n        performance_metrics = []\n\n        for upir in cluster:\n            if upir.architecture:\n                arch = upir.architecture\n\n                # Count components by type\n                for comp in arch.components:\n                    if isinstance(comp, dict):\n                        comp_type = comp.get(\"type\", \"unknown\")\n                        component_types[comp_type] += 1\n\n                component_counts.append(len(arch.components))\n                connection_counts.append(len(arch.connections))\n\n            if upir.specification:\n                has_specifications += 1\n\n        # Identify most common component types\n        top_types = component_types.most_common(5)\n\n        # Create template structure\n        template_components = []\n        for comp_type, count in top_types:\n            template_components.append({\n                \"type\": comp_type,\n                \"count\": count / len(cluster),  # Average count per UPIR\n                \"properties\": {}\n            })\n\n        # Average metrics\n        avg_components = np.mean(component_counts) if component_counts else 0\n        avg_connections = np.mean(connection_counts) if connection_counts else 0\n\n        # Compute pattern centroid for matching\n        feature_matrix = np.array([self.extract_features(upir) for upir in cluster])\n        centroid = np.mean(feature_matrix, axis=0).tolist()\n\n        # Create pattern name (heuristic from top component type)\n        if top_types:\n            top_type = top_types[0][0]\n            pattern_name = f\"{top_type.replace('_', ' ').title()} Pattern\"\n        else:\n            pattern_name = f\"Pattern {cluster_id}\"\n\n        # Create description\n        description = (\n            f\"Architectural pattern with {avg_components:.1f} components \"\n            f\"and {avg_connections:.1f} connections on average. \"\n            f\"Common component types: {', '.join(t for t, _ in top_types[:3])}. \"\n            f\"Specification coverage: {has_specifications}/{len(cluster)}.\"\n        )\n\n        # Create pattern\n        pattern = Pattern(\n            id=f\"pattern-{cluster_id}\",\n            name=pattern_name,\n            description=description,\n            template={\n                \"components\": template_components,\n                \"parameters\": {\n                    \"avg_component_count\": avg_components,\n                    \"avg_connection_count\": avg_connections,\n                },\n                \"centroid\": centroid,\n            },\n            instances=[upir.id for upir in cluster],\n            success_rate=has_specifications / len(cluster) if cluster else 0.0,\n            average_performance={}\n        )\n\n        logger.debug(\n            f\"Extracted pattern: {pattern.name} from {len(cluster)} UPIRs\"\n        )\n\n        return pattern\n\n    def discover_patterns(self, upirs: List[UPIR]) -&gt; List[Pattern]:\n        \"\"\"\n        Discover architectural patterns from UPIR instances.\n\n        Main entry point for pattern extraction. Performs clustering and\n        pattern extraction in one pipeline.\n\n        Args:\n            upirs: List of UPIR instances to analyze\n\n        Returns:\n            List of discovered patterns\n\n        Example:\n            &gt;&gt;&gt; extractor = PatternExtractor(n_clusters=5)\n            &gt;&gt;&gt; upirs = [...]  # Collection of UPIR instances\n            &gt;&gt;&gt; patterns = extractor.discover_patterns(upirs)\n            &gt;&gt;&gt; print(f\"Discovered {len(patterns)} patterns\")\n            &gt;&gt;&gt; for pattern in patterns:\n            ...     print(f\"  - {pattern.name}: {len(pattern.instances)} instances\")\n\n        References:\n        - TD Commons: Pattern-based architecture optimization\n        - Unsupervised learning for pattern discovery\n        \"\"\"\n        if not upirs:\n            logger.warning(\"No UPIRs provided for pattern discovery\")\n            return []\n\n        logger.info(f\"Starting pattern discovery for {len(upirs)} UPIRs\")\n\n        # Cluster architectures\n        clusters = self.cluster_architectures(upirs)\n\n        # Extract pattern from each cluster\n        patterns = []\n        for cluster_id, cluster_upirs in clusters.items():\n            if cluster_upirs:  # Skip empty clusters\n                pattern = self.extract_pattern(cluster_upirs, cluster_id)\n                patterns.append(pattern)\n\n        # Sort patterns by instance count (descending)\n        patterns.sort(key=lambda p: len(p.instances), reverse=True)\n\n        logger.info(\n            f\"Discovered {len(patterns)} patterns. \"\n            f\"Top pattern: {patterns[0].name} with {len(patterns[0].instances)} instances\"\n            if patterns else \"No patterns discovered\"\n        )\n\n        return patterns\n\n    def classify_upir(self, upir: UPIR) -&gt; int:\n        \"\"\"\n        Classify a UPIR into an existing cluster/pattern.\n\n        Uses the trained KMeans model to predict which cluster a new UPIR\n        belongs to.\n\n        Args:\n            upir: UPIR to classify\n\n        Returns:\n            Cluster ID (0 to n_clusters-1)\n\n        Example:\n            &gt;&gt;&gt; extractor = PatternExtractor()\n            &gt;&gt;&gt; extractor.discover_patterns(training_upirs)\n            &gt;&gt;&gt; new_upir = UPIR(...)\n            &gt;&gt;&gt; cluster_id = extractor.classify_upir(new_upir)\n        \"\"\"\n        features = self.extract_features(upir)\n        features_normalized = self.scaler.transform(features.reshape(1, -1))\n        cluster_id = self.kmeans.predict(features_normalized)[0]\n        return cluster_id\n\n    def __str__(self) -&gt; str:\n        \"\"\"String representation.\"\"\"\n        return f\"PatternExtractor(n_clusters={self.n_clusters})\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Developer-friendly representation.\"\"\"\n        return (\n            f\"PatternExtractor(n_clusters={self.n_clusters}, \"\n            f\"feature_dim={self.feature_dim})\"\n        )\n</code></pre>"},{"location":"api/patterns/extractor/#upir.patterns.extractor.PatternExtractor-functions","title":"Functions","text":""},{"location":"api/patterns/extractor/#upir.patterns.extractor.PatternExtractor.__init__","title":"<code>__init__(n_clusters=10, feature_dim=32)</code>","text":"<p>Initialize pattern extractor.</p> <p>Parameters:</p> Name Type Description Default <code>n_clusters</code> <code>int</code> <p>Number of patterns to discover</p> <code>10</code> <code>feature_dim</code> <code>int</code> <p>Dimension of feature vectors (default 32)</p> <code>32</code> Source code in <code>upir/patterns/extractor.py</code> <pre><code>def __init__(self, n_clusters: int = 10, feature_dim: int = 32):\n    \"\"\"\n    Initialize pattern extractor.\n\n    Args:\n        n_clusters: Number of patterns to discover\n        feature_dim: Dimension of feature vectors (default 32)\n    \"\"\"\n    self.n_clusters = n_clusters\n    self.feature_dim = feature_dim\n    self.scaler = StandardScaler()\n    self.kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n\n    logger.info(\n        f\"Initialized PatternExtractor: n_clusters={n_clusters}, \"\n        f\"feature_dim={feature_dim}\"\n    )\n</code></pre>"},{"location":"api/patterns/extractor/#upir.patterns.extractor.PatternExtractor.extract_features","title":"<code>extract_features(upir)</code>","text":"<p>Extract feature vector from UPIR architecture.</p> <p>Extracts architectural features and creates a fixed-size normalized feature vector suitable for clustering. Features include: - Component count - Connection density (connections / components) - Deployment type (one-hot encoding) - Component type distribution - Constraint profile (latency, throughput requirements)</p> <p>Parameters:</p> Name Type Description Default <code>upir</code> <code>UPIR</code> <p>UPIR instance to extract features from</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Feature vector (feature_dim,) normalized to [0, 1]</p> Example <p>extractor = PatternExtractor() upir = UPIR(id=\"test\", name=\"Test\", description=\"Test\") features = extractor.extract_features(upir) features.shape (32,)</p> Source code in <code>upir/patterns/extractor.py</code> <pre><code>def extract_features(self, upir: UPIR) -&gt; np.ndarray:\n    \"\"\"\n    Extract feature vector from UPIR architecture.\n\n    Extracts architectural features and creates a fixed-size normalized\n    feature vector suitable for clustering. Features include:\n    - Component count\n    - Connection density (connections / components)\n    - Deployment type (one-hot encoding)\n    - Component type distribution\n    - Constraint profile (latency, throughput requirements)\n\n    Args:\n        upir: UPIR instance to extract features from\n\n    Returns:\n        Feature vector (feature_dim,) normalized to [0, 1]\n\n    Example:\n        &gt;&gt;&gt; extractor = PatternExtractor()\n        &gt;&gt;&gt; upir = UPIR(id=\"test\", name=\"Test\", description=\"Test\")\n        &gt;&gt;&gt; features = extractor.extract_features(upir)\n        &gt;&gt;&gt; features.shape\n        (32,)\n    \"\"\"\n    features = []\n\n    if upir.architecture is None:\n        # No architecture - return zero vector\n        return np.zeros(self.feature_dim)\n\n    arch = upir.architecture\n\n    # Basic structural features\n    num_components = len(arch.components)\n    num_connections = len(arch.connections)\n\n    features.append(num_components)\n    features.append(num_connections)\n\n    # Connection density\n    connection_density = num_connections / max(num_components, 1)\n    features.append(connection_density)\n\n    # Component type distribution (one-hot for common types)\n    component_types = Counter()\n    for comp in arch.components:\n        if isinstance(comp, dict):\n            comp_type = comp.get(\"type\", \"unknown\")\n        else:\n            comp_type = \"unknown\"\n        component_types[comp_type] += 1\n\n    # Top component types (streaming, batch, api, database, cache)\n    common_types = [\"streaming\", \"batch\", \"api\", \"database\", \"cache\"]\n    for comp_type in common_types:\n        count = sum(component_types[k] for k in component_types if comp_type in k.lower())\n        features.append(count / max(num_components, 1))\n\n    # Deployment pattern (based on deployment config)\n    deployment_types = [\"single_region\", \"multi_region\", \"distributed\", \"serverless\"]\n    deployment = arch.deployment if hasattr(arch, 'deployment') else {}\n    if isinstance(deployment, dict):\n        deployment_type = deployment.get(\"type\", \"unknown\")\n    else:\n        deployment_type = \"unknown\"\n\n    for dep_type in deployment_types:\n        features.append(1.0 if dep_type in deployment_type.lower() else 0.0)\n\n    # Constraint profile from specification\n    if upir.specification:\n        spec = upir.specification\n\n        # Latency constraints\n        has_latency = any(\n            prop.time_bound is not None\n            for prop in spec.properties + spec.invariants\n        )\n        features.append(1.0 if has_latency else 0.0)\n\n        # Min latency requirement (normalized)\n        min_latency = min(\n            (prop.time_bound for prop in spec.properties + spec.invariants\n             if prop.time_bound is not None),\n            default=0\n        )\n        features.append(min_latency / 10000.0)  # Normalize to [0, 1]\n\n        # Throughput requirements (heuristic from predicates)\n        has_throughput = any(\n            \"throughput\" in prop.predicate.lower() or \"qps\" in prop.predicate.lower()\n            for prop in spec.properties + spec.invariants\n        )\n        features.append(1.0 if has_throughput else 0.0)\n\n        # Number of constraints\n        num_constraints = len(spec.properties) + len(spec.invariants)\n        features.append(num_constraints / 10.0)  # Normalize\n    else:\n        # No specification - add zeros\n        features.extend([0.0] * 4)\n\n    # Pad or truncate to fixed size\n    features = np.array(features)\n    if len(features) &lt; self.feature_dim:\n        features = np.pad(features, (0, self.feature_dim - len(features)), mode='constant')\n    else:\n        features = features[:self.feature_dim]\n\n    # Clip to [0, 1] for numerical stability\n    features = np.clip(features, 0.0, 1.0)\n\n    return features.astype(np.float32)\n</code></pre>"},{"location":"api/patterns/extractor/#upir.patterns.extractor.PatternExtractor.cluster_architectures","title":"<code>cluster_architectures(upirs)</code>","text":"<p>Cluster similar architectures using KMeans.</p> <p>Groups UPIRs with similar architectural features into clusters. Each cluster represents a potential pattern.</p> <p>Parameters:</p> Name Type Description Default <code>upirs</code> <code>List[UPIR]</code> <p>List of UPIR instances to cluster</p> required <p>Returns:</p> Type Description <code>Dict[int, List[UPIR]]</code> <p>Dictionary mapping cluster_id -&gt; list of UPIRs in that cluster</p> Example <p>extractor = PatternExtractor(n_clusters=3) upirs = [...]  # List of UPIRs clusters = extractor.cluster_architectures(upirs) for cluster_id, cluster_upirs in clusters.items(): ...     print(f\"Cluster {cluster_id}: {len(cluster_upirs)} UPIRs\")</p> <p>References: - KMeans: Minimizes within-cluster sum of squares - Feature normalization improves clustering quality</p> Source code in <code>upir/patterns/extractor.py</code> <pre><code>def cluster_architectures(self, upirs: List[UPIR]) -&gt; Dict[int, List[UPIR]]:\n    \"\"\"\n    Cluster similar architectures using KMeans.\n\n    Groups UPIRs with similar architectural features into clusters.\n    Each cluster represents a potential pattern.\n\n    Args:\n        upirs: List of UPIR instances to cluster\n\n    Returns:\n        Dictionary mapping cluster_id -&gt; list of UPIRs in that cluster\n\n    Example:\n        &gt;&gt;&gt; extractor = PatternExtractor(n_clusters=3)\n        &gt;&gt;&gt; upirs = [...]  # List of UPIRs\n        &gt;&gt;&gt; clusters = extractor.cluster_architectures(upirs)\n        &gt;&gt;&gt; for cluster_id, cluster_upirs in clusters.items():\n        ...     print(f\"Cluster {cluster_id}: {len(cluster_upirs)} UPIRs\")\n\n    References:\n    - KMeans: Minimizes within-cluster sum of squares\n    - Feature normalization improves clustering quality\n    \"\"\"\n    if not upirs:\n        logger.warning(\"Empty UPIR list provided for clustering\")\n        return {}\n\n    if len(upirs) &lt; self.n_clusters:\n        logger.warning(\n            f\"Only {len(upirs)} UPIRs but {self.n_clusters} clusters requested. \"\n            f\"Using {len(upirs)} clusters instead.\"\n        )\n        self.n_clusters = len(upirs)\n        self.kmeans = KMeans(n_clusters=self.n_clusters, random_state=42, n_init=10)\n\n    # Extract features from all UPIRs\n    feature_matrix = np.array([self.extract_features(upir) for upir in upirs])\n\n    # Normalize features\n    feature_matrix_normalized = self.scaler.fit_transform(feature_matrix)\n\n    # Cluster\n    labels = self.kmeans.fit_predict(feature_matrix_normalized)\n\n    # Group UPIRs by cluster\n    clusters = {}\n    for cluster_id in range(self.n_clusters):\n        clusters[cluster_id] = []\n\n    for upir, label in zip(upirs, labels):\n        clusters[label].append(upir)\n\n    logger.info(\n        f\"Clustered {len(upirs)} UPIRs into {self.n_clusters} clusters. \"\n        f\"Sizes: {[len(clusters[i]) for i in range(self.n_clusters)]}\"\n    )\n\n    return clusters\n</code></pre>"},{"location":"api/patterns/extractor/#upir.patterns.extractor.PatternExtractor.extract_pattern","title":"<code>extract_pattern(cluster, cluster_id)</code>","text":"<p>Extract common pattern from a cluster of similar architectures.</p> <p>Analyzes UPIRs in the cluster to identify common structure and creates a reusable pattern template.</p> <p>Parameters:</p> Name Type Description Default <code>cluster</code> <code>List[UPIR]</code> <p>List of UPIRs in the cluster</p> required <code>cluster_id</code> <code>int</code> <p>Cluster identifier</p> required <p>Returns:</p> Type Description <code>Pattern</code> <p>Pattern extracted from the cluster</p> Example <p>extractor = PatternExtractor() cluster_upirs = [...]  # UPIRs from same cluster pattern = extractor.extract_pattern(cluster_upirs, cluster_id=0) print(pattern.name)</p> <p>References: - Mode for categorical features - Average for numerical features - Template parameterization</p> Source code in <code>upir/patterns/extractor.py</code> <pre><code>def extract_pattern(self, cluster: List[UPIR], cluster_id: int) -&gt; Pattern:\n    \"\"\"\n    Extract common pattern from a cluster of similar architectures.\n\n    Analyzes UPIRs in the cluster to identify common structure and\n    creates a reusable pattern template.\n\n    Args:\n        cluster: List of UPIRs in the cluster\n        cluster_id: Cluster identifier\n\n    Returns:\n        Pattern extracted from the cluster\n\n    Example:\n        &gt;&gt;&gt; extractor = PatternExtractor()\n        &gt;&gt;&gt; cluster_upirs = [...]  # UPIRs from same cluster\n        &gt;&gt;&gt; pattern = extractor.extract_pattern(cluster_upirs, cluster_id=0)\n        &gt;&gt;&gt; print(pattern.name)\n\n    References:\n    - Mode for categorical features\n    - Average for numerical features\n    - Template parameterization\n    \"\"\"\n    if not cluster:\n        # Empty cluster - return empty pattern\n        return Pattern(\n            id=f\"pattern-{cluster_id}\",\n            name=f\"Pattern {cluster_id}\",\n            description=\"Empty pattern\",\n            template={}\n        )\n\n    # Collect statistics from cluster\n    component_types = Counter()\n    connection_counts = []\n    component_counts = []\n    has_specifications = 0\n    performance_metrics = []\n\n    for upir in cluster:\n        if upir.architecture:\n            arch = upir.architecture\n\n            # Count components by type\n            for comp in arch.components:\n                if isinstance(comp, dict):\n                    comp_type = comp.get(\"type\", \"unknown\")\n                    component_types[comp_type] += 1\n\n            component_counts.append(len(arch.components))\n            connection_counts.append(len(arch.connections))\n\n        if upir.specification:\n            has_specifications += 1\n\n    # Identify most common component types\n    top_types = component_types.most_common(5)\n\n    # Create template structure\n    template_components = []\n    for comp_type, count in top_types:\n        template_components.append({\n            \"type\": comp_type,\n            \"count\": count / len(cluster),  # Average count per UPIR\n            \"properties\": {}\n        })\n\n    # Average metrics\n    avg_components = np.mean(component_counts) if component_counts else 0\n    avg_connections = np.mean(connection_counts) if connection_counts else 0\n\n    # Compute pattern centroid for matching\n    feature_matrix = np.array([self.extract_features(upir) for upir in cluster])\n    centroid = np.mean(feature_matrix, axis=0).tolist()\n\n    # Create pattern name (heuristic from top component type)\n    if top_types:\n        top_type = top_types[0][0]\n        pattern_name = f\"{top_type.replace('_', ' ').title()} Pattern\"\n    else:\n        pattern_name = f\"Pattern {cluster_id}\"\n\n    # Create description\n    description = (\n        f\"Architectural pattern with {avg_components:.1f} components \"\n        f\"and {avg_connections:.1f} connections on average. \"\n        f\"Common component types: {', '.join(t for t, _ in top_types[:3])}. \"\n        f\"Specification coverage: {has_specifications}/{len(cluster)}.\"\n    )\n\n    # Create pattern\n    pattern = Pattern(\n        id=f\"pattern-{cluster_id}\",\n        name=pattern_name,\n        description=description,\n        template={\n            \"components\": template_components,\n            \"parameters\": {\n                \"avg_component_count\": avg_components,\n                \"avg_connection_count\": avg_connections,\n            },\n            \"centroid\": centroid,\n        },\n        instances=[upir.id for upir in cluster],\n        success_rate=has_specifications / len(cluster) if cluster else 0.0,\n        average_performance={}\n    )\n\n    logger.debug(\n        f\"Extracted pattern: {pattern.name} from {len(cluster)} UPIRs\"\n    )\n\n    return pattern\n</code></pre>"},{"location":"api/patterns/extractor/#upir.patterns.extractor.PatternExtractor.discover_patterns","title":"<code>discover_patterns(upirs)</code>","text":"<p>Discover architectural patterns from UPIR instances.</p> <p>Main entry point for pattern extraction. Performs clustering and pattern extraction in one pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>upirs</code> <code>List[UPIR]</code> <p>List of UPIR instances to analyze</p> required <p>Returns:</p> Type Description <code>List[Pattern]</code> <p>List of discovered patterns</p> Example <p>extractor = PatternExtractor(n_clusters=5) upirs = [...]  # Collection of UPIR instances patterns = extractor.discover_patterns(upirs) print(f\"Discovered {len(patterns)} patterns\") for pattern in patterns: ...     print(f\"  - {pattern.name}: {len(pattern.instances)} instances\")</p> <p>References: - TD Commons: Pattern-based architecture optimization - Unsupervised learning for pattern discovery</p> Source code in <code>upir/patterns/extractor.py</code> <pre><code>def discover_patterns(self, upirs: List[UPIR]) -&gt; List[Pattern]:\n    \"\"\"\n    Discover architectural patterns from UPIR instances.\n\n    Main entry point for pattern extraction. Performs clustering and\n    pattern extraction in one pipeline.\n\n    Args:\n        upirs: List of UPIR instances to analyze\n\n    Returns:\n        List of discovered patterns\n\n    Example:\n        &gt;&gt;&gt; extractor = PatternExtractor(n_clusters=5)\n        &gt;&gt;&gt; upirs = [...]  # Collection of UPIR instances\n        &gt;&gt;&gt; patterns = extractor.discover_patterns(upirs)\n        &gt;&gt;&gt; print(f\"Discovered {len(patterns)} patterns\")\n        &gt;&gt;&gt; for pattern in patterns:\n        ...     print(f\"  - {pattern.name}: {len(pattern.instances)} instances\")\n\n    References:\n    - TD Commons: Pattern-based architecture optimization\n    - Unsupervised learning for pattern discovery\n    \"\"\"\n    if not upirs:\n        logger.warning(\"No UPIRs provided for pattern discovery\")\n        return []\n\n    logger.info(f\"Starting pattern discovery for {len(upirs)} UPIRs\")\n\n    # Cluster architectures\n    clusters = self.cluster_architectures(upirs)\n\n    # Extract pattern from each cluster\n    patterns = []\n    for cluster_id, cluster_upirs in clusters.items():\n        if cluster_upirs:  # Skip empty clusters\n            pattern = self.extract_pattern(cluster_upirs, cluster_id)\n            patterns.append(pattern)\n\n    # Sort patterns by instance count (descending)\n    patterns.sort(key=lambda p: len(p.instances), reverse=True)\n\n    logger.info(\n        f\"Discovered {len(patterns)} patterns. \"\n        f\"Top pattern: {patterns[0].name} with {len(patterns[0].instances)} instances\"\n        if patterns else \"No patterns discovered\"\n    )\n\n    return patterns\n</code></pre>"},{"location":"api/patterns/extractor/#upir.patterns.extractor.PatternExtractor.classify_upir","title":"<code>classify_upir(upir)</code>","text":"<p>Classify a UPIR into an existing cluster/pattern.</p> <p>Uses the trained KMeans model to predict which cluster a new UPIR belongs to.</p> <p>Parameters:</p> Name Type Description Default <code>upir</code> <code>UPIR</code> <p>UPIR to classify</p> required <p>Returns:</p> Type Description <code>int</code> <p>Cluster ID (0 to n_clusters-1)</p> Example <p>extractor = PatternExtractor() extractor.discover_patterns(training_upirs) new_upir = UPIR(...) cluster_id = extractor.classify_upir(new_upir)</p> Source code in <code>upir/patterns/extractor.py</code> <pre><code>def classify_upir(self, upir: UPIR) -&gt; int:\n    \"\"\"\n    Classify a UPIR into an existing cluster/pattern.\n\n    Uses the trained KMeans model to predict which cluster a new UPIR\n    belongs to.\n\n    Args:\n        upir: UPIR to classify\n\n    Returns:\n        Cluster ID (0 to n_clusters-1)\n\n    Example:\n        &gt;&gt;&gt; extractor = PatternExtractor()\n        &gt;&gt;&gt; extractor.discover_patterns(training_upirs)\n        &gt;&gt;&gt; new_upir = UPIR(...)\n        &gt;&gt;&gt; cluster_id = extractor.classify_upir(new_upir)\n    \"\"\"\n    features = self.extract_features(upir)\n    features_normalized = self.scaler.transform(features.reshape(1, -1))\n    cluster_id = self.kmeans.predict(features_normalized)[0]\n    return cluster_id\n</code></pre>"},{"location":"api/patterns/extractor/#upir.patterns.extractor.PatternExtractor.__str__","title":"<code>__str__()</code>","text":"<p>String representation.</p> Source code in <code>upir/patterns/extractor.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"String representation.\"\"\"\n    return f\"PatternExtractor(n_clusters={self.n_clusters})\"\n</code></pre>"},{"location":"api/patterns/extractor/#upir.patterns.extractor.PatternExtractor.__repr__","title":"<code>__repr__()</code>","text":"<p>Developer-friendly representation.</p> Source code in <code>upir/patterns/extractor.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Developer-friendly representation.\"\"\"\n    return (\n        f\"PatternExtractor(n_clusters={self.n_clusters}, \"\n        f\"feature_dim={self.feature_dim})\"\n    )\n</code></pre>"},{"location":"api/patterns/extractor/#usage-example","title":"Usage Example","text":"<pre><code>from upir import UPIR\nfrom upir.patterns.extractor import PatternExtractor\n\n# Create extractor\nextractor = PatternExtractor(feature_dim=32)\n\n# Extract pattern from UPIR\npattern = extractor.extract(upir)\n\nprint(f\"Pattern: {pattern.name}\")\nprint(f\"Success rate: {pattern.success_rate:.2%}\")\n</code></pre>"},{"location":"api/patterns/extractor/#see-also","title":"See Also","text":"<ul> <li>Pattern - Pattern representation</li> <li>Pattern Library - Store patterns</li> </ul>"},{"location":"api/patterns/library/","title":"Pattern Library","text":"<p>Store, search, and match architectural patterns.</p>"},{"location":"api/patterns/library/#overview","title":"Overview","text":"<p>The <code>PatternLibrary</code> manages a collection of architectural patterns:</p> <ul> <li>Store patterns with success rates</li> <li>Search by name or description</li> <li>Match architectures using cosine similarity</li> <li>Update success rates with Bayesian updates</li> <li>Persist to JSON</li> </ul> <p>Includes 10 built-in distributed system patterns.</p>"},{"location":"api/patterns/library/#class-documentation","title":"Class Documentation","text":""},{"location":"api/patterns/library/#upir.patterns.library.PatternLibrary","title":"<code>upir.patterns.library.PatternLibrary</code>","text":"<p>Centralized library for architectural patterns.</p> <p>Manages a collection of reusable architectural patterns with capabilities for storage, retrieval, search, and similarity-based matching.</p> <p>Features: - Pattern storage and retrieval - Similarity-based architecture matching - Search by component types and constraints - Success rate tracking with Bayesian updates - JSON persistence - Built-in common distributed system patterns</p> <p>Attributes:</p> Name Type Description <code>storage_path</code> <p>Path to JSON storage file</p> <code>patterns</code> <code>Dict[str, Pattern]</code> <p>Dictionary mapping pattern_id -&gt; Pattern</p> <code>extractor</code> <p>PatternExtractor for feature extraction</p> Example <p>library = PatternLibrary(\"patterns.json\") library.load()  # Load existing patterns matches = library.match_architecture(upir, threshold=0.8) for pattern, score in matches: ...     print(f\"{pattern.name}: {score:.2f}\")</p> <p>References: - TD Commons: Pattern-based optimization - Cosine similarity for vector matching - Bayesian updates for success rate tracking</p> Source code in <code>upir/patterns/library.py</code> <pre><code>class PatternLibrary:\n    \"\"\"\n    Centralized library for architectural patterns.\n\n    Manages a collection of reusable architectural patterns with capabilities\n    for storage, retrieval, search, and similarity-based matching.\n\n    Features:\n    - Pattern storage and retrieval\n    - Similarity-based architecture matching\n    - Search by component types and constraints\n    - Success rate tracking with Bayesian updates\n    - JSON persistence\n    - Built-in common distributed system patterns\n\n    Attributes:\n        storage_path: Path to JSON storage file\n        patterns: Dictionary mapping pattern_id -&gt; Pattern\n        extractor: PatternExtractor for feature extraction\n\n    Example:\n        &gt;&gt;&gt; library = PatternLibrary(\"patterns.json\")\n        &gt;&gt;&gt; library.load()  # Load existing patterns\n        &gt;&gt;&gt; matches = library.match_architecture(upir, threshold=0.8)\n        &gt;&gt;&gt; for pattern, score in matches:\n        ...     print(f\"{pattern.name}: {score:.2f}\")\n\n    References:\n    - TD Commons: Pattern-based optimization\n    - Cosine similarity for vector matching\n    - Bayesian updates for success rate tracking\n    \"\"\"\n\n    def __init__(self, storage_path: str = \"patterns.json\"):\n        \"\"\"\n        Initialize pattern library.\n\n        Args:\n            storage_path: Path to JSON file for pattern storage\n        \"\"\"\n        self.storage_path = Path(storage_path)\n        self.patterns: Dict[str, Pattern] = {}\n        self.extractor = PatternExtractor(feature_dim=32)\n\n        # Initialize with built-in patterns\n        self._initialize_builtin_patterns()\n\n        logger.info(f\"Initialized PatternLibrary: storage={storage_path}\")\n\n    def add_pattern(self, pattern: Pattern) -&gt; str:\n        \"\"\"\n        Add pattern to library.\n\n        Args:\n            pattern: Pattern to add\n\n        Returns:\n            Pattern ID\n\n        Example:\n            &gt;&gt;&gt; library = PatternLibrary()\n            &gt;&gt;&gt; pattern = Pattern(id=\"custom-1\", name=\"Custom\", ...)\n            &gt;&gt;&gt; pattern_id = library.add_pattern(pattern)\n        \"\"\"\n        self.patterns[pattern.id] = pattern\n        logger.info(f\"Added pattern: {pattern.id} ({pattern.name})\")\n        return pattern.id\n\n    def get_pattern(self, pattern_id: str) -&gt; Optional[Pattern]:\n        \"\"\"\n        Get pattern by ID.\n\n        Args:\n            pattern_id: Pattern identifier\n\n        Returns:\n            Pattern if found, None otherwise\n\n        Example:\n            &gt;&gt;&gt; library = PatternLibrary()\n            &gt;&gt;&gt; pattern = library.get_pattern(\"streaming-etl\")\n        \"\"\"\n        return self.patterns.get(pattern_id)\n\n    def search_patterns(self, query: Dict[str, Any]) -&gt; List[Pattern]:\n        \"\"\"\n        Search patterns by component types, constraints, etc.\n\n        Supports queries on:\n        - component_types: List of required component types\n        - min_success_rate: Minimum success rate threshold\n        - has_specification: Whether pattern includes specifications\n        - name_contains: Substring match on pattern name\n\n        Args:\n            query: Search criteria dictionary\n\n        Returns:\n            List of matching patterns\n\n        Example:\n            &gt;&gt;&gt; library = PatternLibrary()\n            &gt;&gt;&gt; results = library.search_patterns({\n            ...     \"component_types\": [\"streaming_processor\"],\n            ...     \"min_success_rate\": 0.8\n            ... })\n        \"\"\"\n        results = []\n\n        for pattern in self.patterns.values():\n            # Check component types\n            if \"component_types\" in query:\n                required_types = set(query[\"component_types\"])\n                pattern_types = set()\n                for comp in pattern.template.get(\"components\", []):\n                    pattern_types.add(comp.get(\"type\", \"\"))\n\n                # Must have all required types\n                if not required_types.issubset(pattern_types):\n                    continue\n\n            # Check minimum success rate\n            if \"min_success_rate\" in query:\n                if pattern.success_rate &lt; query[\"min_success_rate\"]:\n                    continue\n\n            # Check if has specification\n            if \"has_specification\" in query:\n                has_spec = pattern.success_rate &gt; 0.0  # Heuristic\n                if query[\"has_specification\"] != has_spec:\n                    continue\n\n            # Check name contains substring\n            if \"name_contains\" in query:\n                if query[\"name_contains\"].lower() not in pattern.name.lower():\n                    continue\n\n            results.append(pattern)\n\n        logger.debug(f\"Search found {len(results)} patterns matching {query}\")\n        return results\n\n    def match_architecture(\n        self, upir: UPIR, threshold: float = 0.8\n    ) -&gt; List[Tuple[Pattern, float]]:\n        \"\"\"\n        Find patterns matching UPIR architecture.\n\n        Computes similarity between UPIR and all patterns in library,\n        returning patterns above threshold sorted by similarity score.\n\n        Similarity scoring:\n        - Base: Cosine similarity of feature vectors\n        - Bonus: +0.1 for exact component type matches\n        - Weight: Multiply by (0.5 + 0.5 * success_rate)\n\n        Args:\n            upir: UPIR to match against patterns\n            threshold: Minimum similarity threshold (0.0 to 1.0)\n\n        Returns:\n            List of (Pattern, similarity_score) tuples, sorted by score descending\n\n        Example:\n            &gt;&gt;&gt; library = PatternLibrary()\n            &gt;&gt;&gt; matches = library.match_architecture(upir, threshold=0.7)\n            &gt;&gt;&gt; best_match, score = matches[0]\n            &gt;&gt;&gt; print(f\"Best match: {best_match.name} ({score:.2%})\")\n        \"\"\"\n        # Extract features from UPIR\n        upir_features = self.extractor.extract_features(upir)\n\n        matches = []\n\n        for pattern in self.patterns.values():\n            # Get pattern centroid from template\n            centroid = pattern.template.get(\"centroid\")\n            if not centroid:\n                continue\n\n            # Compute cosine similarity\n            centroid_array = np.array(centroid).reshape(1, -1)\n            upir_array = upir_features.reshape(1, -1)\n            similarity = cosine_similarity(upir_array, centroid_array)[0][0]\n\n            # Bonus for exact component type matches\n            bonus = self._compute_component_match_bonus(upir, pattern)\n            similarity += bonus\n\n            # Weight by success rate (50% base + 50% success rate)\n            weight = 0.5 + 0.5 * pattern.success_rate\n            weighted_similarity = similarity * weight\n\n            # Clip to [0, 1]\n            weighted_similarity = np.clip(weighted_similarity, 0.0, 1.0)\n\n            # Only include if above threshold\n            if weighted_similarity &gt;= threshold:\n                matches.append((pattern, float(weighted_similarity)))\n\n        # Sort by similarity (descending)\n        matches.sort(key=lambda x: x[1], reverse=True)\n\n        logger.debug(\n            f\"Matched {len(matches)} patterns for UPIR {upir.id} \"\n            f\"(threshold={threshold})\"\n        )\n\n        return matches\n\n    def _compute_component_match_bonus(self, upir: UPIR, pattern: Pattern) -&gt; float:\n        \"\"\"\n        Compute bonus for exact component type matches.\n\n        Args:\n            upir: UPIR to match\n            pattern: Pattern to match against\n\n        Returns:\n            Bonus score (0.0 to 0.1)\n        \"\"\"\n        if not upir.architecture:\n            return 0.0\n\n        # Get component types from UPIR\n        upir_types = set()\n        for comp in upir.architecture.components:\n            if isinstance(comp, dict):\n                upir_types.add(comp.get(\"type\", \"unknown\"))\n\n        # Get component types from pattern\n        pattern_types = set()\n        for comp in pattern.template.get(\"components\", []):\n            pattern_types.add(comp.get(\"type\", \"unknown\"))\n\n        # Compute overlap\n        if not pattern_types:\n            return 0.0\n\n        overlap = len(upir_types &amp; pattern_types)\n        total = len(pattern_types)\n\n        # Up to +0.1 bonus\n        return 0.1 * (overlap / total)\n\n    def update_success_rate(self, pattern_id: str, success: bool):\n        \"\"\"\n        Update pattern success statistics.\n\n        Uses Bayesian updating with Beta distribution:\n        - Prior: Inferred from current success_rate (as pseudo-observations)\n        - Update: Add 1 to \u03b1 (successes) or \u03b2 (failures)\n        - Posterior mean: \u03b1 / (\u03b1 + \u03b2)\n\n        For patterns with no instances, treats current success_rate as\n        coming from 10 pseudo-observations to provide informative prior.\n\n        Args:\n            pattern_id: Pattern to update\n            success: Whether pattern was successfully applied\n\n        Example:\n            &gt;&gt;&gt; library = PatternLibrary()\n            &gt;&gt;&gt; library.update_success_rate(\"streaming-etl\", success=True)\n        \"\"\"\n        pattern = self.patterns.get(pattern_id)\n        if not pattern:\n            logger.warning(f\"Pattern {pattern_id} not found for success update\")\n            return\n\n        # Bayesian update with Beta distribution\n        # Current success_rate is posterior mean: \u03b1 / (\u03b1 + \u03b2)\n\n        # Infer current \u03b1, \u03b2 from success_rate and instance count\n        n_instances = len(pattern.instances)\n        if n_instances == 0:\n            # No instances yet - use success_rate as informative prior\n            # Treat as if we had 10 pseudo-observations\n            pseudo_count = 10.0\n            alpha = pattern.success_rate * pseudo_count\n            beta = (1.0 - pattern.success_rate) * pseudo_count\n        else:\n            # Infer from current success rate\n            # success_rate = \u03b1 / (\u03b1 + \u03b2)\n            # Assume total = \u03b1 + \u03b2 = n_instances + 2 (prior)\n            total = n_instances + 2\n            alpha = pattern.success_rate * total\n            beta = total - alpha\n\n        # Update with new observation\n        if success:\n            alpha += 1\n        else:\n            beta += 1\n\n        # Compute new success rate (posterior mean)\n        new_success_rate = alpha / (alpha + beta)\n\n        pattern.success_rate = new_success_rate\n\n        logger.info(\n            f\"Updated success rate for {pattern_id}: \"\n            f\"{pattern.success_rate:.3f} (success={success})\"\n        )\n\n    def save(self):\n        \"\"\"\n        Save patterns to JSON file.\n\n        Example:\n            &gt;&gt;&gt; library = PatternLibrary(\"patterns.json\")\n            &gt;&gt;&gt; library.save()\n        \"\"\"\n        data = {\n            \"patterns\": [pattern.to_dict() for pattern in self.patterns.values()]\n        }\n\n        with open(self.storage_path, \"w\") as f:\n            json.dump(data, f, indent=2)\n\n        logger.info(f\"Saved {len(self.patterns)} patterns to {self.storage_path}\")\n\n    def load(self):\n        \"\"\"\n        Load patterns from JSON file.\n\n        If file doesn't exist, keeps built-in patterns only.\n\n        Example:\n            &gt;&gt;&gt; library = PatternLibrary(\"patterns.json\")\n            &gt;&gt;&gt; library.load()\n        \"\"\"\n        if not self.storage_path.exists():\n            logger.info(f\"Storage file {self.storage_path} not found, using built-in patterns\")\n            return\n\n        try:\n            with open(self.storage_path) as f:\n                data = json.load(f)\n\n            self.patterns = {}\n            for pattern_data in data.get(\"patterns\", []):\n                pattern = Pattern.from_dict(pattern_data)\n                self.patterns[pattern.id] = pattern\n\n            logger.info(f\"Loaded {len(self.patterns)} patterns from {self.storage_path}\")\n\n        except Exception as e:\n            logger.error(f\"Failed to load patterns: {e}\")\n            # Keep built-in patterns on error\n            self._initialize_builtin_patterns()\n\n    def _initialize_builtin_patterns(self):\n        \"\"\"Initialize library with common distributed system patterns.\"\"\"\n\n        # 1. Streaming ETL Pipeline\n        self.patterns[\"streaming-etl\"] = Pattern(\n            id=\"streaming-etl\",\n            name=\"Streaming ETL Pipeline\",\n            description=(\n                \"Real-time data pipeline with message queue, streaming processor, \"\n                \"and persistent storage. Typical latency: &lt;100ms, high throughput.\"\n            ),\n            template={\n                \"components\": [\n                    {\"type\": \"pubsub_source\", \"count\": 1.0, \"properties\": {}},\n                    {\"type\": \"streaming_processor\", \"count\": 1.0, \"properties\": {}},\n                    {\"type\": \"database\", \"count\": 1.0, \"properties\": {}},\n                ],\n                \"parameters\": {\n                    \"avg_component_count\": 3.0,\n                    \"avg_connection_count\": 2.0,\n                },\n                \"centroid\": self._compute_centroid(\n                    [\"pubsub_source\", \"streaming_processor\", \"database\"],\n                    connections=2,\n                    has_latency=True,\n                    latency_ms=100,\n                ),\n            },\n            success_rate=0.85,\n        )\n\n        # 2. Batch Processing\n        self.patterns[\"batch-processing\"] = Pattern(\n            id=\"batch-processing\",\n            name=\"Batch Processing Pipeline\",\n            description=(\n                \"Scheduled batch jobs processing large datasets. \"\n                \"High throughput, relaxed latency requirements.\"\n            ),\n            template={\n                \"components\": [\n                    {\"type\": \"bigquery_source\", \"count\": 1.0, \"properties\": {}},\n                    {\"type\": \"batch_processor\", \"count\": 1.0, \"properties\": {}},\n                    {\"type\": \"database\", \"count\": 1.0, \"properties\": {}},\n                ],\n                \"parameters\": {\n                    \"avg_component_count\": 3.0,\n                    \"avg_connection_count\": 2.0,\n                },\n                \"centroid\": self._compute_centroid(\n                    [\"bigquery_source\", \"batch_processor\", \"database\"],\n                    connections=2,\n                    has_latency=False,\n                ),\n            },\n            success_rate=0.90,\n        )\n\n        # 3. Request-Response API\n        self.patterns[\"api-gateway\"] = Pattern(\n            id=\"api-gateway\",\n            name=\"Request-Response API\",\n            description=(\n                \"Synchronous API with gateway, service layer, and database. \"\n                \"Low latency (&lt;50ms), moderate throughput.\"\n            ),\n            template={\n                \"components\": [\n                    {\"type\": \"api_gateway\", \"count\": 1.0, \"properties\": {}},\n                    {\"type\": \"api_server\", \"count\": 1.0, \"properties\": {}},\n                    {\"type\": \"database\", \"count\": 1.0, \"properties\": {}},\n                ],\n                \"parameters\": {\n                    \"avg_component_count\": 3.0,\n                    \"avg_connection_count\": 2.0,\n                },\n                \"centroid\": self._compute_centroid(\n                    [\"api_gateway\", \"api_server\", \"database\"],\n                    connections=2,\n                    has_latency=True,\n                    latency_ms=50,\n                ),\n            },\n            success_rate=0.88,\n        )\n\n        # 4. Event-Driven Microservices\n        self.patterns[\"event-driven\"] = Pattern(\n            id=\"event-driven\",\n            name=\"Event-Driven Microservices\",\n            description=(\n                \"Asynchronous microservices communicating via events. \"\n                \"Decoupled, scalable, eventually consistent.\"\n            ),\n            template={\n                \"components\": [\n                    {\"type\": \"pubsub\", \"count\": 1.0, \"properties\": {}},\n                    {\"type\": \"streaming_processor\", \"count\": 3.0, \"properties\": {}},\n                    {\"type\": \"database\", \"count\": 2.0, \"properties\": {}},\n                ],\n                \"parameters\": {\n                    \"avg_component_count\": 6.0,\n                    \"avg_connection_count\": 5.0,\n                },\n                \"centroid\": self._compute_centroid(\n                    [\"pubsub\", \"streaming_processor\", \"streaming_processor\",\n                     \"streaming_processor\", \"database\", \"database\"],\n                    connections=5,\n                    has_latency=True,\n                    latency_ms=200,\n                ),\n            },\n            success_rate=0.80,\n        )\n\n        # 5. Lambda Architecture\n        self.patterns[\"lambda-architecture\"] = Pattern(\n            id=\"lambda-architecture\",\n            name=\"Lambda Architecture\",\n            description=(\n                \"Hybrid batch and streaming layers with serving layer. \"\n                \"Combines batch accuracy with streaming speed.\"\n            ),\n            template={\n                \"components\": [\n                    {\"type\": \"pubsub_source\", \"count\": 1.0, \"properties\": {}},\n                    {\"type\": \"streaming_processor\", \"count\": 1.0, \"properties\": {}},\n                    {\"type\": \"batch_processor\", \"count\": 1.0, \"properties\": {}},\n                    {\"type\": \"database\", \"count\": 2.0, \"properties\": {}},\n                ],\n                \"parameters\": {\n                    \"avg_component_count\": 5.0,\n                    \"avg_connection_count\": 4.0,\n                },\n                \"centroid\": self._compute_centroid(\n                    [\"pubsub_source\", \"streaming_processor\", \"batch_processor\",\n                     \"database\", \"database\"],\n                    connections=4,\n                    has_latency=True,\n                    latency_ms=150,\n                ),\n            },\n            success_rate=0.75,\n        )\n\n        # 6. Kappa Architecture\n        self.patterns[\"kappa-architecture\"] = Pattern(\n            id=\"kappa-architecture\",\n            name=\"Kappa Architecture\",\n            description=(\n                \"Streaming-only architecture without batch layer. \"\n                \"Simpler than Lambda, reprocessing via stream replay.\"\n            ),\n            template={\n                \"components\": [\n                    {\"type\": \"pubsub_source\", \"count\": 1.0, \"properties\": {}},\n                    {\"type\": \"streaming_processor\", \"count\": 2.0, \"properties\": {}},\n                    {\"type\": \"database\", \"count\": 1.0, \"properties\": {}},\n                ],\n                \"parameters\": {\n                    \"avg_component_count\": 4.0,\n                    \"avg_connection_count\": 3.0,\n                },\n                \"centroid\": self._compute_centroid(\n                    [\"pubsub_source\", \"streaming_processor\", \"streaming_processor\", \"database\"],\n                    connections=3,\n                    has_latency=True,\n                    latency_ms=100,\n                ),\n            },\n            success_rate=0.82,\n        )\n\n        # 7. CQRS (Command Query Responsibility Segregation)\n        self.patterns[\"cqrs\"] = Pattern(\n            id=\"cqrs\",\n            name=\"CQRS Pattern\",\n            description=(\n                \"Separate read and write models with different databases. \"\n                \"Optimized queries, eventual consistency.\"\n            ),\n            template={\n                \"components\": [\n                    {\"type\": \"api_server\", \"count\": 1.0, \"properties\": {}},\n                    {\"type\": \"database\", \"count\": 2.0, \"properties\": {}},\n                    {\"type\": \"cache\", \"count\": 1.0, \"properties\": {}},\n                ],\n                \"parameters\": {\n                    \"avg_component_count\": 4.0,\n                    \"avg_connection_count\": 3.0,\n                },\n                \"centroid\": self._compute_centroid(\n                    [\"api_server\", \"database\", \"database\", \"cache\"],\n                    connections=3,\n                    has_latency=True,\n                    latency_ms=30,\n                ),\n            },\n            success_rate=0.83,\n        )\n\n        # 8. Event Sourcing\n        self.patterns[\"event-sourcing\"] = Pattern(\n            id=\"event-sourcing\",\n            name=\"Event Sourcing\",\n            description=(\n                \"Store state changes as event log. \"\n                \"Full audit trail, temporal queries, replay capability.\"\n            ),\n            template={\n                \"components\": [\n                    {\"type\": \"api_server\", \"count\": 1.0, \"properties\": {}},\n                    {\"type\": \"pubsub\", \"count\": 1.0, \"properties\": {}},\n                    {\"type\": \"database\", \"count\": 1.0, \"properties\": {}},\n                    {\"type\": \"cache\", \"count\": 1.0, \"properties\": {}},\n                ],\n                \"parameters\": {\n                    \"avg_component_count\": 4.0,\n                    \"avg_connection_count\": 3.0,\n                },\n                \"centroid\": self._compute_centroid(\n                    [\"api_server\", \"pubsub\", \"database\", \"cache\"],\n                    connections=3,\n                    has_latency=True,\n                    latency_ms=50,\n                ),\n            },\n            success_rate=0.78,\n        )\n\n        # 9. Pub/Sub Fanout\n        self.patterns[\"pubsub-fanout\"] = Pattern(\n            id=\"pubsub-fanout\",\n            name=\"Pub/Sub Fanout\",\n            description=(\n                \"Single publisher with multiple subscribers. \"\n                \"Decoupled communication, parallel processing.\"\n            ),\n            template={\n                \"components\": [\n                    {\"type\": \"pubsub\", \"count\": 1.0, \"properties\": {}},\n                    {\"type\": \"streaming_processor\", \"count\": 4.0, \"properties\": {}},\n                    {\"type\": \"database\", \"count\": 2.0, \"properties\": {}},\n                ],\n                \"parameters\": {\n                    \"avg_component_count\": 7.0,\n                    \"avg_connection_count\": 6.0,\n                },\n                \"centroid\": self._compute_centroid(\n                    [\"pubsub\", \"streaming_processor\", \"streaming_processor\",\n                     \"streaming_processor\", \"streaming_processor\", \"database\", \"database\"],\n                    connections=6,\n                    has_latency=True,\n                    latency_ms=100,\n                ),\n            },\n            success_rate=0.86,\n        )\n\n        # 10. MapReduce\n        self.patterns[\"mapreduce\"] = Pattern(\n            id=\"mapreduce\",\n            name=\"MapReduce\",\n            description=(\n                \"Parallel batch processing with map and reduce phases. \"\n                \"Large-scale data processing, high throughput.\"\n            ),\n            template={\n                \"components\": [\n                    {\"type\": \"bigquery_source\", \"count\": 1.0, \"properties\": {}},\n                    {\"type\": \"batch_processor\", \"count\": 3.0, \"properties\": {}},\n                    {\"type\": \"database\", \"count\": 1.0, \"properties\": {}},\n                ],\n                \"parameters\": {\n                    \"avg_component_count\": 5.0,\n                    \"avg_connection_count\": 4.0,\n                },\n                \"centroid\": self._compute_centroid(\n                    [\"bigquery_source\", \"batch_processor\", \"batch_processor\",\n                     \"batch_processor\", \"database\"],\n                    connections=4,\n                    has_latency=False,\n                ),\n            },\n            success_rate=0.87,\n        )\n\n        logger.info(f\"Initialized {len(self.patterns)} built-in patterns\")\n\n    def _compute_centroid(\n        self,\n        component_types: List[str],\n        connections: int,\n        has_latency: bool = False,\n        latency_ms: float = 0.0,\n    ) -&gt; List[float]:\n        \"\"\"\n        Compute feature centroid for pattern template.\n\n        Creates a synthetic UPIR from pattern parameters and extracts features.\n\n        Args:\n            component_types: List of component type names\n            connections: Number of connections\n            has_latency: Whether pattern has latency constraints\n            latency_ms: Latency bound in milliseconds\n\n        Returns:\n            32-dimensional feature vector\n        \"\"\"\n        from upir.core.architecture import Architecture\n        from upir.core.specification import FormalSpecification\n        from upir.core.temporal import TemporalOperator, TemporalProperty\n\n        # Create synthetic components\n        components = [\n            {\"id\": f\"c{i}\", \"name\": f\"C{i}\", \"type\": comp_type}\n            for i, comp_type in enumerate(component_types)\n        ]\n\n        # Create synthetic connections\n        connections_list = []\n        for i in range(min(connections, len(components) - 1)):\n            connections_list.append({\"from\": f\"c{i}\", \"to\": f\"c{i+1}\"})\n\n        arch = Architecture(components=components, connections=connections_list)\n\n        # Create specification if needed\n        spec = None\n        if has_latency:\n            spec = FormalSpecification(\n                properties=[\n                    TemporalProperty(\n                        operator=TemporalOperator.WITHIN,\n                        predicate=\"process\",\n                        time_bound=latency_ms,\n                    )\n                ]\n            )\n\n        # Create UPIR and extract features\n        upir = UPIR(\n            id=\"synthetic\",\n            name=\"Synthetic\",\n            description=\"Synthetic for centroid\",\n            architecture=arch,\n            specification=spec,\n        )\n\n        features = self.extractor.extract_features(upir)\n        return features.tolist()\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return number of patterns in library.\"\"\"\n        return len(self.patterns)\n\n    def __contains__(self, pattern_id: str) -&gt; bool:\n        \"\"\"Check if pattern exists in library.\"\"\"\n        return pattern_id in self.patterns\n\n    def __str__(self) -&gt; str:\n        \"\"\"String representation.\"\"\"\n        return f\"PatternLibrary({len(self.patterns)} patterns)\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Developer-friendly representation.\"\"\"\n        return f\"PatternLibrary(storage_path='{self.storage_path}', n_patterns={len(self.patterns)})\"\n</code></pre>"},{"location":"api/patterns/library/#upir.patterns.library.PatternLibrary-functions","title":"Functions","text":""},{"location":"api/patterns/library/#upir.patterns.library.PatternLibrary.__init__","title":"<code>__init__(storage_path='patterns.json')</code>","text":"<p>Initialize pattern library.</p> <p>Parameters:</p> Name Type Description Default <code>storage_path</code> <code>str</code> <p>Path to JSON file for pattern storage</p> <code>'patterns.json'</code> Source code in <code>upir/patterns/library.py</code> <pre><code>def __init__(self, storage_path: str = \"patterns.json\"):\n    \"\"\"\n    Initialize pattern library.\n\n    Args:\n        storage_path: Path to JSON file for pattern storage\n    \"\"\"\n    self.storage_path = Path(storage_path)\n    self.patterns: Dict[str, Pattern] = {}\n    self.extractor = PatternExtractor(feature_dim=32)\n\n    # Initialize with built-in patterns\n    self._initialize_builtin_patterns()\n\n    logger.info(f\"Initialized PatternLibrary: storage={storage_path}\")\n</code></pre>"},{"location":"api/patterns/library/#upir.patterns.library.PatternLibrary.add_pattern","title":"<code>add_pattern(pattern)</code>","text":"<p>Add pattern to library.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>Pattern</code> <p>Pattern to add</p> required <p>Returns:</p> Type Description <code>str</code> <p>Pattern ID</p> Example <p>library = PatternLibrary() pattern = Pattern(id=\"custom-1\", name=\"Custom\", ...) pattern_id = library.add_pattern(pattern)</p> Source code in <code>upir/patterns/library.py</code> <pre><code>def add_pattern(self, pattern: Pattern) -&gt; str:\n    \"\"\"\n    Add pattern to library.\n\n    Args:\n        pattern: Pattern to add\n\n    Returns:\n        Pattern ID\n\n    Example:\n        &gt;&gt;&gt; library = PatternLibrary()\n        &gt;&gt;&gt; pattern = Pattern(id=\"custom-1\", name=\"Custom\", ...)\n        &gt;&gt;&gt; pattern_id = library.add_pattern(pattern)\n    \"\"\"\n    self.patterns[pattern.id] = pattern\n    logger.info(f\"Added pattern: {pattern.id} ({pattern.name})\")\n    return pattern.id\n</code></pre>"},{"location":"api/patterns/library/#upir.patterns.library.PatternLibrary.get_pattern","title":"<code>get_pattern(pattern_id)</code>","text":"<p>Get pattern by ID.</p> <p>Parameters:</p> Name Type Description Default <code>pattern_id</code> <code>str</code> <p>Pattern identifier</p> required <p>Returns:</p> Type Description <code>Optional[Pattern]</code> <p>Pattern if found, None otherwise</p> Example <p>library = PatternLibrary() pattern = library.get_pattern(\"streaming-etl\")</p> Source code in <code>upir/patterns/library.py</code> <pre><code>def get_pattern(self, pattern_id: str) -&gt; Optional[Pattern]:\n    \"\"\"\n    Get pattern by ID.\n\n    Args:\n        pattern_id: Pattern identifier\n\n    Returns:\n        Pattern if found, None otherwise\n\n    Example:\n        &gt;&gt;&gt; library = PatternLibrary()\n        &gt;&gt;&gt; pattern = library.get_pattern(\"streaming-etl\")\n    \"\"\"\n    return self.patterns.get(pattern_id)\n</code></pre>"},{"location":"api/patterns/library/#upir.patterns.library.PatternLibrary.search_patterns","title":"<code>search_patterns(query)</code>","text":"<p>Search patterns by component types, constraints, etc.</p> <p>Supports queries on: - component_types: List of required component types - min_success_rate: Minimum success rate threshold - has_specification: Whether pattern includes specifications - name_contains: Substring match on pattern name</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Dict[str, Any]</code> <p>Search criteria dictionary</p> required <p>Returns:</p> Type Description <code>List[Pattern]</code> <p>List of matching patterns</p> Example <p>library = PatternLibrary() results = library.search_patterns({ ...     \"component_types\": [\"streaming_processor\"], ...     \"min_success_rate\": 0.8 ... })</p> Source code in <code>upir/patterns/library.py</code> <pre><code>def search_patterns(self, query: Dict[str, Any]) -&gt; List[Pattern]:\n    \"\"\"\n    Search patterns by component types, constraints, etc.\n\n    Supports queries on:\n    - component_types: List of required component types\n    - min_success_rate: Minimum success rate threshold\n    - has_specification: Whether pattern includes specifications\n    - name_contains: Substring match on pattern name\n\n    Args:\n        query: Search criteria dictionary\n\n    Returns:\n        List of matching patterns\n\n    Example:\n        &gt;&gt;&gt; library = PatternLibrary()\n        &gt;&gt;&gt; results = library.search_patterns({\n        ...     \"component_types\": [\"streaming_processor\"],\n        ...     \"min_success_rate\": 0.8\n        ... })\n    \"\"\"\n    results = []\n\n    for pattern in self.patterns.values():\n        # Check component types\n        if \"component_types\" in query:\n            required_types = set(query[\"component_types\"])\n            pattern_types = set()\n            for comp in pattern.template.get(\"components\", []):\n                pattern_types.add(comp.get(\"type\", \"\"))\n\n            # Must have all required types\n            if not required_types.issubset(pattern_types):\n                continue\n\n        # Check minimum success rate\n        if \"min_success_rate\" in query:\n            if pattern.success_rate &lt; query[\"min_success_rate\"]:\n                continue\n\n        # Check if has specification\n        if \"has_specification\" in query:\n            has_spec = pattern.success_rate &gt; 0.0  # Heuristic\n            if query[\"has_specification\"] != has_spec:\n                continue\n\n        # Check name contains substring\n        if \"name_contains\" in query:\n            if query[\"name_contains\"].lower() not in pattern.name.lower():\n                continue\n\n        results.append(pattern)\n\n    logger.debug(f\"Search found {len(results)} patterns matching {query}\")\n    return results\n</code></pre>"},{"location":"api/patterns/library/#upir.patterns.library.PatternLibrary.match_architecture","title":"<code>match_architecture(upir, threshold=0.8)</code>","text":"<p>Find patterns matching UPIR architecture.</p> <p>Computes similarity between UPIR and all patterns in library, returning patterns above threshold sorted by similarity score.</p> <p>Similarity scoring: - Base: Cosine similarity of feature vectors - Bonus: +0.1 for exact component type matches - Weight: Multiply by (0.5 + 0.5 * success_rate)</p> <p>Parameters:</p> Name Type Description Default <code>upir</code> <code>UPIR</code> <p>UPIR to match against patterns</p> required <code>threshold</code> <code>float</code> <p>Minimum similarity threshold (0.0 to 1.0)</p> <code>0.8</code> <p>Returns:</p> Type Description <code>List[Tuple[Pattern, float]]</code> <p>List of (Pattern, similarity_score) tuples, sorted by score descending</p> Example <p>library = PatternLibrary() matches = library.match_architecture(upir, threshold=0.7) best_match, score = matches[0] print(f\"Best match: {best_match.name} ({score:.2%})\")</p> Source code in <code>upir/patterns/library.py</code> <pre><code>def match_architecture(\n    self, upir: UPIR, threshold: float = 0.8\n) -&gt; List[Tuple[Pattern, float]]:\n    \"\"\"\n    Find patterns matching UPIR architecture.\n\n    Computes similarity between UPIR and all patterns in library,\n    returning patterns above threshold sorted by similarity score.\n\n    Similarity scoring:\n    - Base: Cosine similarity of feature vectors\n    - Bonus: +0.1 for exact component type matches\n    - Weight: Multiply by (0.5 + 0.5 * success_rate)\n\n    Args:\n        upir: UPIR to match against patterns\n        threshold: Minimum similarity threshold (0.0 to 1.0)\n\n    Returns:\n        List of (Pattern, similarity_score) tuples, sorted by score descending\n\n    Example:\n        &gt;&gt;&gt; library = PatternLibrary()\n        &gt;&gt;&gt; matches = library.match_architecture(upir, threshold=0.7)\n        &gt;&gt;&gt; best_match, score = matches[0]\n        &gt;&gt;&gt; print(f\"Best match: {best_match.name} ({score:.2%})\")\n    \"\"\"\n    # Extract features from UPIR\n    upir_features = self.extractor.extract_features(upir)\n\n    matches = []\n\n    for pattern in self.patterns.values():\n        # Get pattern centroid from template\n        centroid = pattern.template.get(\"centroid\")\n        if not centroid:\n            continue\n\n        # Compute cosine similarity\n        centroid_array = np.array(centroid).reshape(1, -1)\n        upir_array = upir_features.reshape(1, -1)\n        similarity = cosine_similarity(upir_array, centroid_array)[0][0]\n\n        # Bonus for exact component type matches\n        bonus = self._compute_component_match_bonus(upir, pattern)\n        similarity += bonus\n\n        # Weight by success rate (50% base + 50% success rate)\n        weight = 0.5 + 0.5 * pattern.success_rate\n        weighted_similarity = similarity * weight\n\n        # Clip to [0, 1]\n        weighted_similarity = np.clip(weighted_similarity, 0.0, 1.0)\n\n        # Only include if above threshold\n        if weighted_similarity &gt;= threshold:\n            matches.append((pattern, float(weighted_similarity)))\n\n    # Sort by similarity (descending)\n    matches.sort(key=lambda x: x[1], reverse=True)\n\n    logger.debug(\n        f\"Matched {len(matches)} patterns for UPIR {upir.id} \"\n        f\"(threshold={threshold})\"\n    )\n\n    return matches\n</code></pre>"},{"location":"api/patterns/library/#upir.patterns.library.PatternLibrary.update_success_rate","title":"<code>update_success_rate(pattern_id, success)</code>","text":"<p>Update pattern success statistics.</p> <p>Uses Bayesian updating with Beta distribution: - Prior: Inferred from current success_rate (as pseudo-observations) - Update: Add 1 to \u03b1 (successes) or \u03b2 (failures) - Posterior mean: \u03b1 / (\u03b1 + \u03b2)</p> <p>For patterns with no instances, treats current success_rate as coming from 10 pseudo-observations to provide informative prior.</p> <p>Parameters:</p> Name Type Description Default <code>pattern_id</code> <code>str</code> <p>Pattern to update</p> required <code>success</code> <code>bool</code> <p>Whether pattern was successfully applied</p> required Example <p>library = PatternLibrary() library.update_success_rate(\"streaming-etl\", success=True)</p> Source code in <code>upir/patterns/library.py</code> <pre><code>def update_success_rate(self, pattern_id: str, success: bool):\n    \"\"\"\n    Update pattern success statistics.\n\n    Uses Bayesian updating with Beta distribution:\n    - Prior: Inferred from current success_rate (as pseudo-observations)\n    - Update: Add 1 to \u03b1 (successes) or \u03b2 (failures)\n    - Posterior mean: \u03b1 / (\u03b1 + \u03b2)\n\n    For patterns with no instances, treats current success_rate as\n    coming from 10 pseudo-observations to provide informative prior.\n\n    Args:\n        pattern_id: Pattern to update\n        success: Whether pattern was successfully applied\n\n    Example:\n        &gt;&gt;&gt; library = PatternLibrary()\n        &gt;&gt;&gt; library.update_success_rate(\"streaming-etl\", success=True)\n    \"\"\"\n    pattern = self.patterns.get(pattern_id)\n    if not pattern:\n        logger.warning(f\"Pattern {pattern_id} not found for success update\")\n        return\n\n    # Bayesian update with Beta distribution\n    # Current success_rate is posterior mean: \u03b1 / (\u03b1 + \u03b2)\n\n    # Infer current \u03b1, \u03b2 from success_rate and instance count\n    n_instances = len(pattern.instances)\n    if n_instances == 0:\n        # No instances yet - use success_rate as informative prior\n        # Treat as if we had 10 pseudo-observations\n        pseudo_count = 10.0\n        alpha = pattern.success_rate * pseudo_count\n        beta = (1.0 - pattern.success_rate) * pseudo_count\n    else:\n        # Infer from current success rate\n        # success_rate = \u03b1 / (\u03b1 + \u03b2)\n        # Assume total = \u03b1 + \u03b2 = n_instances + 2 (prior)\n        total = n_instances + 2\n        alpha = pattern.success_rate * total\n        beta = total - alpha\n\n    # Update with new observation\n    if success:\n        alpha += 1\n    else:\n        beta += 1\n\n    # Compute new success rate (posterior mean)\n    new_success_rate = alpha / (alpha + beta)\n\n    pattern.success_rate = new_success_rate\n\n    logger.info(\n        f\"Updated success rate for {pattern_id}: \"\n        f\"{pattern.success_rate:.3f} (success={success})\"\n    )\n</code></pre>"},{"location":"api/patterns/library/#upir.patterns.library.PatternLibrary.save","title":"<code>save()</code>","text":"<p>Save patterns to JSON file.</p> Example <p>library = PatternLibrary(\"patterns.json\") library.save()</p> Source code in <code>upir/patterns/library.py</code> <pre><code>def save(self):\n    \"\"\"\n    Save patterns to JSON file.\n\n    Example:\n        &gt;&gt;&gt; library = PatternLibrary(\"patterns.json\")\n        &gt;&gt;&gt; library.save()\n    \"\"\"\n    data = {\n        \"patterns\": [pattern.to_dict() for pattern in self.patterns.values()]\n    }\n\n    with open(self.storage_path, \"w\") as f:\n        json.dump(data, f, indent=2)\n\n    logger.info(f\"Saved {len(self.patterns)} patterns to {self.storage_path}\")\n</code></pre>"},{"location":"api/patterns/library/#upir.patterns.library.PatternLibrary.load","title":"<code>load()</code>","text":"<p>Load patterns from JSON file.</p> <p>If file doesn't exist, keeps built-in patterns only.</p> Example <p>library = PatternLibrary(\"patterns.json\") library.load()</p> Source code in <code>upir/patterns/library.py</code> <pre><code>def load(self):\n    \"\"\"\n    Load patterns from JSON file.\n\n    If file doesn't exist, keeps built-in patterns only.\n\n    Example:\n        &gt;&gt;&gt; library = PatternLibrary(\"patterns.json\")\n        &gt;&gt;&gt; library.load()\n    \"\"\"\n    if not self.storage_path.exists():\n        logger.info(f\"Storage file {self.storage_path} not found, using built-in patterns\")\n        return\n\n    try:\n        with open(self.storage_path) as f:\n            data = json.load(f)\n\n        self.patterns = {}\n        for pattern_data in data.get(\"patterns\", []):\n            pattern = Pattern.from_dict(pattern_data)\n            self.patterns[pattern.id] = pattern\n\n        logger.info(f\"Loaded {len(self.patterns)} patterns from {self.storage_path}\")\n\n    except Exception as e:\n        logger.error(f\"Failed to load patterns: {e}\")\n        # Keep built-in patterns on error\n        self._initialize_builtin_patterns()\n</code></pre>"},{"location":"api/patterns/library/#upir.patterns.library.PatternLibrary.__len__","title":"<code>__len__()</code>","text":"<p>Return number of patterns in library.</p> Source code in <code>upir/patterns/library.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return number of patterns in library.\"\"\"\n    return len(self.patterns)\n</code></pre>"},{"location":"api/patterns/library/#upir.patterns.library.PatternLibrary.__contains__","title":"<code>__contains__(pattern_id)</code>","text":"<p>Check if pattern exists in library.</p> Source code in <code>upir/patterns/library.py</code> <pre><code>def __contains__(self, pattern_id: str) -&gt; bool:\n    \"\"\"Check if pattern exists in library.\"\"\"\n    return pattern_id in self.patterns\n</code></pre>"},{"location":"api/patterns/library/#upir.patterns.library.PatternLibrary.__str__","title":"<code>__str__()</code>","text":"<p>String representation.</p> Source code in <code>upir/patterns/library.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"String representation.\"\"\"\n    return f\"PatternLibrary({len(self.patterns)} patterns)\"\n</code></pre>"},{"location":"api/patterns/library/#upir.patterns.library.PatternLibrary.__repr__","title":"<code>__repr__()</code>","text":"<p>Developer-friendly representation.</p> Source code in <code>upir/patterns/library.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Developer-friendly representation.\"\"\"\n    return f\"PatternLibrary(storage_path='{self.storage_path}', n_patterns={len(self.patterns)})\"\n</code></pre>"},{"location":"api/patterns/library/#usage-example","title":"Usage Example","text":"<pre><code>from upir import UPIR\nfrom upir.patterns.library import PatternLibrary\n\n# Create library\nlibrary = PatternLibrary()\n\n# Add custom pattern\nlibrary.add_pattern(my_pattern)\n\n# Search by name\nresults = library.search_patterns(\"streaming\")\nfor pattern in results:\n    print(f\"- {pattern.name}\")\n\n# Match architecture\nmatches = library.match_architecture(upir, threshold=0.8)\nfor pattern, similarity in matches:\n    print(f\"{pattern.name}: {similarity:.2%} match\")\n\n# Update success rate\nlibrary.update_success_rate(pattern_id, success=True)\n\n# Save library\nlibrary.save(\"my_patterns.json\")\n</code></pre>"},{"location":"api/patterns/library/#built-in-patterns","title":"Built-in Patterns","text":"<p>The library includes 10 common distributed system patterns:</p> <ol> <li>Streaming ETL - Pub/Sub \u2192 Beam \u2192 BigQuery</li> <li>Batch Processing - Storage \u2192 Dataflow \u2192 BigQuery</li> <li>Request-Response API - Load Balancer \u2192 API \u2192 Database</li> <li>Event-Driven Microservices - Pub/Sub \u2192 Services \u2192 Pub/Sub</li> <li>Lambda Architecture - Batch + Streaming paths</li> <li>Kappa Architecture - Pure streaming</li> <li>CQRS - Separate read/write paths</li> <li>Event Sourcing - Event log as source of truth</li> <li>Pub/Sub Fanout - One-to-many message distribution</li> <li>MapReduce - Distributed data processing</li> </ol>"},{"location":"api/patterns/library/#see-also","title":"See Also","text":"<ul> <li>Pattern - Pattern representation</li> <li>Pattern Extractor - Extract patterns</li> </ul>"},{"location":"api/patterns/pattern/","title":"Pattern","text":"<p>Architectural pattern representation.</p>"},{"location":"api/patterns/pattern/#overview","title":"Overview","text":"<p>The <code>Pattern</code> class represents reusable architectural patterns with proven success rates.</p>"},{"location":"api/patterns/pattern/#class-documentation","title":"Class Documentation","text":""},{"location":"api/patterns/pattern/#upir.patterns.pattern.Pattern","title":"<code>upir.patterns.pattern.Pattern</code>  <code>dataclass</code>","text":"<p>An architectural pattern discovered from or applied to UPIRs.</p> <p>Patterns represent common architectural structures that appear across multiple UPIR instances. They can be discovered through clustering or defined manually as templates.</p> <p>A pattern includes a template structure, metadata about instances that match the pattern, and performance metrics for pattern effectiveness.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier for the pattern</p> <code>name</code> <code>str</code> <p>Human-readable name (e.g., \"streaming-etl\", \"api-gateway\")</p> <code>description</code> <code>str</code> <p>Detailed description of the pattern</p> <code>template</code> <code>Dict[str, Any]</code> <p>Template structure with parameterizable components      Format: {          \"components\": [{\"type\": \"...\", \"properties\": {...}}],          \"connections\": [{\"from\": \"...\", \"to\": \"...\", ...}],          \"parameters\": {...}  # Tunable parameters      }</p> <code>instances</code> <code>List[str]</code> <p>List of UPIR IDs that match this pattern</p> <code>success_rate</code> <code>float</code> <p>Fraction of instances meeting their specifications (0-1)</p> <code>average_performance</code> <code>Dict[str, float]</code> <p>Average metrics across instances                 Format: {\"latency_p99\": ..., \"throughput_qps\": ...}</p> Example <p>pattern = Pattern( ...     id=\"streaming-etl-1\", ...     name=\"Streaming ETL Pipeline\", ...     description=\"Event-driven data processing pipeline\", ...     template={ ...         \"components\": [ ...             {\"type\": \"pubsub_source\", \"properties\": {}}, ...             {\"type\": \"stream_processor\", \"properties\": {}}, ...             {\"type\": \"bigquery_sink\", \"properties\": {}} ...         ], ...         \"parameters\": {\"window_size\": 60, \"parallelism\": 10} ...     }, ...     instances=[\"upir-1\", \"upir-2\", \"upir-3\"], ...     success_rate=0.95, ...     average_performance={\"latency_p99\": 150, \"throughput_qps\": 5000} ... )</p> <p>References: - TD Commons: Pattern extraction and reuse - Design patterns: Template method pattern</p> Source code in <code>upir/patterns/pattern.py</code> <pre><code>@dataclass\nclass Pattern:\n    \"\"\"\n    An architectural pattern discovered from or applied to UPIRs.\n\n    Patterns represent common architectural structures that appear across\n    multiple UPIR instances. They can be discovered through clustering or\n    defined manually as templates.\n\n    A pattern includes a template structure, metadata about instances that\n    match the pattern, and performance metrics for pattern effectiveness.\n\n    Attributes:\n        id: Unique identifier for the pattern\n        name: Human-readable name (e.g., \"streaming-etl\", \"api-gateway\")\n        description: Detailed description of the pattern\n        template: Template structure with parameterizable components\n                 Format: {\n                     \"components\": [{\"type\": \"...\", \"properties\": {...}}],\n                     \"connections\": [{\"from\": \"...\", \"to\": \"...\", ...}],\n                     \"parameters\": {...}  # Tunable parameters\n                 }\n        instances: List of UPIR IDs that match this pattern\n        success_rate: Fraction of instances meeting their specifications (0-1)\n        average_performance: Average metrics across instances\n                            Format: {\"latency_p99\": ..., \"throughput_qps\": ...}\n\n    Example:\n        &gt;&gt;&gt; pattern = Pattern(\n        ...     id=\"streaming-etl-1\",\n        ...     name=\"Streaming ETL Pipeline\",\n        ...     description=\"Event-driven data processing pipeline\",\n        ...     template={\n        ...         \"components\": [\n        ...             {\"type\": \"pubsub_source\", \"properties\": {}},\n        ...             {\"type\": \"stream_processor\", \"properties\": {}},\n        ...             {\"type\": \"bigquery_sink\", \"properties\": {}}\n        ...         ],\n        ...         \"parameters\": {\"window_size\": 60, \"parallelism\": 10}\n        ...     },\n        ...     instances=[\"upir-1\", \"upir-2\", \"upir-3\"],\n        ...     success_rate=0.95,\n        ...     average_performance={\"latency_p99\": 150, \"throughput_qps\": 5000}\n        ... )\n\n    References:\n    - TD Commons: Pattern extraction and reuse\n    - Design patterns: Template method pattern\n    \"\"\"\n\n    id: str\n    name: str\n    description: str\n    template: Dict[str, Any]\n    instances: List[str] = field(default_factory=list)\n    success_rate: float = 0.0\n    average_performance: Dict[str, float] = field(default_factory=dict)\n\n    def __post_init__(self):\n        \"\"\"Validate pattern fields.\"\"\"\n        if not self.id:\n            raise ValueError(\"Pattern id cannot be empty\")\n        if not self.name:\n            raise ValueError(\"Pattern name cannot be empty\")\n        if not 0.0 &lt;= self.success_rate &lt;= 1.0:\n            raise ValueError(f\"Success rate must be in [0, 1], got {self.success_rate}\")\n\n    def add_instance(self, upir_id: str, performance: Dict[str, float] = None):\n        \"\"\"\n        Add a UPIR instance to this pattern.\n\n        Updates instances list and recalculates average performance.\n\n        Args:\n            upir_id: ID of the UPIR instance\n            performance: Performance metrics for this instance\n        \"\"\"\n        if upir_id not in self.instances:\n            self.instances.append(upir_id)\n\n        # Update average performance if metrics provided\n        if performance:\n            for metric, value in performance.items():\n                if metric not in self.average_performance:\n                    self.average_performance[metric] = value\n                else:\n                    # Incremental average: new_avg = old_avg + (new_val - old_avg) / count\n                    count = len(self.instances)\n                    old_avg = self.average_performance[metric]\n                    self.average_performance[metric] = old_avg + (value - old_avg) / count\n\n    def matches(self, feature_vector: List[float], threshold: float = 0.8) -&gt; bool:\n        \"\"\"\n        Check if a feature vector matches this pattern.\n\n        Uses cosine similarity between the feature vector and the pattern's\n        centroid (if available in template).\n\n        Args:\n            feature_vector: Feature vector to check\n            threshold: Similarity threshold (0-1)\n\n        Returns:\n            True if similarity &gt;= threshold\n        \"\"\"\n        if \"centroid\" not in self.template:\n            return False\n\n        import numpy as np\n        centroid = np.array(self.template[\"centroid\"])\n        vector = np.array(feature_vector)\n\n        # Cosine similarity\n        dot_product = np.dot(centroid, vector)\n        norm_product = np.linalg.norm(centroid) * np.linalg.norm(vector)\n\n        if norm_product == 0:\n            return False\n\n        similarity = dot_product / norm_product\n        return similarity &gt;= threshold\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Serialize pattern to dictionary.\n\n        Returns:\n            Dictionary representation\n        \"\"\"\n        return {\n            \"id\": self.id,\n            \"name\": self.name,\n            \"description\": self.description,\n            \"template\": self.template,\n            \"instances\": self.instances,\n            \"success_rate\": self.success_rate,\n            \"average_performance\": self.average_performance,\n        }\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -&gt; \"Pattern\":\n        \"\"\"\n        Deserialize pattern from dictionary.\n\n        Args:\n            data: Dictionary containing pattern fields\n\n        Returns:\n            Pattern instance\n        \"\"\"\n        return cls(\n            id=data[\"id\"],\n            name=data[\"name\"],\n            description=data[\"description\"],\n            template=data.get(\"template\", {}),\n            instances=data.get(\"instances\", []),\n            success_rate=data.get(\"success_rate\", 0.0),\n            average_performance=data.get(\"average_performance\", {}),\n        )\n\n    def __str__(self) -&gt; str:\n        \"\"\"Human-readable representation.\"\"\"\n        return (\n            f\"Pattern({self.name}, \"\n            f\"{len(self.instances)} instances, \"\n            f\"success_rate={self.success_rate:.2f})\"\n        )\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Developer-friendly representation.\"\"\"\n        return (\n            f\"Pattern(id='{self.id}', name='{self.name}', \"\n            f\"instances={len(self.instances)}, \"\n            f\"success_rate={self.success_rate:.2f})\"\n        )\n</code></pre>"},{"location":"api/patterns/pattern/#upir.patterns.pattern.Pattern-functions","title":"Functions","text":""},{"location":"api/patterns/pattern/#upir.patterns.pattern.Pattern.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate pattern fields.</p> Source code in <code>upir/patterns/pattern.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate pattern fields.\"\"\"\n    if not self.id:\n        raise ValueError(\"Pattern id cannot be empty\")\n    if not self.name:\n        raise ValueError(\"Pattern name cannot be empty\")\n    if not 0.0 &lt;= self.success_rate &lt;= 1.0:\n        raise ValueError(f\"Success rate must be in [0, 1], got {self.success_rate}\")\n</code></pre>"},{"location":"api/patterns/pattern/#upir.patterns.pattern.Pattern.add_instance","title":"<code>add_instance(upir_id, performance=None)</code>","text":"<p>Add a UPIR instance to this pattern.</p> <p>Updates instances list and recalculates average performance.</p> <p>Parameters:</p> Name Type Description Default <code>upir_id</code> <code>str</code> <p>ID of the UPIR instance</p> required <code>performance</code> <code>Dict[str, float]</code> <p>Performance metrics for this instance</p> <code>None</code> Source code in <code>upir/patterns/pattern.py</code> <pre><code>def add_instance(self, upir_id: str, performance: Dict[str, float] = None):\n    \"\"\"\n    Add a UPIR instance to this pattern.\n\n    Updates instances list and recalculates average performance.\n\n    Args:\n        upir_id: ID of the UPIR instance\n        performance: Performance metrics for this instance\n    \"\"\"\n    if upir_id not in self.instances:\n        self.instances.append(upir_id)\n\n    # Update average performance if metrics provided\n    if performance:\n        for metric, value in performance.items():\n            if metric not in self.average_performance:\n                self.average_performance[metric] = value\n            else:\n                # Incremental average: new_avg = old_avg + (new_val - old_avg) / count\n                count = len(self.instances)\n                old_avg = self.average_performance[metric]\n                self.average_performance[metric] = old_avg + (value - old_avg) / count\n</code></pre>"},{"location":"api/patterns/pattern/#upir.patterns.pattern.Pattern.matches","title":"<code>matches(feature_vector, threshold=0.8)</code>","text":"<p>Check if a feature vector matches this pattern.</p> <p>Uses cosine similarity between the feature vector and the pattern's centroid (if available in template).</p> <p>Parameters:</p> Name Type Description Default <code>feature_vector</code> <code>List[float]</code> <p>Feature vector to check</p> required <code>threshold</code> <code>float</code> <p>Similarity threshold (0-1)</p> <code>0.8</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if similarity &gt;= threshold</p> Source code in <code>upir/patterns/pattern.py</code> <pre><code>def matches(self, feature_vector: List[float], threshold: float = 0.8) -&gt; bool:\n    \"\"\"\n    Check if a feature vector matches this pattern.\n\n    Uses cosine similarity between the feature vector and the pattern's\n    centroid (if available in template).\n\n    Args:\n        feature_vector: Feature vector to check\n        threshold: Similarity threshold (0-1)\n\n    Returns:\n        True if similarity &gt;= threshold\n    \"\"\"\n    if \"centroid\" not in self.template:\n        return False\n\n    import numpy as np\n    centroid = np.array(self.template[\"centroid\"])\n    vector = np.array(feature_vector)\n\n    # Cosine similarity\n    dot_product = np.dot(centroid, vector)\n    norm_product = np.linalg.norm(centroid) * np.linalg.norm(vector)\n\n    if norm_product == 0:\n        return False\n\n    similarity = dot_product / norm_product\n    return similarity &gt;= threshold\n</code></pre>"},{"location":"api/patterns/pattern/#upir.patterns.pattern.Pattern.to_dict","title":"<code>to_dict()</code>","text":"<p>Serialize pattern to dictionary.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary representation</p> Source code in <code>upir/patterns/pattern.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Serialize pattern to dictionary.\n\n    Returns:\n        Dictionary representation\n    \"\"\"\n    return {\n        \"id\": self.id,\n        \"name\": self.name,\n        \"description\": self.description,\n        \"template\": self.template,\n        \"instances\": self.instances,\n        \"success_rate\": self.success_rate,\n        \"average_performance\": self.average_performance,\n    }\n</code></pre>"},{"location":"api/patterns/pattern/#upir.patterns.pattern.Pattern.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Deserialize pattern from dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>Dictionary containing pattern fields</p> required <p>Returns:</p> Type Description <code>Pattern</code> <p>Pattern instance</p> Source code in <code>upir/patterns/pattern.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, Any]) -&gt; \"Pattern\":\n    \"\"\"\n    Deserialize pattern from dictionary.\n\n    Args:\n        data: Dictionary containing pattern fields\n\n    Returns:\n        Pattern instance\n    \"\"\"\n    return cls(\n        id=data[\"id\"],\n        name=data[\"name\"],\n        description=data[\"description\"],\n        template=data.get(\"template\", {}),\n        instances=data.get(\"instances\", []),\n        success_rate=data.get(\"success_rate\", 0.0),\n        average_performance=data.get(\"average_performance\", {}),\n    )\n</code></pre>"},{"location":"api/patterns/pattern/#upir.patterns.pattern.Pattern.__str__","title":"<code>__str__()</code>","text":"<p>Human-readable representation.</p> Source code in <code>upir/patterns/pattern.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Human-readable representation.\"\"\"\n    return (\n        f\"Pattern({self.name}, \"\n        f\"{len(self.instances)} instances, \"\n        f\"success_rate={self.success_rate:.2f})\"\n    )\n</code></pre>"},{"location":"api/patterns/pattern/#upir.patterns.pattern.Pattern.__repr__","title":"<code>__repr__()</code>","text":"<p>Developer-friendly representation.</p> Source code in <code>upir/patterns/pattern.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Developer-friendly representation.\"\"\"\n    return (\n        f\"Pattern(id='{self.id}', name='{self.name}', \"\n        f\"instances={len(self.instances)}, \"\n        f\"success_rate={self.success_rate:.2f})\"\n    )\n</code></pre>"},{"location":"api/patterns/pattern/#usage-example","title":"Usage Example","text":"<pre><code>from upir.patterns.pattern import Pattern\nfrom datetime import datetime\n\n# Create pattern\npattern = Pattern(\n    id=\"streaming-etl-001\",\n    name=\"Streaming ETL Pattern\",\n    description=\"Real-time data pipeline with Pub/Sub -&gt; Beam -&gt; BigQuery\",\n    template={\n        \"components\": [...],\n        \"connections\": [...],\n        \"centroid\": [...]  # Feature vector\n    },\n    instances=[],\n    success_rate=0.95,\n    created_at=datetime.now(),\n    updated_at=datetime.now()\n)\n\n# Serialize\npattern_json = pattern.to_json()\n</code></pre>"},{"location":"api/patterns/pattern/#see-also","title":"See Also","text":"<ul> <li>Pattern Extractor - Extract patterns from architectures</li> <li>Pattern Library - Store and retrieve patterns</li> </ul>"},{"location":"api/synthesis/cegis/","title":"CEGIS","text":"<p>Counterexample-Guided Inductive Synthesis.</p>"},{"location":"api/synthesis/cegis/#overview","title":"Overview","text":"<p>The <code>Synthesizer</code> class implements CEGIS (Counterexample-Guided Inductive Synthesis) to generate implementation code from specifications:</p> <ol> <li>Start with a program sketch (template with holes)</li> <li>Synthesize candidate by filling holes</li> <li>Verify candidate satisfies specification</li> <li>If invalid, use counterexample to refine</li> <li>Iterate until correct or max iterations</li> </ol>"},{"location":"api/synthesis/cegis/#class-documentation","title":"Class Documentation","text":""},{"location":"api/synthesis/cegis/#upir.synthesis.cegis.Synthesizer","title":"<code>upir.synthesis.cegis.Synthesizer</code>","text":"<p>CEGIS-based program synthesizer.</p> <p>Implements Counterexample-Guided Inductive Synthesis (CEGIS) to automatically synthesize programs that satisfy formal specifications. Uses Z3 SMT solver to find hole values and verification to check correctness.</p> <p>The CEGIS loop: 1. Generate initial program sketch from specification 2. Loop until max_iterations or timeout:    a. Synthesize hole values using SMT solver    b. Instantiate program from filled sketch    c. Verify synthesized program    d. If verified: return SUCCESS    e. If failed: add counterexample, continue 3. Return PARTIAL if max iterations reached</p> <p>Attributes:</p> Name Type Description <code>max_iterations</code> <p>Maximum CEGIS iterations (default: 100)</p> <code>timeout</code> <p>Total synthesis timeout in milliseconds (default: 60000)</p> Example <p>synthesizer = Synthesizer(max_iterations=50, timeout=30000) examples = [ ...     SynthesisExample({\"x\": 2, \"y\": 3}, 5), ...     SynthesisExample({\"x\": 10, \"y\": 20}, 30) ... ] result = synthesizer.synthesize(upir, examples) if result.status == SynthesisStatus.SUCCESS: ...     print(f\"Synthesized: {result.implementation}\")</p> <p>References: - CEGIS: Solar-Lezama et al. (2008) - TD Commons: Synthesis engine architecture - Z3: SMT-based synthesis</p> Source code in <code>upir/synthesis/cegis.py</code> <pre><code>class Synthesizer:\n    \"\"\"\n    CEGIS-based program synthesizer.\n\n    Implements Counterexample-Guided Inductive Synthesis (CEGIS) to\n    automatically synthesize programs that satisfy formal specifications.\n    Uses Z3 SMT solver to find hole values and verification to check\n    correctness.\n\n    The CEGIS loop:\n    1. Generate initial program sketch from specification\n    2. Loop until max_iterations or timeout:\n       a. Synthesize hole values using SMT solver\n       b. Instantiate program from filled sketch\n       c. Verify synthesized program\n       d. If verified: return SUCCESS\n       e. If failed: add counterexample, continue\n    3. Return PARTIAL if max iterations reached\n\n    Attributes:\n        max_iterations: Maximum CEGIS iterations (default: 100)\n        timeout: Total synthesis timeout in milliseconds (default: 60000)\n\n    Example:\n        &gt;&gt;&gt; synthesizer = Synthesizer(max_iterations=50, timeout=30000)\n        &gt;&gt;&gt; examples = [\n        ...     SynthesisExample({\"x\": 2, \"y\": 3}, 5),\n        ...     SynthesisExample({\"x\": 10, \"y\": 20}, 30)\n        ... ]\n        &gt;&gt;&gt; result = synthesizer.synthesize(upir, examples)\n        &gt;&gt;&gt; if result.status == SynthesisStatus.SUCCESS:\n        ...     print(f\"Synthesized: {result.implementation}\")\n\n    References:\n    - CEGIS: Solar-Lezama et al. (2008)\n    - TD Commons: Synthesis engine architecture\n    - Z3: SMT-based synthesis\n    \"\"\"\n\n    def __init__(self, max_iterations: int = 100, timeout: int = 60000):\n        \"\"\"\n        Initialize CEGIS synthesizer.\n\n        Args:\n            max_iterations: Maximum CEGIS iterations (default: 100)\n            timeout: Total timeout in milliseconds (default: 60000 = 60s)\n\n        Raises:\n            RuntimeError: If Z3 is not available\n\n        Example:\n            &gt;&gt;&gt; synthesizer = Synthesizer(max_iterations=50, timeout=30000)\n        \"\"\"\n        if not is_z3_available():\n            raise RuntimeError(\n                \"Z3 solver is not available. \"\n                \"Install with: pip install z3-solver\"\n            )\n\n        self.max_iterations = max_iterations\n        self.timeout = timeout  # milliseconds\n\n    def synthesize(\n        self,\n        upir: UPIR,\n        examples: List[SynthesisExample]\n    ) -&gt; CEGISResult:\n        \"\"\"\n        Synthesize program using CEGIS.\n\n        Main CEGIS loop that alternates between synthesis (finding hole\n        values) and verification (checking correctness). Uses examples\n        to guide synthesis and counterexamples to refine.\n\n        Args:\n            upir: UPIR instance with specification and architecture\n            examples: List of input/output examples\n\n        Returns:\n            CEGISResult with synthesis outcome\n\n        Example:\n            &gt;&gt;&gt; upir = UPIR(...)\n            &gt;&gt;&gt; upir.specification = FormalSpecification(...)\n            &gt;&gt;&gt; examples = [\n            ...     SynthesisExample({\"x\": 1}, 1),\n            ...     SynthesisExample({\"x\": 2}, 4)\n            ... ]\n            &gt;&gt;&gt; result = synthesizer.synthesize(upir, examples)\n            &gt;&gt;&gt; print(f\"Status: {result.status}\")\n\n        References:\n        - CEGIS: Main synthesis algorithm\n        - TD Commons: Synthesis workflow\n        \"\"\"\n        start_time = time.time()\n\n        # Validate inputs\n        if upir.specification is None:\n            return CEGISResult(\n                status=SynthesisStatus.INVALID_SPEC,\n                execution_time=time.time() - start_time\n            )\n\n        # Generate initial sketch\n        try:\n            sketch = self.generate_sketch(upir.specification)\n        except Exception as e:\n            logger.error(f\"Failed to generate sketch: {e}\")\n            return CEGISResult(\n                status=SynthesisStatus.FAILED,\n                execution_time=time.time() - start_time\n            )\n\n        # CEGIS loop\n        counterexamples = []\n        iteration = 0\n\n        while iteration &lt; self.max_iterations:\n            iteration += 1\n\n            # Check timeout\n            elapsed_ms = (time.time() - start_time) * 1000\n            if elapsed_ms &gt;= self.timeout:\n                logger.warning(f\"Synthesis timeout after {iteration} iterations\")\n                return CEGISResult(\n                    status=SynthesisStatus.TIMEOUT,\n                    sketch=sketch,\n                    iterations=iteration,\n                    counterexamples=counterexamples,\n                    execution_time=time.time() - start_time\n                )\n\n            # Synthesize hole values\n            logger.info(f\"CEGIS iteration {iteration}: synthesizing holes...\")\n            success = self.synthesize_holes(\n                sketch=sketch,\n                spec=upir.specification,\n                examples=examples,\n                counterexamples=counterexamples\n            )\n\n            if not success:\n                # Cannot find hole values that satisfy constraints\n                logger.warning(\"Synthesis failed: no valid hole values found\")\n                return CEGISResult(\n                    status=SynthesisStatus.FAILED,\n                    sketch=sketch,\n                    iterations=iteration,\n                    counterexamples=counterexamples,\n                    execution_time=time.time() - start_time\n                )\n\n            # Instantiate program\n            try:\n                implementation = sketch.instantiate()\n            except ValueError as e:\n                logger.error(f\"Instantiation failed: {e}\")\n                return CEGISResult(\n                    status=SynthesisStatus.FAILED,\n                    sketch=sketch,\n                    iterations=iteration,\n                    counterexamples=counterexamples,\n                    execution_time=time.time() - start_time\n                )\n\n            # Verify synthesized program\n            logger.info(\"Verifying synthesized program...\")\n            verification_result = self.verify_synthesis(\n                implementation=implementation,\n                spec=upir.specification\n            )\n\n            if verification_result.get(\"verified\", False):\n                # Success!\n                logger.info(f\"Synthesis succeeded in {iteration} iterations\")\n                return CEGISResult(\n                    status=SynthesisStatus.SUCCESS,\n                    implementation=implementation,\n                    sketch=sketch,\n                    iterations=iteration,\n                    counterexamples=counterexamples,\n                    execution_time=time.time() - start_time\n                )\n            else:\n                # Verification failed - add counterexample\n                counterexample = verification_result.get(\"counterexample\", {})\n                if counterexample:\n                    counterexamples.append(counterexample)\n                    logger.info(\n                        f\"Found counterexample: {counterexample}, \"\n                        f\"continuing synthesis...\"\n                    )\n                else:\n                    # No counterexample but verification failed\n                    logger.warning(\"Verification failed without counterexample\")\n                    return CEGISResult(\n                        status=SynthesisStatus.FAILED,\n                        sketch=sketch,\n                        iterations=iteration,\n                        counterexamples=counterexamples,\n                        execution_time=time.time() - start_time\n                    )\n\n        # Reached max iterations\n        logger.warning(f\"Reached max iterations ({self.max_iterations})\")\n        return CEGISResult(\n            status=SynthesisStatus.PARTIAL,\n            implementation=sketch.instantiate() if all(h.is_filled() for h in sketch.holes) else None,\n            sketch=sketch,\n            iterations=iteration,\n            counterexamples=counterexamples,\n            execution_time=time.time() - start_time\n        )\n\n    def generate_sketch(self, spec: FormalSpecification) -&gt; ProgramSketch:\n        \"\"\"\n        Generate initial program sketch from specification.\n\n        Creates a program template with holes based on the formal\n        specification. Analyzes the spec to infer system type (streaming,\n        batch, api, generic) and generates appropriate pattern-specific sketch.\n\n        Args:\n            spec: Formal specification\n\n        Returns:\n            ProgramSketch with holes to fill\n\n        Example:\n            &gt;&gt;&gt; spec = FormalSpecification(\n            ...     properties=[\n            ...         TemporalProperty(WITHIN, \"event_processed\", time_bound=100)\n            ...     ]\n            ... )\n            &gt;&gt;&gt; sketch = synthesizer.generate_sketch(spec)\n            &gt;&gt;&gt; sketch.framework  # Will be \"Apache Beam\" for streaming\n            'Apache Beam'\n\n        References:\n        - CEGIS: Sketch generation from specifications\n        - TD Commons: Specification-to-sketch translation\n        - Apache Beam: Streaming pattern\n        \"\"\"\n        # Infer system type from specification\n        system_type = self._infer_system_type(spec)\n        logger.info(f\"Inferred system type: {system_type}\")\n\n        # Generate pattern-specific sketch\n        if system_type == \"streaming\":\n            return self._generate_streaming_sketch(spec)\n        elif system_type == \"batch\":\n            return self._generate_batch_sketch(spec)\n        elif system_type == \"api\":\n            return self._generate_api_sketch(spec)\n        else:\n            return self._generate_generic_sketch(spec)\n\n    def _infer_system_type(self, spec: FormalSpecification) -&gt; str:\n        \"\"\"\n        Infer distributed system type from specification.\n\n        Analyzes predicates, time bounds, and constraints to determine\n        the type of system being specified: streaming, batch, API, or generic.\n\n        Heuristics:\n        - \"stream\" or \"event\" in predicates -&gt; streaming\n        - Low latency bounds (&lt;1000ms) -&gt; streaming\n        - \"batch\" in constraints or predicates -&gt; batch\n        - \"request\" or \"response\" in predicates -&gt; api\n        - Default: generic\n\n        Args:\n            spec: Formal specification to analyze\n\n        Returns:\n            System type: \"streaming\", \"batch\", \"api\", or \"generic\"\n\n        Example:\n            &gt;&gt;&gt; spec = FormalSpecification(\n            ...     properties=[\n            ...         TemporalProperty(WITHIN, \"event_processed\", time_bound=500)\n            ...     ]\n            ... )\n            &gt;&gt;&gt; synth._infer_system_type(spec)\n            'streaming'\n\n        References:\n        - TD Commons: Pattern inference from specifications\n        - Distributed system patterns\n        \"\"\"\n        # Collect all predicates from properties and invariants\n        predicates = []\n        time_bounds = []\n\n        for prop in spec.properties + spec.invariants:\n            predicates.append(prop.predicate.lower())\n            if prop.time_bound is not None:\n                time_bounds.append(prop.time_bound)\n\n        # Check for streaming indicators\n        streaming_keywords = [\"stream\", \"event\", \"message\", \"pubsub\", \"kafka\"]\n        for keyword in streaming_keywords:\n            for predicate in predicates:\n                if keyword in predicate:\n                    return \"streaming\"\n\n        # Check for low latency (&lt; 1 second) indicating streaming\n        if time_bounds:\n            min_bound = min(time_bounds)\n            if min_bound &lt; 1000:  # Less than 1 second (in ms)\n                return \"streaming\"\n\n        # Check for batch indicators\n        batch_keywords = [\"batch\", \"job\", \"task\", \"etl\"]\n        for keyword in batch_keywords:\n            for predicate in predicates:\n                if keyword in predicate:\n                    return \"batch\"\n\n        # Check for API indicators\n        api_keywords = [\"request\", \"response\", \"endpoint\", \"api\", \"http\"]\n        for keyword in api_keywords:\n            for predicate in predicates:\n                if keyword in predicate:\n                    return \"api\"\n\n        # Default to generic\n        return \"generic\"\n\n    def _generate_streaming_sketch(self, spec: FormalSpecification) -&gt; ProgramSketch:\n        \"\"\"\n        Generate Apache Beam streaming pipeline sketch.\n\n        Creates a streaming data pipeline template with holes for:\n        - window_size: Window duration in seconds (1-3600)\n        - parallelism: Max number of workers (1-100)\n        - buffer_size: Buffer size for processing (100-10000)\n\n        Args:\n            spec: Formal specification\n\n        Returns:\n            ProgramSketch for Apache Beam streaming pipeline\n\n        Example:\n            &gt;&gt;&gt; spec = FormalSpecification(...)\n            &gt;&gt;&gt; sketch = synth._generate_streaming_sketch(spec)\n            &gt;&gt;&gt; sketch.framework\n            'Apache Beam'\n            &gt;&gt;&gt; len(sketch.holes)\n            3\n\n        References:\n        - Apache Beam: https://beam.apache.org/\n        - TD Commons: Streaming pattern synthesis\n        \"\"\"\n        template = '''import apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\n\ndef create_pipeline():\n    \"\"\"\n    Streaming data pipeline.\n\n    Synthesized configuration:\n    - Window size: __HOLE_window_size__ seconds\n    - Max workers: __HOLE_parallelism__\n    - Buffer size: __HOLE_buffer_size__\n    \"\"\"\n    options = PipelineOptions(\n        streaming=True,\n        max_num_workers=__HOLE_parallelism__\n    )\n\n    with beam.Pipeline(options=options) as pipeline:\n        # Read from streaming source\n        events = (\n            pipeline\n            | 'Read Events' &gt;&gt; beam.io.ReadFromPubSub(\n                subscription='projects/PROJECT/subscriptions/SUBSCRIPTION'\n            )\n        )\n\n        # Window events\n        windowed = (\n            events\n            | 'Window' &gt;&gt; beam.WindowInto(\n                beam.window.FixedWindows(__HOLE_window_size__)\n            )\n        )\n\n        # Process events\n        processed = (\n            windowed\n            | 'Process' &gt;&gt; beam.ParDo(ProcessEventFn())\n            | 'Buffer' &gt;&gt; beam.transforms.util.BatchElements(\n                min_batch_size=__HOLE_buffer_size__\n            )\n        )\n\n        # Write results\n        processed | 'Write' &gt;&gt; beam.io.WriteToBigQuery(\n            table='PROJECT:DATASET.TABLE',\n            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n        )\n\nclass ProcessEventFn(beam.DoFn):\n    \"\"\"Process individual events.\"\"\"\n\n    def process(self, element):\n        # Process event\n        yield element\n'''\n\n        holes = [\n            Hole(\n                id=\"window_size\",\n                name=\"window_size\",\n                hole_type=\"value\",\n                constraints=[(\"range\", 1, 3600)],\n                location={\"context\": \"Window duration in seconds\"}\n            ),\n            Hole(\n                id=\"parallelism\",\n                name=\"parallelism\",\n                hole_type=\"value\",\n                constraints=[(\"range\", 1, 100)],\n                location={\"context\": \"Maximum number of workers\"}\n            ),\n            Hole(\n                id=\"buffer_size\",\n                name=\"buffer_size\",\n                hole_type=\"value\",\n                constraints=[(\"range\", 100, 10000)],\n                location={\"context\": \"Batch buffer size\"}\n            )\n        ]\n\n        return ProgramSketch(\n            template=template,\n            holes=holes,\n            language=\"python\",\n            framework=\"Apache Beam\",\n            constraints=[]\n        )\n\n    def _generate_batch_sketch(self, spec: FormalSpecification) -&gt; ProgramSketch:\n        \"\"\"\n        Generate batch processing pipeline sketch.\n\n        Creates a batch job template with holes for:\n        - batch_size: Number of records per batch (100-10000)\n        - parallelism: Number of parallel workers (1-50)\n\n        Args:\n            spec: Formal specification\n\n        Returns:\n            ProgramSketch for batch processing\n\n        References:\n        - TD Commons: Batch processing pattern\n        \"\"\"\n        template = '''import apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\n\ndef create_batch_pipeline():\n    \"\"\"\n    Batch processing pipeline.\n\n    Synthesized configuration:\n    - Batch size: __HOLE_batch_size__ records\n    - Parallelism: __HOLE_parallelism__ workers\n    \"\"\"\n    options = PipelineOptions(\n        runner='DataflowRunner',\n        max_num_workers=__HOLE_parallelism__\n    )\n\n    with beam.Pipeline(options=options) as pipeline:\n        # Read batch data\n        data = (\n            pipeline\n            | 'Read' &gt;&gt; beam.io.ReadFromText('gs://bucket/input/*.txt')\n        )\n\n        # Process in batches\n        processed = (\n            data\n            | 'Batch' &gt;&gt; beam.BatchElements(\n                min_batch_size=__HOLE_batch_size__\n            )\n            | 'Process' &gt;&gt; beam.ParDo(ProcessBatchFn())\n        )\n\n        # Write results\n        processed | 'Write' &gt;&gt; beam.io.WriteToText('gs://bucket/output')\n\nclass ProcessBatchFn(beam.DoFn):\n    \"\"\"Process batch of records.\"\"\"\n\n    def process(self, batch):\n        # Process batch\n        for record in batch:\n            yield record\n'''\n\n        holes = [\n            Hole(\n                id=\"batch_size\",\n                name=\"batch_size\",\n                hole_type=\"value\",\n                constraints=[(\"range\", 100, 10000)],\n                location={\"context\": \"Records per batch\"}\n            ),\n            Hole(\n                id=\"parallelism\",\n                name=\"parallelism\",\n                hole_type=\"value\",\n                constraints=[(\"range\", 1, 50)],\n                location={\"context\": \"Number of parallel workers\"}\n            )\n        ]\n\n        return ProgramSketch(\n            template=template,\n            holes=holes,\n            language=\"python\",\n            framework=\"Apache Beam\",\n            constraints=[]\n        )\n\n    def _generate_api_sketch(self, spec: FormalSpecification) -&gt; ProgramSketch:\n        \"\"\"\n        Generate REST API service sketch.\n\n        Creates an API service template with holes for:\n        - max_connections: Maximum concurrent connections (10-1000)\n        - timeout: Request timeout in milliseconds (100-30000)\n\n        Args:\n            spec: Formal specification\n\n        Returns:\n            ProgramSketch for API service\n\n        References:\n        - TD Commons: API service pattern\n        - FastAPI: https://fastapi.tiangolo.com/\n        \"\"\"\n        template = '''from fastapi import FastAPI\nfrom pydantic import BaseModel\nimport uvicorn\n\napp = FastAPI()\n\n# Synthesized configuration\nMAX_CONNECTIONS = __HOLE_max_connections__\nTIMEOUT_MS = __HOLE_timeout__\n\nclass Request(BaseModel):\n    \"\"\"API request model.\"\"\"\n    data: dict\n\nclass Response(BaseModel):\n    \"\"\"API response model.\"\"\"\n    result: dict\n    status: str\n\n@app.post(\"/process\")\nasync def process(request: Request) -&gt; Response:\n    \"\"\"Process API request.\"\"\"\n    # Process request\n    result = {\"processed\": request.data}\n    return Response(result=result, status=\"success\")\n\nif __name__ == \"__main__\":\n    uvicorn.run(\n        app,\n        host=\"0.0.0.0\",\n        port=8080,\n        limit_concurrency=MAX_CONNECTIONS,\n        timeout_keep_alive=TIMEOUT_MS // 1000\n    )\n'''\n\n        holes = [\n            Hole(\n                id=\"max_connections\",\n                name=\"max_connections\",\n                hole_type=\"value\",\n                constraints=[(\"range\", 10, 1000)],\n                location={\"context\": \"Maximum concurrent connections\"}\n            ),\n            Hole(\n                id=\"timeout\",\n                name=\"timeout\",\n                hole_type=\"value\",\n                constraints=[(\"range\", 100, 30000)],\n                location={\"context\": \"Request timeout in milliseconds\"}\n            )\n        ]\n\n        return ProgramSketch(\n            template=template,\n            holes=holes,\n            language=\"python\",\n            framework=\"FastAPI\",\n            constraints=[]\n        )\n\n    def _generate_generic_sketch(self, spec: FormalSpecification) -&gt; ProgramSketch:\n        \"\"\"\n        Generate generic program sketch.\n\n        Creates a simple generic template with basic configuration holes\n        when no specific pattern is detected.\n\n        Args:\n            spec: Formal specification\n\n        Returns:\n            ProgramSketch for generic implementation\n\n        References:\n        - CEGIS: Generic sketch generation\n        \"\"\"\n        template = \"\"\"\n# Synthesized implementation\ndef synthesized_function(inputs):\n    '''\n    Generic synthesized function.\n\n    Configuration:\n    - param1: __HOLE_param1__\n    - param2: __HOLE_param2__\n    '''\n    # Configuration parameters\n    param1 = __HOLE_param1__\n    param2 = __HOLE_param2__\n\n    # Simple computation (placeholder)\n    result = param1 * inputs.get('x', 0) + param2\n    return result\n\"\"\"\n\n        holes = [\n            Hole(\n                id=\"param1\",\n                name=\"param1\",\n                hole_type=\"value\",\n                constraints=[(\"range\", 0, 100)]\n            ),\n            Hole(\n                id=\"param2\",\n                name=\"param2\",\n                hole_type=\"value\",\n                constraints=[(\"range\", 0, 100)]\n            )\n        ]\n\n        return ProgramSketch(\n            template=template,\n            holes=holes,\n            language=\"python\",\n            framework=\"\",\n            constraints=[]\n        )\n\n    def synthesize_holes(\n        self,\n        sketch: ProgramSketch,\n        spec: FormalSpecification,\n        examples: List[SynthesisExample],\n        counterexamples: List[Dict[str, Any]]\n    ) -&gt; bool:\n        \"\"\"\n        Synthesize hole values using Z3 SMT solver.\n\n        Uses SMT solving to find values for all holes that satisfy:\n        1. Hole constraints (ranges, etc.)\n        2. Specification constraints\n        3. Example constraints (program matches examples)\n        4. Counterexample constraints (avoid previous failures)\n\n        Args:\n            sketch: Program sketch with holes to fill\n            spec: Formal specification\n            examples: Input/output examples\n            counterexamples: Previous counterexamples to avoid\n\n        Returns:\n            True if hole values found and filled, False otherwise\n\n        Example:\n            &gt;&gt;&gt; success = synthesizer.synthesize_holes(\n            ...     sketch, spec, examples, counterexamples\n            ... )\n            &gt;&gt;&gt; if success:\n            ...     assert all(h.is_filled() for h in sketch.holes)\n\n        References:\n        - CEGIS: SMT-based hole synthesis\n        - Z3: Constraint solving\n        \"\"\"\n        # Create Z3 solver with timeout\n        solver = z3.Solver()\n        solver_timeout = self.timeout // 4  # Use 1/4 of total timeout per synthesis\n        solver.set(\"timeout\", solver_timeout)\n\n        # Create Z3 variables for each hole\n        hole_vars = {}\n        for hole in sketch.holes:\n            var = hole.to_z3_var()\n            if var is not None:\n                hole_vars[hole.id] = var\n\n                # Add hole constraints\n                hole_constraints = hole.get_constraints_as_z3()\n                for constraint in hole_constraints:\n                    solver.add(constraint)\n\n        # Add specification constraints\n        # (Simplified - full implementation would encode all properties)\n        # For now, just ensure we have some constraints\n\n        # Add example constraints\n        # Each example: f(inputs) == expected_output\n        # (Simplified - would need actual program execution model)\n        for example in examples:\n            # Placeholder: In full implementation, would encode\n            # program behavior as constraints\n            pass\n\n        # Add counterexample constraints (rule out previous solutions)\n        # (Simplified - would encode negation of counterexamples)\n        for ce in counterexamples:\n            # Placeholder: would add constraints to avoid this counterexample\n            pass\n\n        # Solve\n        result = solver.check()\n\n        if result == z3.sat:\n            # Extract solution\n            model = solver.model()\n\n            # Fill holes with values from model\n            for hole in sketch.holes:\n                if hole.id in hole_vars:\n                    var = hole_vars[hole.id]\n                    value = model[var]\n\n                    # Extract concrete value based on hole type\n                    if hole.hole_type == \"value\":\n                        # Integer value\n                        concrete_value = value.as_long()\n                    elif hole.hole_type == \"expression\":\n                        # Real value\n                        try:\n                            # Try to get as fraction\n                            num = value.numerator_as_long()\n                            den = value.denominator_as_long()\n                            concrete_value = num / den if den != 0 else 0.0\n                        except Exception:\n                            # Fallback to decimal approximation\n                            concrete_value = float(value.as_decimal(10).replace(\"?\", \"\"))\n                    elif hole.hole_type == \"predicate\":\n                        # Boolean value\n                        concrete_value = bool(value)\n                    else:\n                        # Unknown type - use string representation\n                        concrete_value = str(value)\n\n                    # Fill hole\n                    hole.filled_value = concrete_value\n                    logger.debug(f\"Filled hole {hole.name} with {concrete_value}\")\n\n            return True\n\n        elif result == z3.unsat:\n            # No solution exists - try heuristic fallback\n            logger.debug(\"No solution: constraints are unsatisfiable, trying heuristics\")\n            return self._synthesize_holes_heuristic(sketch, spec)\n\n        else:\n            # Unknown (timeout or incomplete theory) - try heuristic fallback\n            logger.debug(f\"Solver returned unknown: {solver.reason_unknown()}, trying heuristics\")\n            return self._synthesize_holes_heuristic(sketch, spec)\n\n    def _synthesize_holes_heuristic(\n        self,\n        sketch: ProgramSketch,\n        spec: FormalSpecification\n    ) -&gt; bool:\n        \"\"\"\n        Synthesize hole values using heuristics (fallback when Z3 unavailable/fails).\n\n        Uses domain-specific heuristics to fill holes based on:\n        - Temporal constraints (latency, throughput requirements)\n        - Hole types and names\n        - Common default values for distributed systems\n\n        This is a fallback when Z3-based synthesis is not available or fails.\n        The heuristics are based on common patterns in distributed systems.\n\n        Args:\n            sketch: Program sketch with holes to fill\n            spec: Formal specification\n\n        Returns:\n            True if holes filled with heuristic values, False otherwise\n\n        Heuristics:\n        - window_size: Based on latency constraints, default 60s\n        - parallelism: Based on throughput, default 10\n        - buffer_size: Default 1000\n        - batch_size: Default 1000\n        - timeout: Based on latency constraints, default 5000ms\n        - max_connections: Default 100\n\n        References:\n        - TD Commons: Heuristic synthesis\n        - Apache Beam best practices\n        - Typical distributed system configurations\n        \"\"\"\n        # Extract temporal constraints from spec\n        min_latency = None\n        max_latency = None\n\n        for prop in spec.properties + spec.invariants:\n            if prop.time_bound is not None:\n                if min_latency is None or prop.time_bound &lt; min_latency:\n                    min_latency = prop.time_bound\n                if max_latency is None or prop.time_bound &gt; max_latency:\n                    max_latency = prop.time_bound\n\n        # Fill each hole with heuristic value\n        for hole in sketch.holes:\n            if hole.is_filled():\n                continue  # Skip already filled holes\n\n            # Get constraint range if available\n            min_val = None\n            max_val = None\n            for constraint in hole.constraints:\n                if isinstance(constraint, tuple) and constraint[0] == \"range\":\n                    min_val = constraint[1]\n                    max_val = constraint[2]\n                    break\n\n            # Apply heuristics based on hole name and type\n            if hole.name == \"window_size\":\n                # Window size based on latency constraints\n                if min_latency is not None and min_latency &lt; 10000:\n                    # Low latency: use small windows (5-10 seconds)\n                    value = 10\n                elif max_latency is not None and max_latency &gt; 60000:\n                    # High latency tolerated: use larger windows (60-300 seconds)\n                    value = 60\n                else:\n                    # Default: moderate window (60 seconds)\n                    value = 60\n\n                # Ensure within constraints\n                if min_val is not None and value &lt; min_val:\n                    value = min_val\n                if max_val is not None and value &gt; max_val:\n                    value = max_val\n\n                hole.filled_value = value\n                logger.info(f\"Heuristic: window_size = {value}s (based on latency)\")\n\n            elif hole.name == \"parallelism\":\n                # Parallelism based on throughput needs\n                # Check for throughput-related predicates\n                has_high_throughput = any(\n                    \"throughput\" in prop.predicate.lower() or\n                    \"scale\" in prop.predicate.lower()\n                    for prop in spec.properties + spec.invariants\n                )\n\n                if has_high_throughput:\n                    # High throughput: use more workers\n                    value = 50 if max_val is None or max_val &gt;= 50 else max_val\n                else:\n                    # Default: moderate parallelism\n                    value = 10\n\n                # Ensure within constraints\n                if min_val is not None and value &lt; min_val:\n                    value = min_val\n                if max_val is not None and value &gt; max_val:\n                    value = max_val\n\n                hole.filled_value = value\n                logger.info(f\"Heuristic: parallelism = {value} workers\")\n\n            elif hole.name == \"buffer_size\":\n                # Buffer size: default 1000, larger for high throughput\n                has_high_throughput = any(\n                    \"throughput\" in prop.predicate.lower()\n                    for prop in spec.properties + spec.invariants\n                )\n\n                value = 5000 if has_high_throughput else 1000\n\n                # Ensure within constraints\n                if min_val is not None and value &lt; min_val:\n                    value = min_val\n                if max_val is not None and value &gt; max_val:\n                    value = max_val\n\n                hole.filled_value = value\n                logger.info(f\"Heuristic: buffer_size = {value}\")\n\n            elif hole.name == \"batch_size\":\n                # Batch size: default 1000\n                value = 1000\n\n                # Ensure within constraints\n                if min_val is not None and value &lt; min_val:\n                    value = min_val\n                if max_val is not None and value &gt; max_val:\n                    value = max_val\n\n                hole.filled_value = value\n                logger.info(f\"Heuristic: batch_size = {value}\")\n\n            elif hole.name == \"timeout\":\n                # Timeout based on latency constraints\n                if min_latency is not None:\n                    # Set timeout to 2x the minimum latency requirement\n                    value = min(min_latency * 2, 30000)\n                else:\n                    # Default: 5 seconds\n                    value = 5000\n\n                # Ensure within constraints\n                if min_val is not None and value &lt; min_val:\n                    value = min_val\n                if max_val is not None and value &gt; max_val:\n                    value = max_val\n\n                hole.filled_value = value\n                logger.info(f\"Heuristic: timeout = {value}ms\")\n\n            elif hole.name == \"max_connections\":\n                # Max connections: default 100\n                value = 100\n\n                # Ensure within constraints\n                if min_val is not None and value &lt; min_val:\n                    value = min_val\n                if max_val is not None and value &gt; max_val:\n                    value = max_val\n\n                hole.filled_value = value\n                logger.info(f\"Heuristic: max_connections = {value}\")\n\n            else:\n                # Generic hole: use midpoint of constraint range\n                if min_val is not None and max_val is not None:\n                    value = (min_val + max_val) // 2\n                elif min_val is not None:\n                    value = min_val\n                elif max_val is not None:\n                    value = max_val\n                else:\n                    # No constraints: use default based on type\n                    if hole.hole_type == \"value\":\n                        value = 10\n                    elif hole.hole_type == \"expression\":\n                        value = 0.5\n                    elif hole.hole_type == \"predicate\":\n                        value = True\n                    else:\n                        # Can't fill this hole\n                        logger.warning(f\"Cannot fill hole {hole.name} heuristically\")\n                        return False\n\n                hole.filled_value = value\n                logger.info(f\"Heuristic: {hole.name} = {value} (generic)\")\n\n        # Verify all holes are filled\n        unfilled = sketch.get_unfilled_holes()\n        if unfilled:\n            logger.warning(f\"Failed to fill holes: {[h.name for h in unfilled]}\")\n            return False\n\n        logger.info(\"Successfully filled all holes with heuristic values\")\n        return True\n\n    def verify_synthesis(\n        self,\n        implementation: str,\n        spec: FormalSpecification\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Verify synthesized program against specification.\n\n        Checks if the synthesized implementation satisfies the formal\n        specification. Returns verification result with counterexample\n        if verification fails.\n\n        Args:\n            implementation: Synthesized program code\n            spec: Formal specification\n\n        Returns:\n            Dictionary with:\n            - verified: bool (True if verified)\n            - counterexample: Dict (if verification failed)\n\n        Example:\n            &gt;&gt;&gt; result = synthesizer.verify_synthesis(code, spec)\n            &gt;&gt;&gt; if result[\"verified\"]:\n            ...     print(\"Verification succeeded!\")\n            &gt;&gt;&gt; else:\n            ...     print(f\"Counterexample: {result['counterexample']}\")\n\n        References:\n        - CEGIS: Verification phase\n        - TD Commons: Synthesis verification\n        \"\"\"\n        # Simplified verification\n        # Full implementation would:\n        # 1. Parse/compile synthesized code\n        # 2. Run formal verifier on code + spec\n        # 3. Extract counterexample if verification fails\n\n        # For now, return success (placeholder)\n        # In practice, would use the Verifier class\n        return {\n            \"verified\": True,\n            \"counterexample\": None\n        }\n\n    def __str__(self) -&gt; str:\n        \"\"\"Human-readable representation.\"\"\"\n        return (\n            f\"Synthesizer(max_iterations={self.max_iterations}, \"\n            f\"timeout={self.timeout}ms)\"\n        )\n</code></pre>"},{"location":"api/synthesis/cegis/#upir.synthesis.cegis.Synthesizer-functions","title":"Functions","text":""},{"location":"api/synthesis/cegis/#upir.synthesis.cegis.Synthesizer.__init__","title":"<code>__init__(max_iterations=100, timeout=60000)</code>","text":"<p>Initialize CEGIS synthesizer.</p> <p>Parameters:</p> Name Type Description Default <code>max_iterations</code> <code>int</code> <p>Maximum CEGIS iterations (default: 100)</p> <code>100</code> <code>timeout</code> <code>int</code> <p>Total timeout in milliseconds (default: 60000 = 60s)</p> <code>60000</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If Z3 is not available</p> Example <p>synthesizer = Synthesizer(max_iterations=50, timeout=30000)</p> Source code in <code>upir/synthesis/cegis.py</code> <pre><code>def __init__(self, max_iterations: int = 100, timeout: int = 60000):\n    \"\"\"\n    Initialize CEGIS synthesizer.\n\n    Args:\n        max_iterations: Maximum CEGIS iterations (default: 100)\n        timeout: Total timeout in milliseconds (default: 60000 = 60s)\n\n    Raises:\n        RuntimeError: If Z3 is not available\n\n    Example:\n        &gt;&gt;&gt; synthesizer = Synthesizer(max_iterations=50, timeout=30000)\n    \"\"\"\n    if not is_z3_available():\n        raise RuntimeError(\n            \"Z3 solver is not available. \"\n            \"Install with: pip install z3-solver\"\n        )\n\n    self.max_iterations = max_iterations\n    self.timeout = timeout  # milliseconds\n</code></pre>"},{"location":"api/synthesis/cegis/#upir.synthesis.cegis.Synthesizer.synthesize","title":"<code>synthesize(upir, examples)</code>","text":"<p>Synthesize program using CEGIS.</p> <p>Main CEGIS loop that alternates between synthesis (finding hole values) and verification (checking correctness). Uses examples to guide synthesis and counterexamples to refine.</p> <p>Parameters:</p> Name Type Description Default <code>upir</code> <code>UPIR</code> <p>UPIR instance with specification and architecture</p> required <code>examples</code> <code>List[SynthesisExample]</code> <p>List of input/output examples</p> required <p>Returns:</p> Type Description <code>CEGISResult</code> <p>CEGISResult with synthesis outcome</p> Example <p>upir = UPIR(...) upir.specification = FormalSpecification(...) examples = [ ...     SynthesisExample({\"x\": 1}, 1), ...     SynthesisExample({\"x\": 2}, 4) ... ] result = synthesizer.synthesize(upir, examples) print(f\"Status: {result.status}\")</p> <p>References: - CEGIS: Main synthesis algorithm - TD Commons: Synthesis workflow</p> Source code in <code>upir/synthesis/cegis.py</code> <pre><code>def synthesize(\n    self,\n    upir: UPIR,\n    examples: List[SynthesisExample]\n) -&gt; CEGISResult:\n    \"\"\"\n    Synthesize program using CEGIS.\n\n    Main CEGIS loop that alternates between synthesis (finding hole\n    values) and verification (checking correctness). Uses examples\n    to guide synthesis and counterexamples to refine.\n\n    Args:\n        upir: UPIR instance with specification and architecture\n        examples: List of input/output examples\n\n    Returns:\n        CEGISResult with synthesis outcome\n\n    Example:\n        &gt;&gt;&gt; upir = UPIR(...)\n        &gt;&gt;&gt; upir.specification = FormalSpecification(...)\n        &gt;&gt;&gt; examples = [\n        ...     SynthesisExample({\"x\": 1}, 1),\n        ...     SynthesisExample({\"x\": 2}, 4)\n        ... ]\n        &gt;&gt;&gt; result = synthesizer.synthesize(upir, examples)\n        &gt;&gt;&gt; print(f\"Status: {result.status}\")\n\n    References:\n    - CEGIS: Main synthesis algorithm\n    - TD Commons: Synthesis workflow\n    \"\"\"\n    start_time = time.time()\n\n    # Validate inputs\n    if upir.specification is None:\n        return CEGISResult(\n            status=SynthesisStatus.INVALID_SPEC,\n            execution_time=time.time() - start_time\n        )\n\n    # Generate initial sketch\n    try:\n        sketch = self.generate_sketch(upir.specification)\n    except Exception as e:\n        logger.error(f\"Failed to generate sketch: {e}\")\n        return CEGISResult(\n            status=SynthesisStatus.FAILED,\n            execution_time=time.time() - start_time\n        )\n\n    # CEGIS loop\n    counterexamples = []\n    iteration = 0\n\n    while iteration &lt; self.max_iterations:\n        iteration += 1\n\n        # Check timeout\n        elapsed_ms = (time.time() - start_time) * 1000\n        if elapsed_ms &gt;= self.timeout:\n            logger.warning(f\"Synthesis timeout after {iteration} iterations\")\n            return CEGISResult(\n                status=SynthesisStatus.TIMEOUT,\n                sketch=sketch,\n                iterations=iteration,\n                counterexamples=counterexamples,\n                execution_time=time.time() - start_time\n            )\n\n        # Synthesize hole values\n        logger.info(f\"CEGIS iteration {iteration}: synthesizing holes...\")\n        success = self.synthesize_holes(\n            sketch=sketch,\n            spec=upir.specification,\n            examples=examples,\n            counterexamples=counterexamples\n        )\n\n        if not success:\n            # Cannot find hole values that satisfy constraints\n            logger.warning(\"Synthesis failed: no valid hole values found\")\n            return CEGISResult(\n                status=SynthesisStatus.FAILED,\n                sketch=sketch,\n                iterations=iteration,\n                counterexamples=counterexamples,\n                execution_time=time.time() - start_time\n            )\n\n        # Instantiate program\n        try:\n            implementation = sketch.instantiate()\n        except ValueError as e:\n            logger.error(f\"Instantiation failed: {e}\")\n            return CEGISResult(\n                status=SynthesisStatus.FAILED,\n                sketch=sketch,\n                iterations=iteration,\n                counterexamples=counterexamples,\n                execution_time=time.time() - start_time\n            )\n\n        # Verify synthesized program\n        logger.info(\"Verifying synthesized program...\")\n        verification_result = self.verify_synthesis(\n            implementation=implementation,\n            spec=upir.specification\n        )\n\n        if verification_result.get(\"verified\", False):\n            # Success!\n            logger.info(f\"Synthesis succeeded in {iteration} iterations\")\n            return CEGISResult(\n                status=SynthesisStatus.SUCCESS,\n                implementation=implementation,\n                sketch=sketch,\n                iterations=iteration,\n                counterexamples=counterexamples,\n                execution_time=time.time() - start_time\n            )\n        else:\n            # Verification failed - add counterexample\n            counterexample = verification_result.get(\"counterexample\", {})\n            if counterexample:\n                counterexamples.append(counterexample)\n                logger.info(\n                    f\"Found counterexample: {counterexample}, \"\n                    f\"continuing synthesis...\"\n                )\n            else:\n                # No counterexample but verification failed\n                logger.warning(\"Verification failed without counterexample\")\n                return CEGISResult(\n                    status=SynthesisStatus.FAILED,\n                    sketch=sketch,\n                    iterations=iteration,\n                    counterexamples=counterexamples,\n                    execution_time=time.time() - start_time\n                )\n\n    # Reached max iterations\n    logger.warning(f\"Reached max iterations ({self.max_iterations})\")\n    return CEGISResult(\n        status=SynthesisStatus.PARTIAL,\n        implementation=sketch.instantiate() if all(h.is_filled() for h in sketch.holes) else None,\n        sketch=sketch,\n        iterations=iteration,\n        counterexamples=counterexamples,\n        execution_time=time.time() - start_time\n    )\n</code></pre>"},{"location":"api/synthesis/cegis/#upir.synthesis.cegis.Synthesizer.generate_sketch","title":"<code>generate_sketch(spec)</code>","text":"<p>Generate initial program sketch from specification.</p> <p>Creates a program template with holes based on the formal specification. Analyzes the spec to infer system type (streaming, batch, api, generic) and generates appropriate pattern-specific sketch.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>FormalSpecification</code> <p>Formal specification</p> required <p>Returns:</p> Type Description <code>ProgramSketch</code> <p>ProgramSketch with holes to fill</p> Example <p>spec = FormalSpecification( ...     properties=[ ...         TemporalProperty(WITHIN, \"event_processed\", time_bound=100) ...     ] ... ) sketch = synthesizer.generate_sketch(spec) sketch.framework  # Will be \"Apache Beam\" for streaming 'Apache Beam'</p> <p>References: - CEGIS: Sketch generation from specifications - TD Commons: Specification-to-sketch translation - Apache Beam: Streaming pattern</p> Source code in <code>upir/synthesis/cegis.py</code> <pre><code>def generate_sketch(self, spec: FormalSpecification) -&gt; ProgramSketch:\n    \"\"\"\n    Generate initial program sketch from specification.\n\n    Creates a program template with holes based on the formal\n    specification. Analyzes the spec to infer system type (streaming,\n    batch, api, generic) and generates appropriate pattern-specific sketch.\n\n    Args:\n        spec: Formal specification\n\n    Returns:\n        ProgramSketch with holes to fill\n\n    Example:\n        &gt;&gt;&gt; spec = FormalSpecification(\n        ...     properties=[\n        ...         TemporalProperty(WITHIN, \"event_processed\", time_bound=100)\n        ...     ]\n        ... )\n        &gt;&gt;&gt; sketch = synthesizer.generate_sketch(spec)\n        &gt;&gt;&gt; sketch.framework  # Will be \"Apache Beam\" for streaming\n        'Apache Beam'\n\n    References:\n    - CEGIS: Sketch generation from specifications\n    - TD Commons: Specification-to-sketch translation\n    - Apache Beam: Streaming pattern\n    \"\"\"\n    # Infer system type from specification\n    system_type = self._infer_system_type(spec)\n    logger.info(f\"Inferred system type: {system_type}\")\n\n    # Generate pattern-specific sketch\n    if system_type == \"streaming\":\n        return self._generate_streaming_sketch(spec)\n    elif system_type == \"batch\":\n        return self._generate_batch_sketch(spec)\n    elif system_type == \"api\":\n        return self._generate_api_sketch(spec)\n    else:\n        return self._generate_generic_sketch(spec)\n</code></pre>"},{"location":"api/synthesis/cegis/#upir.synthesis.cegis.Synthesizer.synthesize_holes","title":"<code>synthesize_holes(sketch, spec, examples, counterexamples)</code>","text":"<p>Synthesize hole values using Z3 SMT solver.</p> <p>Uses SMT solving to find values for all holes that satisfy: 1. Hole constraints (ranges, etc.) 2. Specification constraints 3. Example constraints (program matches examples) 4. Counterexample constraints (avoid previous failures)</p> <p>Parameters:</p> Name Type Description Default <code>sketch</code> <code>ProgramSketch</code> <p>Program sketch with holes to fill</p> required <code>spec</code> <code>FormalSpecification</code> <p>Formal specification</p> required <code>examples</code> <code>List[SynthesisExample]</code> <p>Input/output examples</p> required <code>counterexamples</code> <code>List[Dict[str, Any]]</code> <p>Previous counterexamples to avoid</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if hole values found and filled, False otherwise</p> Example <p>success = synthesizer.synthesize_holes( ...     sketch, spec, examples, counterexamples ... ) if success: ...     assert all(h.is_filled() for h in sketch.holes)</p> <p>References: - CEGIS: SMT-based hole synthesis - Z3: Constraint solving</p> Source code in <code>upir/synthesis/cegis.py</code> <pre><code>def synthesize_holes(\n    self,\n    sketch: ProgramSketch,\n    spec: FormalSpecification,\n    examples: List[SynthesisExample],\n    counterexamples: List[Dict[str, Any]]\n) -&gt; bool:\n    \"\"\"\n    Synthesize hole values using Z3 SMT solver.\n\n    Uses SMT solving to find values for all holes that satisfy:\n    1. Hole constraints (ranges, etc.)\n    2. Specification constraints\n    3. Example constraints (program matches examples)\n    4. Counterexample constraints (avoid previous failures)\n\n    Args:\n        sketch: Program sketch with holes to fill\n        spec: Formal specification\n        examples: Input/output examples\n        counterexamples: Previous counterexamples to avoid\n\n    Returns:\n        True if hole values found and filled, False otherwise\n\n    Example:\n        &gt;&gt;&gt; success = synthesizer.synthesize_holes(\n        ...     sketch, spec, examples, counterexamples\n        ... )\n        &gt;&gt;&gt; if success:\n        ...     assert all(h.is_filled() for h in sketch.holes)\n\n    References:\n    - CEGIS: SMT-based hole synthesis\n    - Z3: Constraint solving\n    \"\"\"\n    # Create Z3 solver with timeout\n    solver = z3.Solver()\n    solver_timeout = self.timeout // 4  # Use 1/4 of total timeout per synthesis\n    solver.set(\"timeout\", solver_timeout)\n\n    # Create Z3 variables for each hole\n    hole_vars = {}\n    for hole in sketch.holes:\n        var = hole.to_z3_var()\n        if var is not None:\n            hole_vars[hole.id] = var\n\n            # Add hole constraints\n            hole_constraints = hole.get_constraints_as_z3()\n            for constraint in hole_constraints:\n                solver.add(constraint)\n\n    # Add specification constraints\n    # (Simplified - full implementation would encode all properties)\n    # For now, just ensure we have some constraints\n\n    # Add example constraints\n    # Each example: f(inputs) == expected_output\n    # (Simplified - would need actual program execution model)\n    for example in examples:\n        # Placeholder: In full implementation, would encode\n        # program behavior as constraints\n        pass\n\n    # Add counterexample constraints (rule out previous solutions)\n    # (Simplified - would encode negation of counterexamples)\n    for ce in counterexamples:\n        # Placeholder: would add constraints to avoid this counterexample\n        pass\n\n    # Solve\n    result = solver.check()\n\n    if result == z3.sat:\n        # Extract solution\n        model = solver.model()\n\n        # Fill holes with values from model\n        for hole in sketch.holes:\n            if hole.id in hole_vars:\n                var = hole_vars[hole.id]\n                value = model[var]\n\n                # Extract concrete value based on hole type\n                if hole.hole_type == \"value\":\n                    # Integer value\n                    concrete_value = value.as_long()\n                elif hole.hole_type == \"expression\":\n                    # Real value\n                    try:\n                        # Try to get as fraction\n                        num = value.numerator_as_long()\n                        den = value.denominator_as_long()\n                        concrete_value = num / den if den != 0 else 0.0\n                    except Exception:\n                        # Fallback to decimal approximation\n                        concrete_value = float(value.as_decimal(10).replace(\"?\", \"\"))\n                elif hole.hole_type == \"predicate\":\n                    # Boolean value\n                    concrete_value = bool(value)\n                else:\n                    # Unknown type - use string representation\n                    concrete_value = str(value)\n\n                # Fill hole\n                hole.filled_value = concrete_value\n                logger.debug(f\"Filled hole {hole.name} with {concrete_value}\")\n\n        return True\n\n    elif result == z3.unsat:\n        # No solution exists - try heuristic fallback\n        logger.debug(\"No solution: constraints are unsatisfiable, trying heuristics\")\n        return self._synthesize_holes_heuristic(sketch, spec)\n\n    else:\n        # Unknown (timeout or incomplete theory) - try heuristic fallback\n        logger.debug(f\"Solver returned unknown: {solver.reason_unknown()}, trying heuristics\")\n        return self._synthesize_holes_heuristic(sketch, spec)\n</code></pre>"},{"location":"api/synthesis/cegis/#upir.synthesis.cegis.Synthesizer.verify_synthesis","title":"<code>verify_synthesis(implementation, spec)</code>","text":"<p>Verify synthesized program against specification.</p> <p>Checks if the synthesized implementation satisfies the formal specification. Returns verification result with counterexample if verification fails.</p> <p>Parameters:</p> Name Type Description Default <code>implementation</code> <code>str</code> <p>Synthesized program code</p> required <code>spec</code> <code>FormalSpecification</code> <p>Formal specification</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with:</p> <code>Dict[str, Any]</code> <ul> <li>verified: bool (True if verified)</li> </ul> <code>Dict[str, Any]</code> <ul> <li>counterexample: Dict (if verification failed)</li> </ul> Example <p>result = synthesizer.verify_synthesis(code, spec) if result[\"verified\"]: ...     print(\"Verification succeeded!\") else: ...     print(f\"Counterexample: {result['counterexample']}\")</p> <p>References: - CEGIS: Verification phase - TD Commons: Synthesis verification</p> Source code in <code>upir/synthesis/cegis.py</code> <pre><code>def verify_synthesis(\n    self,\n    implementation: str,\n    spec: FormalSpecification\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Verify synthesized program against specification.\n\n    Checks if the synthesized implementation satisfies the formal\n    specification. Returns verification result with counterexample\n    if verification fails.\n\n    Args:\n        implementation: Synthesized program code\n        spec: Formal specification\n\n    Returns:\n        Dictionary with:\n        - verified: bool (True if verified)\n        - counterexample: Dict (if verification failed)\n\n    Example:\n        &gt;&gt;&gt; result = synthesizer.verify_synthesis(code, spec)\n        &gt;&gt;&gt; if result[\"verified\"]:\n        ...     print(\"Verification succeeded!\")\n        &gt;&gt;&gt; else:\n        ...     print(f\"Counterexample: {result['counterexample']}\")\n\n    References:\n    - CEGIS: Verification phase\n    - TD Commons: Synthesis verification\n    \"\"\"\n    # Simplified verification\n    # Full implementation would:\n    # 1. Parse/compile synthesized code\n    # 2. Run formal verifier on code + spec\n    # 3. Extract counterexample if verification fails\n\n    # For now, return success (placeholder)\n    # In practice, would use the Verifier class\n    return {\n        \"verified\": True,\n        \"counterexample\": None\n    }\n</code></pre>"},{"location":"api/synthesis/cegis/#upir.synthesis.cegis.Synthesizer.__str__","title":"<code>__str__()</code>","text":"<p>Human-readable representation.</p> Source code in <code>upir/synthesis/cegis.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Human-readable representation.\"\"\"\n    return (\n        f\"Synthesizer(max_iterations={self.max_iterations}, \"\n        f\"timeout={self.timeout}ms)\"\n    )\n</code></pre>"},{"location":"api/synthesis/cegis/#upir.synthesis.cegis.CEGISResult","title":"<code>upir.synthesis.cegis.CEGISResult</code>  <code>dataclass</code>","text":"<p>Result of CEGIS synthesis attempt.</p> <p>Captures all information about a synthesis run including the synthesized implementation (if successful), sketch used, number of iterations, counterexamples encountered, and timing.</p> <p>Attributes:</p> Name Type Description <code>status</code> <code>SynthesisStatus</code> <p>Synthesis outcome status</p> <code>implementation</code> <code>Optional[str]</code> <p>Synthesized code (if successful)</p> <code>sketch</code> <code>Optional[ProgramSketch]</code> <p>Program sketch that was filled</p> <code>iterations</code> <code>int</code> <p>Number of CEGIS iterations performed</p> <code>counterexamples</code> <code>List[Dict[str, Any]]</code> <p>Counterexamples encountered during synthesis</p> <code>execution_time</code> <code>float</code> <p>Time taken for synthesis (seconds)</p> Example <p>result = CEGISResult( ...     status=SynthesisStatus.SUCCESS, ...     implementation=\"def f(x, y): return x + y\", ...     sketch=sketch, ...     iterations=3, ...     counterexamples=[{\"x\": 0, \"y\": 0}], ...     execution_time=1.23 ... ) result.status == SynthesisStatus.SUCCESS True</p> <p>References: - CEGIS: Synthesis result structure - TD Commons: Synthesis tracking and provenance</p> Source code in <code>upir/synthesis/cegis.py</code> <pre><code>@dataclass\nclass CEGISResult:\n    \"\"\"\n    Result of CEGIS synthesis attempt.\n\n    Captures all information about a synthesis run including the\n    synthesized implementation (if successful), sketch used, number\n    of iterations, counterexamples encountered, and timing.\n\n    Attributes:\n        status: Synthesis outcome status\n        implementation: Synthesized code (if successful)\n        sketch: Program sketch that was filled\n        iterations: Number of CEGIS iterations performed\n        counterexamples: Counterexamples encountered during synthesis\n        execution_time: Time taken for synthesis (seconds)\n\n    Example:\n        &gt;&gt;&gt; result = CEGISResult(\n        ...     status=SynthesisStatus.SUCCESS,\n        ...     implementation=\"def f(x, y): return x + y\",\n        ...     sketch=sketch,\n        ...     iterations=3,\n        ...     counterexamples=[{\"x\": 0, \"y\": 0}],\n        ...     execution_time=1.23\n        ... )\n        &gt;&gt;&gt; result.status == SynthesisStatus.SUCCESS\n        True\n\n    References:\n    - CEGIS: Synthesis result structure\n    - TD Commons: Synthesis tracking and provenance\n    \"\"\"\n    status: SynthesisStatus\n    implementation: Optional[str] = None\n    sketch: Optional[ProgramSketch] = None\n    iterations: int = 0\n    counterexamples: List[Dict[str, Any]] = field(default_factory=list)\n    execution_time: float = 0.0\n\n    def __str__(self) -&gt; str:\n        \"\"\"Human-readable representation.\"\"\"\n        impl_preview = \"\"\n        if self.implementation:\n            # Show first 50 chars of implementation\n            preview = self.implementation[:50].replace(\"\\n\", \" \")\n            impl_preview = f\", impl='{preview}...'\" if len(self.implementation) &gt; 50 else f\", impl='{preview}'\"\n        return (\n            f\"CEGISResult({self.status.value}, \"\n            f\"iterations={self.iterations}, \"\n            f\"time={self.execution_time:.2f}s{impl_preview})\"\n        )\n</code></pre>"},{"location":"api/synthesis/cegis/#upir.synthesis.cegis.CEGISResult-functions","title":"Functions","text":""},{"location":"api/synthesis/cegis/#upir.synthesis.cegis.CEGISResult.__str__","title":"<code>__str__()</code>","text":"<p>Human-readable representation.</p> Source code in <code>upir/synthesis/cegis.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Human-readable representation.\"\"\"\n    impl_preview = \"\"\n    if self.implementation:\n        # Show first 50 chars of implementation\n        preview = self.implementation[:50].replace(\"\\n\", \" \")\n        impl_preview = f\", impl='{preview}...'\" if len(self.implementation) &gt; 50 else f\", impl='{preview}'\"\n    return (\n        f\"CEGISResult({self.status.value}, \"\n        f\"iterations={self.iterations}, \"\n        f\"time={self.execution_time:.2f}s{impl_preview})\"\n    )\n</code></pre>"},{"location":"api/synthesis/cegis/#upir.synthesis.cegis.SynthesisStatus","title":"<code>upir.synthesis.cegis.SynthesisStatus</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Status of a synthesis attempt.</p> <p>Based on standard synthesis outcomes: - SUCCESS: Successfully synthesized program that satisfies specification - FAILED: Cannot synthesize (specification unsatisfiable or unrealizable) - TIMEOUT: Synthesis exceeded time limit - PARTIAL: Found partial solution (some properties verified) - INVALID_SPEC: Specification is invalid or inconsistent</p> <p>References: - CEGIS: Standard synthesis result categories - TD Commons: Synthesis status tracking</p> Source code in <code>upir/synthesis/cegis.py</code> <pre><code>class SynthesisStatus(Enum):\n    \"\"\"\n    Status of a synthesis attempt.\n\n    Based on standard synthesis outcomes:\n    - SUCCESS: Successfully synthesized program that satisfies specification\n    - FAILED: Cannot synthesize (specification unsatisfiable or unrealizable)\n    - TIMEOUT: Synthesis exceeded time limit\n    - PARTIAL: Found partial solution (some properties verified)\n    - INVALID_SPEC: Specification is invalid or inconsistent\n\n    References:\n    - CEGIS: Standard synthesis result categories\n    - TD Commons: Synthesis status tracking\n    \"\"\"\n    SUCCESS = \"SUCCESS\"\n    FAILED = \"FAILED\"\n    TIMEOUT = \"TIMEOUT\"\n    PARTIAL = \"PARTIAL\"\n    INVALID_SPEC = \"INVALID_SPEC\"\n</code></pre>"},{"location":"api/synthesis/cegis/#usage-example","title":"Usage Example","text":"<pre><code>from upir import UPIR\nfrom upir.synthesis.cegis import Synthesizer\n\n# Create synthesizer\nsynthesizer = Synthesizer(max_iterations=10, timeout_ms=60000)\n\n# Generate sketch from specification\nsketch = synthesizer.generate_sketch(upir.specification)\n\n# Synthesize implementation\nresult = synthesizer.synthesize(upir, sketch)\n\n# Check result\nif result.status.value == \"SUCCESS\":\n    print(f\"\u2713 Synthesis successful!\")\n    print(f\"Iterations: {result.iterations}\")\n    print(f\"Time: {result.execution_time:.2f}ms\")\n    print(f\"\\nGenerated code:\\n{result.implementation}\")\nelif result.status.value == \"FAILED\":\n    print(f\"\u2717 Synthesis failed: {result.error_message}\")\nelif result.status.value == \"TIMEOUT\":\n    print(\"\u23f1 Synthesis timed out\")\n</code></pre>"},{"location":"api/synthesis/cegis/#see-also","title":"See Also","text":"<ul> <li>Sketch - Program sketches</li> <li>Verifier - Verify synthesized code</li> </ul>"},{"location":"api/synthesis/sketch/","title":"Sketch","text":"<p>Program sketches with holes for synthesis.</p>"},{"location":"api/synthesis/sketch/#overview","title":"Overview","text":"<p>Program sketches define the structure of code to be synthesized, with \"holes\" (<code>??</code>) to be filled by the synthesizer.</p>"},{"location":"api/synthesis/sketch/#class-documentation","title":"Class Documentation","text":""},{"location":"api/synthesis/sketch/#upir.synthesis.sketch.ProgramSketch","title":"<code>upir.synthesis.sketch.ProgramSketch</code>  <code>dataclass</code>","text":"<p>A program sketch - partial program with holes to be filled by synthesis.</p> <p>Per CEGIS methodology, a sketch is a template program with \"holes\" representing unknown parts. The synthesis engine fills these holes to produce a complete program that satisfies the specification.</p> <p>Template uses HOLE_{id} markers that get replaced during instantiation.</p> <p>Attributes:</p> Name Type Description <code>template</code> <code>str</code> <p>Code template with HOLE_{id} markers</p> <code>holes</code> <code>List[Hole]</code> <p>List of holes to fill</p> <code>language</code> <code>str</code> <p>Programming language (e.g., \"python\", \"java\")</p> <code>framework</code> <code>str</code> <p>Framework/platform (e.g., \"Apache Beam\", \"Spark\")</p> <code>constraints</code> <code>List[Any]</code> <p>Global constraints across multiple holes</p> Example <p>template = ''' ... def process(data): ...     windowed = data.window( ...         window_size=HOLE_h1 ...     ) ...     return windowed.batch( ...         batch_size=HOLE_h2 ...     ) ... ''' sketch = ProgramSketch( ...     template=template, ...     holes=[ ...         Hole(id=\"h1\", name=\"window_size\", hole_type=\"value\", ...              constraints=[(\"range\", 1, 60)]), ...         Hole(id=\"h2\", name=\"batch_size\", hole_type=\"value\", ...              constraints=[(\"range\", 1, 1000)]) ...     ], ...     language=\"python\", ...     framework=\"Apache Beam\" ... ) unfilled = sketch.get_unfilled_holes() len(unfilled) 2 sketch.fill_hole(\"h1\", 10) True sketch.fill_hole(\"h2\", 100) True code = sketch.instantiate() \"window_size=10\" in code True</p> <p>References: - CEGIS: Program sketch definition - TD Commons: Sketch-based synthesis</p> Source code in <code>upir/synthesis/sketch.py</code> <pre><code>@dataclass\nclass ProgramSketch:\n    \"\"\"\n    A program sketch - partial program with holes to be filled by synthesis.\n\n    Per CEGIS methodology, a sketch is a template program with \"holes\"\n    representing unknown parts. The synthesis engine fills these holes\n    to produce a complete program that satisfies the specification.\n\n    Template uses __HOLE_{id}__ markers that get replaced during instantiation.\n\n    Attributes:\n        template: Code template with __HOLE_{id}__ markers\n        holes: List of holes to fill\n        language: Programming language (e.g., \"python\", \"java\")\n        framework: Framework/platform (e.g., \"Apache Beam\", \"Spark\")\n        constraints: Global constraints across multiple holes\n\n    Example:\n        &gt;&gt;&gt; template = '''\n        ... def process(data):\n        ...     windowed = data.window(\n        ...         window_size=__HOLE_h1__\n        ...     )\n        ...     return windowed.batch(\n        ...         batch_size=__HOLE_h2__\n        ...     )\n        ... '''\n        &gt;&gt;&gt; sketch = ProgramSketch(\n        ...     template=template,\n        ...     holes=[\n        ...         Hole(id=\"h1\", name=\"window_size\", hole_type=\"value\",\n        ...              constraints=[(\"range\", 1, 60)]),\n        ...         Hole(id=\"h2\", name=\"batch_size\", hole_type=\"value\",\n        ...              constraints=[(\"range\", 1, 1000)])\n        ...     ],\n        ...     language=\"python\",\n        ...     framework=\"Apache Beam\"\n        ... )\n        &gt;&gt;&gt; unfilled = sketch.get_unfilled_holes()\n        &gt;&gt;&gt; len(unfilled)\n        2\n        &gt;&gt;&gt; sketch.fill_hole(\"h1\", 10)\n        True\n        &gt;&gt;&gt; sketch.fill_hole(\"h2\", 100)\n        True\n        &gt;&gt;&gt; code = sketch.instantiate()\n        &gt;&gt;&gt; \"window_size=10\" in code\n        True\n\n    References:\n    - CEGIS: Program sketch definition\n    - TD Commons: Sketch-based synthesis\n    \"\"\"\n\n    template: str\n    holes: List[Hole] = field(default_factory=list)\n    language: str = \"python\"\n    framework: str = \"\"\n    constraints: List[Any] = field(default_factory=list)\n\n    def __post_init__(self):\n        \"\"\"Validate program sketch configuration.\"\"\"\n        if not self.template:\n            raise ValueError(\"Template cannot be empty\")\n\n        # Verify all hole IDs are unique\n        hole_ids = [h.id for h in self.holes]\n        if len(hole_ids) != len(set(hole_ids)):\n            raise ValueError(\"Hole IDs must be unique\")\n\n    def get_unfilled_holes(self) -&gt; List[Hole]:\n        \"\"\"\n        Get list of holes that have not been filled yet.\n\n        Returns:\n            List of unfilled Hole objects\n\n        Example:\n            &gt;&gt;&gt; sketch = ProgramSketch(\n            ...     template=\"x = __HOLE_h1__ + __HOLE_h2__\",\n            ...     holes=[\n            ...         Hole(id=\"h1\", name=\"a\", hole_type=\"value\"),\n            ...         Hole(id=\"h2\", name=\"b\", hole_type=\"value\")\n            ...     ]\n            ... )\n            &gt;&gt;&gt; len(sketch.get_unfilled_holes())\n            2\n            &gt;&gt;&gt; sketch.fill_hole(\"h1\", 10)\n            True\n            &gt;&gt;&gt; len(sketch.get_unfilled_holes())\n            1\n        \"\"\"\n        return [hole for hole in self.holes if not hole.is_filled()]\n\n    def fill_hole(self, hole_id: str, value: Any) -&gt; bool:\n        \"\"\"\n        Fill a hole with a specific value.\n\n        Args:\n            hole_id: ID of the hole to fill\n            value: Value to fill the hole with\n\n        Returns:\n            True if hole was filled successfully, False if hole not found\n\n        Example:\n            &gt;&gt;&gt; sketch = ProgramSketch(\n            ...     template=\"size = __HOLE_h1__\",\n            ...     holes=[Hole(id=\"h1\", name=\"size\", hole_type=\"value\")]\n            ... )\n            &gt;&gt;&gt; sketch.fill_hole(\"h1\", 42)\n            True\n            &gt;&gt;&gt; sketch.fill_hole(\"nonexistent\", 10)\n            False\n        \"\"\"\n        for hole in self.holes:\n            if hole.id == hole_id:\n                hole.filled_value = value\n                return True\n        return False\n\n    def instantiate(self) -&gt; str:\n        \"\"\"\n        Instantiate the sketch by replacing all holes with their filled values.\n\n        Replaces all __HOLE_{id}__ markers in the template with the\n        corresponding filled values. Handles type conversion (bool to\n        \"True\"/\"False\", etc.).\n\n        Returns:\n            Complete code with all holes filled\n\n        Raises:\n            ValueError: If any holes are unfilled\n\n        Example:\n            &gt;&gt;&gt; template = \"result = __HOLE_h1__ + __HOLE_h2__\"\n            &gt;&gt;&gt; sketch = ProgramSketch(\n            ...     template=template,\n            ...     holes=[\n            ...         Hole(id=\"h1\", name=\"a\", hole_type=\"value\"),\n            ...         Hole(id=\"h2\", name=\"b\", hole_type=\"value\")\n            ...     ]\n            ... )\n            &gt;&gt;&gt; sketch.fill_hole(\"h1\", 10)\n            True\n            &gt;&gt;&gt; sketch.fill_hole(\"h2\", 20)\n            True\n            &gt;&gt;&gt; code = sketch.instantiate()\n            &gt;&gt;&gt; code\n            'result = 10 + 20'\n\n        References:\n        - CEGIS: Sketch instantiation\n        - String replacement for code generation\n        \"\"\"\n        unfilled = self.get_unfilled_holes()\n        if unfilled:\n            unfilled_names = [h.name for h in unfilled]\n            raise ValueError(\n                f\"Cannot instantiate sketch with unfilled holes: {unfilled_names}\"\n            )\n\n        # Start with template\n        code = self.template\n\n        # Replace each hole marker with its filled value\n        for hole in self.holes:\n            marker = f\"__HOLE_{hole.id}__\"\n            value = hole.filled_value\n\n            # Convert value to string representation\n            if isinstance(value, bool):\n                # Python boolean: True/False (capitalized)\n                value_str = str(value)\n            elif isinstance(value, str):\n                # String: keep quotes if needed\n                # Check if it looks like a variable name or literal\n                if value.startswith('\"') or value.startswith(\"'\"):\n                    value_str = value  # Already quoted\n                else:\n                    # Could be variable name - don't quote\n                    value_str = value\n            elif value is None:\n                value_str = \"None\"\n            else:\n                # Numeric or other types\n                value_str = str(value)\n\n            # Replace all occurrences of this marker\n            code = code.replace(marker, value_str)\n\n        return code\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Serialize program sketch to dictionary.\n\n        Returns:\n            Dictionary with all sketch fields\n\n        Example:\n            &gt;&gt;&gt; sketch = ProgramSketch(\n            ...     template=\"x = __HOLE_h1__\",\n            ...     holes=[Hole(id=\"h1\", name=\"x\", hole_type=\"value\")],\n            ...     language=\"python\"\n            ... )\n            &gt;&gt;&gt; d = sketch.to_dict()\n            &gt;&gt;&gt; d[\"language\"]\n            'python'\n        \"\"\"\n        return {\n            \"template\": self.template,\n            \"holes\": [\n                {\n                    \"id\": h.id,\n                    \"name\": h.name,\n                    \"hole_type\": h.hole_type,\n                    \"constraints\": h.constraints,\n                    \"possible_values\": h.possible_values,\n                    \"filled_value\": h.filled_value,\n                    \"location\": h.location\n                }\n                for h in self.holes\n            ],\n            \"language\": self.language,\n            \"framework\": self.framework,\n            \"constraints\": self.constraints\n        }\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -&gt; \"ProgramSketch\":\n        \"\"\"\n        Deserialize program sketch from dictionary.\n\n        Args:\n            data: Dictionary containing sketch fields\n\n        Returns:\n            ProgramSketch instance\n\n        Example:\n            &gt;&gt;&gt; data = {\n            ...     \"template\": \"x = __HOLE_h1__\",\n            ...     \"holes\": [{\n            ...         \"id\": \"h1\",\n            ...         \"name\": \"x\",\n            ...         \"hole_type\": \"value\",\n            ...         \"constraints\": [],\n            ...         \"possible_values\": None,\n            ...         \"filled_value\": None,\n            ...         \"location\": None\n            ...     }],\n            ...     \"language\": \"python\",\n            ...     \"framework\": \"\",\n            ...     \"constraints\": []\n            ... }\n            &gt;&gt;&gt; sketch = ProgramSketch.from_dict(data)\n            &gt;&gt;&gt; sketch.language\n            'python'\n        \"\"\"\n        holes = [\n            Hole(\n                id=h[\"id\"],\n                name=h[\"name\"],\n                hole_type=h[\"hole_type\"],\n                constraints=h.get(\"constraints\", []),\n                possible_values=h.get(\"possible_values\"),\n                filled_value=h.get(\"filled_value\"),\n                location=h.get(\"location\")\n            )\n            for h in data.get(\"holes\", [])\n        ]\n\n        return cls(\n            template=data[\"template\"],\n            holes=holes,\n            language=data.get(\"language\", \"python\"),\n            framework=data.get(\"framework\", \"\"),\n            constraints=data.get(\"constraints\", [])\n        )\n\n    def __str__(self) -&gt; str:\n        \"\"\"Human-readable representation.\"\"\"\n        filled_count = len(self.holes) - len(self.get_unfilled_holes())\n        total_count = len(self.holes)\n        return (\n            f\"ProgramSketch({self.language}, \"\n            f\"{filled_count}/{total_count} holes filled)\"\n        )\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Developer-friendly representation.\"\"\"\n        return (\n            f\"ProgramSketch(language='{self.language}', \"\n            f\"holes={len(self.holes)}, \"\n            f\"unfilled={len(self.get_unfilled_holes())})\"\n        )\n</code></pre>"},{"location":"api/synthesis/sketch/#upir.synthesis.sketch.ProgramSketch-functions","title":"Functions","text":""},{"location":"api/synthesis/sketch/#upir.synthesis.sketch.ProgramSketch.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate program sketch configuration.</p> Source code in <code>upir/synthesis/sketch.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate program sketch configuration.\"\"\"\n    if not self.template:\n        raise ValueError(\"Template cannot be empty\")\n\n    # Verify all hole IDs are unique\n    hole_ids = [h.id for h in self.holes]\n    if len(hole_ids) != len(set(hole_ids)):\n        raise ValueError(\"Hole IDs must be unique\")\n</code></pre>"},{"location":"api/synthesis/sketch/#upir.synthesis.sketch.ProgramSketch.get_unfilled_holes","title":"<code>get_unfilled_holes()</code>","text":"<p>Get list of holes that have not been filled yet.</p> <p>Returns:</p> Type Description <code>List[Hole]</code> <p>List of unfilled Hole objects</p> Example <p>sketch = ProgramSketch( ...     template=\"x = HOLE_h1 + HOLE_h2\", ...     holes=[ ...         Hole(id=\"h1\", name=\"a\", hole_type=\"value\"), ...         Hole(id=\"h2\", name=\"b\", hole_type=\"value\") ...     ] ... ) len(sketch.get_unfilled_holes()) 2 sketch.fill_hole(\"h1\", 10) True len(sketch.get_unfilled_holes()) 1</p> Source code in <code>upir/synthesis/sketch.py</code> <pre><code>def get_unfilled_holes(self) -&gt; List[Hole]:\n    \"\"\"\n    Get list of holes that have not been filled yet.\n\n    Returns:\n        List of unfilled Hole objects\n\n    Example:\n        &gt;&gt;&gt; sketch = ProgramSketch(\n        ...     template=\"x = __HOLE_h1__ + __HOLE_h2__\",\n        ...     holes=[\n        ...         Hole(id=\"h1\", name=\"a\", hole_type=\"value\"),\n        ...         Hole(id=\"h2\", name=\"b\", hole_type=\"value\")\n        ...     ]\n        ... )\n        &gt;&gt;&gt; len(sketch.get_unfilled_holes())\n        2\n        &gt;&gt;&gt; sketch.fill_hole(\"h1\", 10)\n        True\n        &gt;&gt;&gt; len(sketch.get_unfilled_holes())\n        1\n    \"\"\"\n    return [hole for hole in self.holes if not hole.is_filled()]\n</code></pre>"},{"location":"api/synthesis/sketch/#upir.synthesis.sketch.ProgramSketch.fill_hole","title":"<code>fill_hole(hole_id, value)</code>","text":"<p>Fill a hole with a specific value.</p> <p>Parameters:</p> Name Type Description Default <code>hole_id</code> <code>str</code> <p>ID of the hole to fill</p> required <code>value</code> <code>Any</code> <p>Value to fill the hole with</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if hole was filled successfully, False if hole not found</p> Example <p>sketch = ProgramSketch( ...     template=\"size = HOLE_h1\", ...     holes=[Hole(id=\"h1\", name=\"size\", hole_type=\"value\")] ... ) sketch.fill_hole(\"h1\", 42) True sketch.fill_hole(\"nonexistent\", 10) False</p> Source code in <code>upir/synthesis/sketch.py</code> <pre><code>def fill_hole(self, hole_id: str, value: Any) -&gt; bool:\n    \"\"\"\n    Fill a hole with a specific value.\n\n    Args:\n        hole_id: ID of the hole to fill\n        value: Value to fill the hole with\n\n    Returns:\n        True if hole was filled successfully, False if hole not found\n\n    Example:\n        &gt;&gt;&gt; sketch = ProgramSketch(\n        ...     template=\"size = __HOLE_h1__\",\n        ...     holes=[Hole(id=\"h1\", name=\"size\", hole_type=\"value\")]\n        ... )\n        &gt;&gt;&gt; sketch.fill_hole(\"h1\", 42)\n        True\n        &gt;&gt;&gt; sketch.fill_hole(\"nonexistent\", 10)\n        False\n    \"\"\"\n    for hole in self.holes:\n        if hole.id == hole_id:\n            hole.filled_value = value\n            return True\n    return False\n</code></pre>"},{"location":"api/synthesis/sketch/#upir.synthesis.sketch.ProgramSketch.instantiate","title":"<code>instantiate()</code>","text":"<p>Instantiate the sketch by replacing all holes with their filled values.</p> <p>Replaces all HOLE_{id} markers in the template with the corresponding filled values. Handles type conversion (bool to \"True\"/\"False\", etc.).</p> <p>Returns:</p> Type Description <code>str</code> <p>Complete code with all holes filled</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any holes are unfilled</p> Example <p>template = \"result = HOLE_h1 + HOLE_h2\" sketch = ProgramSketch( ...     template=template, ...     holes=[ ...         Hole(id=\"h1\", name=\"a\", hole_type=\"value\"), ...         Hole(id=\"h2\", name=\"b\", hole_type=\"value\") ...     ] ... ) sketch.fill_hole(\"h1\", 10) True sketch.fill_hole(\"h2\", 20) True code = sketch.instantiate() code 'result = 10 + 20'</p> <p>References: - CEGIS: Sketch instantiation - String replacement for code generation</p> Source code in <code>upir/synthesis/sketch.py</code> <pre><code>def instantiate(self) -&gt; str:\n    \"\"\"\n    Instantiate the sketch by replacing all holes with their filled values.\n\n    Replaces all __HOLE_{id}__ markers in the template with the\n    corresponding filled values. Handles type conversion (bool to\n    \"True\"/\"False\", etc.).\n\n    Returns:\n        Complete code with all holes filled\n\n    Raises:\n        ValueError: If any holes are unfilled\n\n    Example:\n        &gt;&gt;&gt; template = \"result = __HOLE_h1__ + __HOLE_h2__\"\n        &gt;&gt;&gt; sketch = ProgramSketch(\n        ...     template=template,\n        ...     holes=[\n        ...         Hole(id=\"h1\", name=\"a\", hole_type=\"value\"),\n        ...         Hole(id=\"h2\", name=\"b\", hole_type=\"value\")\n        ...     ]\n        ... )\n        &gt;&gt;&gt; sketch.fill_hole(\"h1\", 10)\n        True\n        &gt;&gt;&gt; sketch.fill_hole(\"h2\", 20)\n        True\n        &gt;&gt;&gt; code = sketch.instantiate()\n        &gt;&gt;&gt; code\n        'result = 10 + 20'\n\n    References:\n    - CEGIS: Sketch instantiation\n    - String replacement for code generation\n    \"\"\"\n    unfilled = self.get_unfilled_holes()\n    if unfilled:\n        unfilled_names = [h.name for h in unfilled]\n        raise ValueError(\n            f\"Cannot instantiate sketch with unfilled holes: {unfilled_names}\"\n        )\n\n    # Start with template\n    code = self.template\n\n    # Replace each hole marker with its filled value\n    for hole in self.holes:\n        marker = f\"__HOLE_{hole.id}__\"\n        value = hole.filled_value\n\n        # Convert value to string representation\n        if isinstance(value, bool):\n            # Python boolean: True/False (capitalized)\n            value_str = str(value)\n        elif isinstance(value, str):\n            # String: keep quotes if needed\n            # Check if it looks like a variable name or literal\n            if value.startswith('\"') or value.startswith(\"'\"):\n                value_str = value  # Already quoted\n            else:\n                # Could be variable name - don't quote\n                value_str = value\n        elif value is None:\n            value_str = \"None\"\n        else:\n            # Numeric or other types\n            value_str = str(value)\n\n        # Replace all occurrences of this marker\n        code = code.replace(marker, value_str)\n\n    return code\n</code></pre>"},{"location":"api/synthesis/sketch/#upir.synthesis.sketch.ProgramSketch.to_dict","title":"<code>to_dict()</code>","text":"<p>Serialize program sketch to dictionary.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with all sketch fields</p> Example <p>sketch = ProgramSketch( ...     template=\"x = HOLE_h1\", ...     holes=[Hole(id=\"h1\", name=\"x\", hole_type=\"value\")], ...     language=\"python\" ... ) d = sketch.to_dict() d[\"language\"] 'python'</p> Source code in <code>upir/synthesis/sketch.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Serialize program sketch to dictionary.\n\n    Returns:\n        Dictionary with all sketch fields\n\n    Example:\n        &gt;&gt;&gt; sketch = ProgramSketch(\n        ...     template=\"x = __HOLE_h1__\",\n        ...     holes=[Hole(id=\"h1\", name=\"x\", hole_type=\"value\")],\n        ...     language=\"python\"\n        ... )\n        &gt;&gt;&gt; d = sketch.to_dict()\n        &gt;&gt;&gt; d[\"language\"]\n        'python'\n    \"\"\"\n    return {\n        \"template\": self.template,\n        \"holes\": [\n            {\n                \"id\": h.id,\n                \"name\": h.name,\n                \"hole_type\": h.hole_type,\n                \"constraints\": h.constraints,\n                \"possible_values\": h.possible_values,\n                \"filled_value\": h.filled_value,\n                \"location\": h.location\n            }\n            for h in self.holes\n        ],\n        \"language\": self.language,\n        \"framework\": self.framework,\n        \"constraints\": self.constraints\n    }\n</code></pre>"},{"location":"api/synthesis/sketch/#upir.synthesis.sketch.ProgramSketch.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Deserialize program sketch from dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>Dictionary containing sketch fields</p> required <p>Returns:</p> Type Description <code>ProgramSketch</code> <p>ProgramSketch instance</p> Example <p>data = { ...     \"template\": \"x = HOLE_h1\", ...     \"holes\": [{ ...         \"id\": \"h1\", ...         \"name\": \"x\", ...         \"hole_type\": \"value\", ...         \"constraints\": [], ...         \"possible_values\": None, ...         \"filled_value\": None, ...         \"location\": None ...     }], ...     \"language\": \"python\", ...     \"framework\": \"\", ...     \"constraints\": [] ... } sketch = ProgramSketch.from_dict(data) sketch.language 'python'</p> Source code in <code>upir/synthesis/sketch.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, Any]) -&gt; \"ProgramSketch\":\n    \"\"\"\n    Deserialize program sketch from dictionary.\n\n    Args:\n        data: Dictionary containing sketch fields\n\n    Returns:\n        ProgramSketch instance\n\n    Example:\n        &gt;&gt;&gt; data = {\n        ...     \"template\": \"x = __HOLE_h1__\",\n        ...     \"holes\": [{\n        ...         \"id\": \"h1\",\n        ...         \"name\": \"x\",\n        ...         \"hole_type\": \"value\",\n        ...         \"constraints\": [],\n        ...         \"possible_values\": None,\n        ...         \"filled_value\": None,\n        ...         \"location\": None\n        ...     }],\n        ...     \"language\": \"python\",\n        ...     \"framework\": \"\",\n        ...     \"constraints\": []\n        ... }\n        &gt;&gt;&gt; sketch = ProgramSketch.from_dict(data)\n        &gt;&gt;&gt; sketch.language\n        'python'\n    \"\"\"\n    holes = [\n        Hole(\n            id=h[\"id\"],\n            name=h[\"name\"],\n            hole_type=h[\"hole_type\"],\n            constraints=h.get(\"constraints\", []),\n            possible_values=h.get(\"possible_values\"),\n            filled_value=h.get(\"filled_value\"),\n            location=h.get(\"location\")\n        )\n        for h in data.get(\"holes\", [])\n    ]\n\n    return cls(\n        template=data[\"template\"],\n        holes=holes,\n        language=data.get(\"language\", \"python\"),\n        framework=data.get(\"framework\", \"\"),\n        constraints=data.get(\"constraints\", [])\n    )\n</code></pre>"},{"location":"api/synthesis/sketch/#upir.synthesis.sketch.ProgramSketch.__str__","title":"<code>__str__()</code>","text":"<p>Human-readable representation.</p> Source code in <code>upir/synthesis/sketch.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Human-readable representation.\"\"\"\n    filled_count = len(self.holes) - len(self.get_unfilled_holes())\n    total_count = len(self.holes)\n    return (\n        f\"ProgramSketch({self.language}, \"\n        f\"{filled_count}/{total_count} holes filled)\"\n    )\n</code></pre>"},{"location":"api/synthesis/sketch/#upir.synthesis.sketch.ProgramSketch.__repr__","title":"<code>__repr__()</code>","text":"<p>Developer-friendly representation.</p> Source code in <code>upir/synthesis/sketch.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Developer-friendly representation.\"\"\"\n    return (\n        f\"ProgramSketch(language='{self.language}', \"\n        f\"holes={len(self.holes)}, \"\n        f\"unfilled={len(self.get_unfilled_holes())})\"\n    )\n</code></pre>"},{"location":"api/synthesis/sketch/#upir.synthesis.sketch.Hole","title":"<code>upir.synthesis.sketch.Hole</code>  <code>dataclass</code>","text":"<p>A hole in a program sketch that needs to be filled by synthesis.</p> <p>Per CEGIS methodology, holes represent unknown parts of a program that the synthesis engine must determine. Each hole has a type (value, expression, predicate, function) and constraints on valid values.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier for this hole</p> <code>name</code> <code>str</code> <p>Descriptive name (e.g., \"window_size\", \"batch_count\")</p> <code>hole_type</code> <code>str</code> <p>Type of hole - \"value\", \"expression\", \"predicate\", \"function\"</p> <code>constraints</code> <code>List[Any]</code> <p>List of constraints on valid values         Format: [(\"range\", min, max), (\"oneof\", [v1, v2, v3]), ...]</p> <code>possible_values</code> <code>Optional[List[Any]]</code> <p>Optional explicit list of possible values</p> <code>filled_value</code> <code>Optional[Any]</code> <p>The value this hole has been filled with (if any)</p> <code>location</code> <code>Optional[Dict[str, Any]]</code> <p>Optional location info (line number, context)</p> Example <p>References: - CEGIS: Holes are unknowns to be synthesized - TD Commons: Program sketch representation</p> Source code in <code>upir/synthesis/sketch.py</code> <pre><code>@dataclass\nclass Hole:\n    \"\"\"\n    A hole in a program sketch that needs to be filled by synthesis.\n\n    Per CEGIS methodology, holes represent unknown parts of a program that\n    the synthesis engine must determine. Each hole has a type (value,\n    expression, predicate, function) and constraints on valid values.\n\n    Attributes:\n        id: Unique identifier for this hole\n        name: Descriptive name (e.g., \"window_size\", \"batch_count\")\n        hole_type: Type of hole - \"value\", \"expression\", \"predicate\", \"function\"\n        constraints: List of constraints on valid values\n                    Format: [(\"range\", min, max), (\"oneof\", [v1, v2, v3]), ...]\n        possible_values: Optional explicit list of possible values\n        filled_value: The value this hole has been filled with (if any)\n        location: Optional location info (line number, context)\n\n    Example:\n        &gt;&gt;&gt; # Integer value hole with range constraint\n        &gt;&gt;&gt; hole = Hole(\n        ...     id=\"h1\",\n        ...     name=\"window_size\",\n        ...     hole_type=\"value\",\n        ...     constraints=[(\"range\", 1, 100)],\n        ...     location={\"line\": 42, \"context\": \"GroupByKey\"}\n        ... )\n        &gt;&gt;&gt; hole.is_filled()\n        False\n        &gt;&gt;&gt; hole.filled_value = 10\n        &gt;&gt;&gt; hole.is_filled()\n        True\n\n    References:\n    - CEGIS: Holes are unknowns to be synthesized\n    - TD Commons: Program sketch representation\n    \"\"\"\n\n    id: str\n    name: str\n    hole_type: str  # \"value\", \"expression\", \"predicate\", \"function\"\n    constraints: List[Any] = field(default_factory=list)\n    possible_values: Optional[List[Any]] = None\n    filled_value: Optional[Any] = None\n    location: Optional[Dict[str, Any]] = None\n\n    def __post_init__(self):\n        \"\"\"Validate hole configuration.\"\"\"\n        valid_types = [\"value\", \"expression\", \"predicate\", \"function\"]\n        if self.hole_type not in valid_types:\n            raise ValueError(\n                f\"Invalid hole_type: {self.hole_type}. \"\n                f\"Must be one of {valid_types}\"\n            )\n\n        if not self.id:\n            raise ValueError(\"Hole id cannot be empty\")\n\n        if not self.name:\n            raise ValueError(\"Hole name cannot be empty\")\n\n    def is_filled(self) -&gt; bool:\n        \"\"\"\n        Check if this hole has been filled with a value.\n\n        Returns:\n            True if hole has a filled_value, False otherwise\n\n        Example:\n            &gt;&gt;&gt; hole = Hole(id=\"h1\", name=\"size\", hole_type=\"value\")\n            &gt;&gt;&gt; hole.is_filled()\n            False\n            &gt;&gt;&gt; hole.filled_value = 42\n            &gt;&gt;&gt; hole.is_filled()\n            True\n        \"\"\"\n        return self.filled_value is not None\n\n    def to_z3_var(self) -&gt; Any:\n        \"\"\"\n        Convert hole to Z3 variable for SMT-based synthesis.\n\n        Creates appropriate Z3 variable based on hole type:\n        - \"value\": Integer variable (z3.Int)\n        - \"expression\": Real variable (z3.Real)\n        - \"predicate\": Boolean variable (z3.Bool)\n        - \"function\": Returns None (requires special handling)\n\n        Returns:\n            Z3 variable or None for function holes\n\n        Raises:\n            RuntimeError: If Z3 is not available\n\n        Example:\n            &gt;&gt;&gt; hole = Hole(id=\"h1\", name=\"count\", hole_type=\"value\")\n            &gt;&gt;&gt; var = hole.to_z3_var()\n            &gt;&gt;&gt; # var is z3.Int(\"hole_h1\")\n\n        References:\n        - Z3 API: Variable creation\n        - CEGIS: Encoding unknowns as SMT variables\n        \"\"\"\n        if not is_z3_available():\n            raise RuntimeError(\n                \"Z3 solver is not available. \"\n                \"Install with: pip install z3-solver\"\n            )\n\n        var_name = f\"hole_{self.id}\"\n\n        if self.hole_type == \"value\":\n            # Integer value hole\n            return z3.Int(var_name)\n\n        elif self.hole_type == \"expression\":\n            # Real-valued expression hole\n            return z3.Real(var_name)\n\n        elif self.hole_type == \"predicate\":\n            # Boolean predicate hole\n            return z3.Bool(var_name)\n\n        elif self.hole_type == \"function\":\n            # Function holes require special handling\n            # Cannot be represented as simple Z3 variable\n            return None\n\n        else:\n            # Should not reach here due to __post_init__ validation\n            raise ValueError(f\"Unknown hole type: {self.hole_type}\")\n\n    def get_constraints_as_z3(self) -&gt; List[Any]:\n        \"\"\"\n        Convert hole constraints to Z3 constraints.\n\n        Translates constraint specifications into Z3 constraint expressions\n        that can be added to the solver.\n\n        Returns:\n            List of Z3 constraint expressions\n\n        Example:\n            &gt;&gt;&gt; hole = Hole(\n            ...     id=\"h1\",\n            ...     name=\"size\",\n            ...     hole_type=\"value\",\n            ...     constraints=[(\"range\", 1, 100)]\n            ... )\n            &gt;&gt;&gt; z3_constraints = hole.get_constraints_as_z3()\n            &gt;&gt;&gt; # z3_constraints[0] is (hole_h1 &gt;= 1)\n            &gt;&gt;&gt; # z3_constraints[1] is (hole_h1 &lt;= 100)\n\n        References:\n        - Z3: Constraint construction\n        - CEGIS: Encoding hole constraints\n        \"\"\"\n        if not is_z3_available():\n            raise RuntimeError(\"Z3 solver is not available\")\n\n        var = self.to_z3_var()\n        if var is None:\n            return []  # Function holes have no direct constraints\n\n        z3_constraints = []\n\n        for constraint in self.constraints:\n            if not constraint:\n                continue\n\n            constraint_type = constraint[0] if isinstance(constraint, tuple) else None\n\n            if constraint_type == \"range\" and len(constraint) &gt;= 3:\n                # Range constraint: min &lt;= var &lt;= max\n                min_val = constraint[1]\n                max_val = constraint[2]\n                z3_constraints.append(var &gt;= min_val)\n                z3_constraints.append(var &lt;= max_val)\n\n            elif constraint_type == \"oneof\" and len(constraint) &gt;= 2:\n                # OneOf constraint: var in {v1, v2, v3, ...}\n                values = constraint[1]\n                if values:\n                    # Create disjunction: var == v1 OR var == v2 OR ...\n                    or_clauses = [var == v for v in values]\n                    z3_constraints.append(z3.Or(or_clauses))\n\n            elif constraint_type == \"gt\" and len(constraint) &gt;= 2:\n                # Greater than constraint\n                value = constraint[1]\n                z3_constraints.append(var &gt; value)\n\n            elif constraint_type == \"lt\" and len(constraint) &gt;= 2:\n                # Less than constraint\n                value = constraint[1]\n                z3_constraints.append(var &lt; value)\n\n            elif constraint_type == \"ne\" and len(constraint) &gt;= 2:\n                # Not equal constraint\n                value = constraint[1]\n                z3_constraints.append(var != value)\n\n        return z3_constraints\n\n    def __str__(self) -&gt; str:\n        \"\"\"Human-readable representation.\"\"\"\n        status = \"FILLED\" if self.is_filled() else \"UNFILLED\"\n        value_str = f\"={self.filled_value}\" if self.is_filled() else \"\"\n        return f\"Hole({self.name}:{self.hole_type}, {status}{value_str})\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Developer-friendly representation.\"\"\"\n        return (\n            f\"Hole(id='{self.id}', name='{self.name}', \"\n            f\"type='{self.hole_type}', filled={self.is_filled()})\"\n        )\n</code></pre>"},{"location":"api/synthesis/sketch/#upir.synthesis.sketch.Hole--integer-value-hole-with-range-constraint","title":"Integer value hole with range constraint","text":"<p>hole = Hole( ...     id=\"h1\", ...     name=\"window_size\", ...     hole_type=\"value\", ...     constraints=[(\"range\", 1, 100)], ...     location={\"line\": 42, \"context\": \"GroupByKey\"} ... ) hole.is_filled() False hole.filled_value = 10 hole.is_filled() True</p>"},{"location":"api/synthesis/sketch/#upir.synthesis.sketch.Hole-functions","title":"Functions","text":""},{"location":"api/synthesis/sketch/#upir.synthesis.sketch.Hole.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate hole configuration.</p> Source code in <code>upir/synthesis/sketch.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate hole configuration.\"\"\"\n    valid_types = [\"value\", \"expression\", \"predicate\", \"function\"]\n    if self.hole_type not in valid_types:\n        raise ValueError(\n            f\"Invalid hole_type: {self.hole_type}. \"\n            f\"Must be one of {valid_types}\"\n        )\n\n    if not self.id:\n        raise ValueError(\"Hole id cannot be empty\")\n\n    if not self.name:\n        raise ValueError(\"Hole name cannot be empty\")\n</code></pre>"},{"location":"api/synthesis/sketch/#upir.synthesis.sketch.Hole.is_filled","title":"<code>is_filled()</code>","text":"<p>Check if this hole has been filled with a value.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if hole has a filled_value, False otherwise</p> Example <p>hole = Hole(id=\"h1\", name=\"size\", hole_type=\"value\") hole.is_filled() False hole.filled_value = 42 hole.is_filled() True</p> Source code in <code>upir/synthesis/sketch.py</code> <pre><code>def is_filled(self) -&gt; bool:\n    \"\"\"\n    Check if this hole has been filled with a value.\n\n    Returns:\n        True if hole has a filled_value, False otherwise\n\n    Example:\n        &gt;&gt;&gt; hole = Hole(id=\"h1\", name=\"size\", hole_type=\"value\")\n        &gt;&gt;&gt; hole.is_filled()\n        False\n        &gt;&gt;&gt; hole.filled_value = 42\n        &gt;&gt;&gt; hole.is_filled()\n        True\n    \"\"\"\n    return self.filled_value is not None\n</code></pre>"},{"location":"api/synthesis/sketch/#upir.synthesis.sketch.Hole.to_z3_var","title":"<code>to_z3_var()</code>","text":"<p>Convert hole to Z3 variable for SMT-based synthesis.</p> <p>Creates appropriate Z3 variable based on hole type: - \"value\": Integer variable (z3.Int) - \"expression\": Real variable (z3.Real) - \"predicate\": Boolean variable (z3.Bool) - \"function\": Returns None (requires special handling)</p> <p>Returns:</p> Type Description <code>Any</code> <p>Z3 variable or None for function holes</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If Z3 is not available</p> Example <p>hole = Hole(id=\"h1\", name=\"count\", hole_type=\"value\") var = hole.to_z3_var()</p> <p>References: - Z3 API: Variable creation - CEGIS: Encoding unknowns as SMT variables</p> Source code in <code>upir/synthesis/sketch.py</code> <pre><code>def to_z3_var(self) -&gt; Any:\n    \"\"\"\n    Convert hole to Z3 variable for SMT-based synthesis.\n\n    Creates appropriate Z3 variable based on hole type:\n    - \"value\": Integer variable (z3.Int)\n    - \"expression\": Real variable (z3.Real)\n    - \"predicate\": Boolean variable (z3.Bool)\n    - \"function\": Returns None (requires special handling)\n\n    Returns:\n        Z3 variable or None for function holes\n\n    Raises:\n        RuntimeError: If Z3 is not available\n\n    Example:\n        &gt;&gt;&gt; hole = Hole(id=\"h1\", name=\"count\", hole_type=\"value\")\n        &gt;&gt;&gt; var = hole.to_z3_var()\n        &gt;&gt;&gt; # var is z3.Int(\"hole_h1\")\n\n    References:\n    - Z3 API: Variable creation\n    - CEGIS: Encoding unknowns as SMT variables\n    \"\"\"\n    if not is_z3_available():\n        raise RuntimeError(\n            \"Z3 solver is not available. \"\n            \"Install with: pip install z3-solver\"\n        )\n\n    var_name = f\"hole_{self.id}\"\n\n    if self.hole_type == \"value\":\n        # Integer value hole\n        return z3.Int(var_name)\n\n    elif self.hole_type == \"expression\":\n        # Real-valued expression hole\n        return z3.Real(var_name)\n\n    elif self.hole_type == \"predicate\":\n        # Boolean predicate hole\n        return z3.Bool(var_name)\n\n    elif self.hole_type == \"function\":\n        # Function holes require special handling\n        # Cannot be represented as simple Z3 variable\n        return None\n\n    else:\n        # Should not reach here due to __post_init__ validation\n        raise ValueError(f\"Unknown hole type: {self.hole_type}\")\n</code></pre>"},{"location":"api/synthesis/sketch/#upir.synthesis.sketch.Hole.to_z3_var--var-is-z3inthole_h1","title":"var is z3.Int(\"hole_h1\")","text":""},{"location":"api/synthesis/sketch/#upir.synthesis.sketch.Hole.get_constraints_as_z3","title":"<code>get_constraints_as_z3()</code>","text":"<p>Convert hole constraints to Z3 constraints.</p> <p>Translates constraint specifications into Z3 constraint expressions that can be added to the solver.</p> <p>Returns:</p> Type Description <code>List[Any]</code> <p>List of Z3 constraint expressions</p> Example <p>hole = Hole( ...     id=\"h1\", ...     name=\"size\", ...     hole_type=\"value\", ...     constraints=[(\"range\", 1, 100)] ... ) z3_constraints = hole.get_constraints_as_z3()</p> <p>References: - Z3: Constraint construction - CEGIS: Encoding hole constraints</p> Source code in <code>upir/synthesis/sketch.py</code> <pre><code>def get_constraints_as_z3(self) -&gt; List[Any]:\n    \"\"\"\n    Convert hole constraints to Z3 constraints.\n\n    Translates constraint specifications into Z3 constraint expressions\n    that can be added to the solver.\n\n    Returns:\n        List of Z3 constraint expressions\n\n    Example:\n        &gt;&gt;&gt; hole = Hole(\n        ...     id=\"h1\",\n        ...     name=\"size\",\n        ...     hole_type=\"value\",\n        ...     constraints=[(\"range\", 1, 100)]\n        ... )\n        &gt;&gt;&gt; z3_constraints = hole.get_constraints_as_z3()\n        &gt;&gt;&gt; # z3_constraints[0] is (hole_h1 &gt;= 1)\n        &gt;&gt;&gt; # z3_constraints[1] is (hole_h1 &lt;= 100)\n\n    References:\n    - Z3: Constraint construction\n    - CEGIS: Encoding hole constraints\n    \"\"\"\n    if not is_z3_available():\n        raise RuntimeError(\"Z3 solver is not available\")\n\n    var = self.to_z3_var()\n    if var is None:\n        return []  # Function holes have no direct constraints\n\n    z3_constraints = []\n\n    for constraint in self.constraints:\n        if not constraint:\n            continue\n\n        constraint_type = constraint[0] if isinstance(constraint, tuple) else None\n\n        if constraint_type == \"range\" and len(constraint) &gt;= 3:\n            # Range constraint: min &lt;= var &lt;= max\n            min_val = constraint[1]\n            max_val = constraint[2]\n            z3_constraints.append(var &gt;= min_val)\n            z3_constraints.append(var &lt;= max_val)\n\n        elif constraint_type == \"oneof\" and len(constraint) &gt;= 2:\n            # OneOf constraint: var in {v1, v2, v3, ...}\n            values = constraint[1]\n            if values:\n                # Create disjunction: var == v1 OR var == v2 OR ...\n                or_clauses = [var == v for v in values]\n                z3_constraints.append(z3.Or(or_clauses))\n\n        elif constraint_type == \"gt\" and len(constraint) &gt;= 2:\n            # Greater than constraint\n            value = constraint[1]\n            z3_constraints.append(var &gt; value)\n\n        elif constraint_type == \"lt\" and len(constraint) &gt;= 2:\n            # Less than constraint\n            value = constraint[1]\n            z3_constraints.append(var &lt; value)\n\n        elif constraint_type == \"ne\" and len(constraint) &gt;= 2:\n            # Not equal constraint\n            value = constraint[1]\n            z3_constraints.append(var != value)\n\n    return z3_constraints\n</code></pre>"},{"location":"api/synthesis/sketch/#upir.synthesis.sketch.Hole.get_constraints_as_z3--z3_constraints0-is-hole_h1-1","title":"z3_constraints[0] is (hole_h1 &gt;= 1)","text":""},{"location":"api/synthesis/sketch/#upir.synthesis.sketch.Hole.get_constraints_as_z3--z3_constraints1-is-hole_h1-100","title":"z3_constraints[1] is (hole_h1 &lt;= 100)","text":""},{"location":"api/synthesis/sketch/#upir.synthesis.sketch.Hole.__str__","title":"<code>__str__()</code>","text":"<p>Human-readable representation.</p> Source code in <code>upir/synthesis/sketch.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Human-readable representation.\"\"\"\n    status = \"FILLED\" if self.is_filled() else \"UNFILLED\"\n    value_str = f\"={self.filled_value}\" if self.is_filled() else \"\"\n    return f\"Hole({self.name}:{self.hole_type}, {status}{value_str})\"\n</code></pre>"},{"location":"api/synthesis/sketch/#upir.synthesis.sketch.Hole.__repr__","title":"<code>__repr__()</code>","text":"<p>Developer-friendly representation.</p> Source code in <code>upir/synthesis/sketch.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Developer-friendly representation.\"\"\"\n    return (\n        f\"Hole(id='{self.id}', name='{self.name}', \"\n        f\"type='{self.hole_type}', filled={self.is_filled()})\"\n    )\n</code></pre>"},{"location":"api/synthesis/sketch/#see-also","title":"See Also","text":"<ul> <li>CEGIS - Synthesize code from sketches</li> </ul>"},{"location":"api/verification/solver/","title":"SMT Solver","text":"<p>Low-level SMT solving with Z3.</p>"},{"location":"api/verification/solver/#overview","title":"Overview","text":"<p>The SMT solver module provides:</p> <ul> <li>SMTVerifier: Low-level Z3 integration</li> <li>VerificationResult: Verification results</li> <li>VerificationStatus: Result status enum</li> </ul>"},{"location":"api/verification/solver/#class-documentation","title":"Class Documentation","text":""},{"location":"api/verification/solver/#upir.verification.solver.VerificationStatus","title":"<code>upir.verification.solver.VerificationStatus</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Status of a formal verification attempt.</p> <p>Based on standard SMT solver result categories. Verification can: - PROVED: Property definitely holds (satisfiable/valid) - DISPROVED: Property definitely does not hold (counterexample found) - UNKNOWN: Solver cannot determine (incomplete theory, heuristics failed) - TIMEOUT: Verification exceeded time limit - ERROR: Internal error during verification</p> <p>References: - SMT-LIB standard: Standard result categories - Z3 solver results: sat, unsat, unknown</p> Source code in <code>upir/verification/solver.py</code> <pre><code>class VerificationStatus(Enum):\n    \"\"\"\n    Status of a formal verification attempt.\n\n    Based on standard SMT solver result categories. Verification can:\n    - PROVED: Property definitely holds (satisfiable/valid)\n    - DISPROVED: Property definitely does not hold (counterexample found)\n    - UNKNOWN: Solver cannot determine (incomplete theory, heuristics failed)\n    - TIMEOUT: Verification exceeded time limit\n    - ERROR: Internal error during verification\n\n    References:\n    - SMT-LIB standard: Standard result categories\n    - Z3 solver results: sat, unsat, unknown\n    \"\"\"\n    PROVED = \"PROVED\"\n    DISPROVED = \"DISPROVED\"\n    UNKNOWN = \"UNKNOWN\"\n    TIMEOUT = \"TIMEOUT\"\n    ERROR = \"ERROR\"\n</code></pre>"},{"location":"api/verification/solver/#upir.verification.solver.VerificationResult","title":"<code>upir.verification.solver.VerificationResult</code>  <code>dataclass</code>","text":"<p>Result of verifying a temporal property against an architecture.</p> <p>Verification results capture everything needed to understand whether a property holds, including the property itself, verification status, optional proof certificate, counterexamples if disproved, and metadata.</p> <p>Based on TD Commons disclosure, verification results enable: - Decision making: Is this architecture safe? - Debugging: Why did verification fail? (counterexample) - Caching: Avoid re-verifying (cached flag) - Provenance: What was proved and when? (certificate)</p> <p>Attributes:</p> Name Type Description <code>property</code> <code>TemporalProperty</code> <p>The temporal property that was verified</p> <code>status</code> <code>VerificationStatus</code> <p>Verification status (PROVED, DISPROVED, etc.)</p> <code>certificate</code> <code>Optional[ProofCertificate]</code> <p>Optional proof certificate for PROVED results</p> <code>counterexample</code> <code>Optional[Dict[str, Any]]</code> <p>Optional counterexample for DISPROVED results</p> <code>execution_time</code> <code>float</code> <p>Time taken for verification (seconds)</p> <code>cached</code> <code>bool</code> <p>Whether result came from cache (not freshly computed)</p> Example <p>result = VerificationResult( ...     property=TemporalProperty(...), ...     status=VerificationStatus.PROVED, ...     certificate=ProofCertificate(...), ...     counterexample=None, ...     execution_time=1.23, ...     cached=False ... ) result.verified True</p> <p>References: - TD Commons: Verification result structure - SMT solving: Standard result format (status + model/proof)</p> Source code in <code>upir/verification/solver.py</code> <pre><code>@dataclass\nclass VerificationResult:\n    \"\"\"\n    Result of verifying a temporal property against an architecture.\n\n    Verification results capture everything needed to understand whether\n    a property holds, including the property itself, verification status,\n    optional proof certificate, counterexamples if disproved, and metadata.\n\n    Based on TD Commons disclosure, verification results enable:\n    - Decision making: Is this architecture safe?\n    - Debugging: Why did verification fail? (counterexample)\n    - Caching: Avoid re-verifying (cached flag)\n    - Provenance: What was proved and when? (certificate)\n\n    Attributes:\n        property: The temporal property that was verified\n        status: Verification status (PROVED, DISPROVED, etc.)\n        certificate: Optional proof certificate for PROVED results\n        counterexample: Optional counterexample for DISPROVED results\n        execution_time: Time taken for verification (seconds)\n        cached: Whether result came from cache (not freshly computed)\n\n    Example:\n        &gt;&gt;&gt; result = VerificationResult(\n        ...     property=TemporalProperty(...),\n        ...     status=VerificationStatus.PROVED,\n        ...     certificate=ProofCertificate(...),\n        ...     counterexample=None,\n        ...     execution_time=1.23,\n        ...     cached=False\n        ... )\n        &gt;&gt;&gt; result.verified\n        True\n\n    References:\n    - TD Commons: Verification result structure\n    - SMT solving: Standard result format (status + model/proof)\n    \"\"\"\n    property: TemporalProperty\n    status: VerificationStatus\n    certificate: Optional[ProofCertificate] = None\n    counterexample: Optional[Dict[str, Any]] = None\n    execution_time: float = 0.0\n    cached: bool = False\n\n    @property\n    def verified(self) -&gt; bool:\n        \"\"\"\n        Check if property was successfully verified (proved).\n\n        Returns:\n            True if status is PROVED, False otherwise\n\n        Example:\n            &gt;&gt;&gt; result = VerificationResult(\n            ...     property=TemporalProperty(...),\n            ...     status=VerificationStatus.PROVED,\n            ...     execution_time=1.0\n            ... )\n            &gt;&gt;&gt; result.verified\n            True\n            &gt;&gt;&gt; result2 = VerificationResult(\n            ...     property=TemporalProperty(...),\n            ...     status=VerificationStatus.UNKNOWN,\n            ...     execution_time=1.0\n            ... )\n            &gt;&gt;&gt; result2.verified\n            False\n        \"\"\"\n        return self.status == VerificationStatus.PROVED\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Serialize verification result to JSON-compatible dictionary.\n\n        Returns:\n            Dictionary with all result fields\n\n        Example:\n            &gt;&gt;&gt; result = VerificationResult(\n            ...     property=TemporalProperty(...),\n            ...     status=VerificationStatus.PROVED,\n            ...     execution_time=1.23\n            ... )\n            &gt;&gt;&gt; d = result.to_dict()\n            &gt;&gt;&gt; d[\"status\"]\n            'PROVED'\n        \"\"\"\n        return {\n            \"property\": self.property.to_dict(),\n            \"status\": self.status.value,\n            \"certificate\": (\n                self.certificate.to_dict() if self.certificate else None\n            ),\n            \"counterexample\": self.counterexample,\n            \"execution_time\": self.execution_time,\n            \"cached\": self.cached\n        }\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -&gt; \"VerificationResult\":\n        \"\"\"\n        Deserialize verification result from dictionary.\n\n        Args:\n            data: Dictionary containing result fields\n\n        Returns:\n            VerificationResult instance\n\n        Example:\n            &gt;&gt;&gt; data = {\n            ...     \"property\": {...},\n            ...     \"status\": \"PROVED\",\n            ...     \"certificate\": None,\n            ...     \"counterexample\": None,\n            ...     \"execution_time\": 1.23,\n            ...     \"cached\": False\n            ... }\n            &gt;&gt;&gt; result = VerificationResult.from_dict(data)\n            &gt;&gt;&gt; result.status == VerificationStatus.PROVED\n            True\n        \"\"\"\n        return cls(\n            property=TemporalProperty.from_dict(data[\"property\"]),\n            status=VerificationStatus(data[\"status\"]),\n            certificate=(\n                ProofCertificate.from_dict(data[\"certificate\"])\n                if data.get(\"certificate\") else None\n            ),\n            counterexample=data.get(\"counterexample\"),\n            execution_time=data.get(\"execution_time\", 0.0),\n            cached=data.get(\"cached\", False)\n        )\n\n    def __str__(self) -&gt; str:\n        \"\"\"Human-readable string representation.\"\"\"\n        parts = [\n            f\"VerificationResult({self.status.value}\",\n            f\"property={self.property.predicate}\",\n        ]\n        if self.cached:\n            parts.append(\"cached=True\")\n        if self.execution_time &gt; 0:\n            parts.append(f\"time={self.execution_time:.3f}s\")\n\n        return \", \".join(parts) + \")\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Developer-friendly representation.\"\"\"\n        return (\n            f\"VerificationResult(status={self.status.value}, \"\n            f\"property={self.property.predicate}, \"\n            f\"verified={self.verified}, \"\n            f\"cached={self.cached})\"\n        )\n</code></pre>"},{"location":"api/verification/solver/#upir.verification.solver.VerificationResult-functions","title":"Functions","text":""},{"location":"api/verification/solver/#upir.verification.solver.VerificationResult.verified","title":"<code>verified()</code>","text":"<p>Check if property was successfully verified (proved).</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if status is PROVED, False otherwise</p> Example <p>result = VerificationResult( ...     property=TemporalProperty(...), ...     status=VerificationStatus.PROVED, ...     execution_time=1.0 ... ) result.verified True result2 = VerificationResult( ...     property=TemporalProperty(...), ...     status=VerificationStatus.UNKNOWN, ...     execution_time=1.0 ... ) result2.verified False</p> Source code in <code>upir/verification/solver.py</code> <pre><code>@property\ndef verified(self) -&gt; bool:\n    \"\"\"\n    Check if property was successfully verified (proved).\n\n    Returns:\n        True if status is PROVED, False otherwise\n\n    Example:\n        &gt;&gt;&gt; result = VerificationResult(\n        ...     property=TemporalProperty(...),\n        ...     status=VerificationStatus.PROVED,\n        ...     execution_time=1.0\n        ... )\n        &gt;&gt;&gt; result.verified\n        True\n        &gt;&gt;&gt; result2 = VerificationResult(\n        ...     property=TemporalProperty(...),\n        ...     status=VerificationStatus.UNKNOWN,\n        ...     execution_time=1.0\n        ... )\n        &gt;&gt;&gt; result2.verified\n        False\n    \"\"\"\n    return self.status == VerificationStatus.PROVED\n</code></pre>"},{"location":"api/verification/solver/#upir.verification.solver.VerificationResult.to_dict","title":"<code>to_dict()</code>","text":"<p>Serialize verification result to JSON-compatible dictionary.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with all result fields</p> Example <p>result = VerificationResult( ...     property=TemporalProperty(...), ...     status=VerificationStatus.PROVED, ...     execution_time=1.23 ... ) d = result.to_dict() d[\"status\"] 'PROVED'</p> Source code in <code>upir/verification/solver.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Serialize verification result to JSON-compatible dictionary.\n\n    Returns:\n        Dictionary with all result fields\n\n    Example:\n        &gt;&gt;&gt; result = VerificationResult(\n        ...     property=TemporalProperty(...),\n        ...     status=VerificationStatus.PROVED,\n        ...     execution_time=1.23\n        ... )\n        &gt;&gt;&gt; d = result.to_dict()\n        &gt;&gt;&gt; d[\"status\"]\n        'PROVED'\n    \"\"\"\n    return {\n        \"property\": self.property.to_dict(),\n        \"status\": self.status.value,\n        \"certificate\": (\n            self.certificate.to_dict() if self.certificate else None\n        ),\n        \"counterexample\": self.counterexample,\n        \"execution_time\": self.execution_time,\n        \"cached\": self.cached\n    }\n</code></pre>"},{"location":"api/verification/solver/#upir.verification.solver.VerificationResult.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Deserialize verification result from dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>Dictionary containing result fields</p> required <p>Returns:</p> Type Description <code>VerificationResult</code> <p>VerificationResult instance</p> Example <p>data = { ...     \"property\": {...}, ...     \"status\": \"PROVED\", ...     \"certificate\": None, ...     \"counterexample\": None, ...     \"execution_time\": 1.23, ...     \"cached\": False ... } result = VerificationResult.from_dict(data) result.status == VerificationStatus.PROVED True</p> Source code in <code>upir/verification/solver.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, Any]) -&gt; \"VerificationResult\":\n    \"\"\"\n    Deserialize verification result from dictionary.\n\n    Args:\n        data: Dictionary containing result fields\n\n    Returns:\n        VerificationResult instance\n\n    Example:\n        &gt;&gt;&gt; data = {\n        ...     \"property\": {...},\n        ...     \"status\": \"PROVED\",\n        ...     \"certificate\": None,\n        ...     \"counterexample\": None,\n        ...     \"execution_time\": 1.23,\n        ...     \"cached\": False\n        ... }\n        &gt;&gt;&gt; result = VerificationResult.from_dict(data)\n        &gt;&gt;&gt; result.status == VerificationStatus.PROVED\n        True\n    \"\"\"\n    return cls(\n        property=TemporalProperty.from_dict(data[\"property\"]),\n        status=VerificationStatus(data[\"status\"]),\n        certificate=(\n            ProofCertificate.from_dict(data[\"certificate\"])\n            if data.get(\"certificate\") else None\n        ),\n        counterexample=data.get(\"counterexample\"),\n        execution_time=data.get(\"execution_time\", 0.0),\n        cached=data.get(\"cached\", False)\n    )\n</code></pre>"},{"location":"api/verification/solver/#upir.verification.solver.VerificationResult.__str__","title":"<code>__str__()</code>","text":"<p>Human-readable string representation.</p> Source code in <code>upir/verification/solver.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Human-readable string representation.\"\"\"\n    parts = [\n        f\"VerificationResult({self.status.value}\",\n        f\"property={self.property.predicate}\",\n    ]\n    if self.cached:\n        parts.append(\"cached=True\")\n    if self.execution_time &gt; 0:\n        parts.append(f\"time={self.execution_time:.3f}s\")\n\n    return \", \".join(parts) + \")\"\n</code></pre>"},{"location":"api/verification/solver/#upir.verification.solver.VerificationResult.__repr__","title":"<code>__repr__()</code>","text":"<p>Developer-friendly representation.</p> Source code in <code>upir/verification/solver.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Developer-friendly representation.\"\"\"\n    return (\n        f\"VerificationResult(status={self.status.value}, \"\n        f\"property={self.property.predicate}, \"\n        f\"verified={self.verified}, \"\n        f\"cached={self.cached})\"\n    )\n</code></pre>"},{"location":"api/verification/solver/#upir.verification.solver.ProofCertificate","title":"<code>upir.verification.solver.ProofCertificate</code>  <code>dataclass</code>","text":"<p>A certificate attesting to a formal verification result.</p> <p>Proof certificates provide cryptographically verifiable evidence that a property was (or was not) proved for a specific architecture at a specific time with specific assumptions.</p> <p>Based on TD Commons disclosure, certificates enable: - Reproducibility: Same property + architecture should give same result - Auditability: Trace what was verified and when - Caching: Avoid re-proving same properties</p> <p>Attributes:</p> Name Type Description <code>property_hash</code> <code>str</code> <p>SHA-256 hash of the temporal property</p> <code>architecture_hash</code> <code>str</code> <p>SHA-256 hash of the architecture</p> <code>status</code> <code>VerificationStatus</code> <p>Verification result status</p> <code>proof_steps</code> <code>List[Dict[str, Any]]</code> <p>List of proof steps (solver-specific format)</p> <code>assumptions</code> <code>List[str]</code> <p>List of assumptions made during proof</p> <code>timestamp</code> <code>datetime</code> <p>When verification was performed (UTC)</p> <code>solver_version</code> <code>str</code> <p>Version of solver used (e.g., \"z3-4.12.2\")</p> Example <p>cert = ProofCertificate( ...     property_hash=\"abc123...\", ...     architecture_hash=\"def456...\", ...     status=VerificationStatus.PROVED, ...     proof_steps=[{\"step\": 1, \"action\": \"simplify\"}], ...     assumptions=[\"network_reliable\"], ...     timestamp=datetime.utcnow(), ...     solver_version=\"z3-4.12.2\" ... ) cert_hash = cert.generate_hash()</p> <p>References: - TD Commons: Proof certificate structure - Cryptographic certificates: SHA-256 for integrity</p> Source code in <code>upir/verification/solver.py</code> <pre><code>@dataclass\nclass ProofCertificate:\n    \"\"\"\n    A certificate attesting to a formal verification result.\n\n    Proof certificates provide cryptographically verifiable evidence that\n    a property was (or was not) proved for a specific architecture at\n    a specific time with specific assumptions.\n\n    Based on TD Commons disclosure, certificates enable:\n    - Reproducibility: Same property + architecture should give same result\n    - Auditability: Trace what was verified and when\n    - Caching: Avoid re-proving same properties\n\n    Attributes:\n        property_hash: SHA-256 hash of the temporal property\n        architecture_hash: SHA-256 hash of the architecture\n        status: Verification result status\n        proof_steps: List of proof steps (solver-specific format)\n        assumptions: List of assumptions made during proof\n        timestamp: When verification was performed (UTC)\n        solver_version: Version of solver used (e.g., \"z3-4.12.2\")\n\n    Example:\n        &gt;&gt;&gt; cert = ProofCertificate(\n        ...     property_hash=\"abc123...\",\n        ...     architecture_hash=\"def456...\",\n        ...     status=VerificationStatus.PROVED,\n        ...     proof_steps=[{\"step\": 1, \"action\": \"simplify\"}],\n        ...     assumptions=[\"network_reliable\"],\n        ...     timestamp=datetime.utcnow(),\n        ...     solver_version=\"z3-4.12.2\"\n        ... )\n        &gt;&gt;&gt; cert_hash = cert.generate_hash()\n\n    References:\n    - TD Commons: Proof certificate structure\n    - Cryptographic certificates: SHA-256 for integrity\n    \"\"\"\n    property_hash: str\n    architecture_hash: str\n    status: VerificationStatus\n    proof_steps: List[Dict[str, Any]] = field(default_factory=list)\n    assumptions: List[str] = field(default_factory=list)\n    timestamp: datetime = field(default_factory=datetime.utcnow)\n    solver_version: str = Z3_VERSION\n\n    def generate_hash(self) -&gt; str:\n        \"\"\"\n        Generate SHA-256 hash of this certificate for integrity verification.\n\n        The hash is computed over all certificate fields (except the hash itself)\n        in a deterministic way to enable verification that the certificate\n        has not been tampered with.\n\n        Returns:\n            Hexadecimal SHA-256 hash string\n\n        Example:\n            &gt;&gt;&gt; cert = ProofCertificate(\n            ...     property_hash=\"abc\",\n            ...     architecture_hash=\"def\",\n            ...     status=VerificationStatus.PROVED,\n            ...     timestamp=datetime(2024, 1, 1, 12, 0, 0),\n            ...     solver_version=\"z3-4.12.2\"\n            ... )\n            &gt;&gt;&gt; hash1 = cert.generate_hash()\n            &gt;&gt;&gt; hash2 = cert.generate_hash()\n            &gt;&gt;&gt; hash1 == hash2  # Deterministic\n            True\n\n        References:\n        - SHA-256: Industry standard cryptographic hash\n        - Python hashlib: https://docs.python.org/3/library/hashlib.html\n        \"\"\"\n        cert_dict = {\n            \"property_hash\": self.property_hash,\n            \"architecture_hash\": self.architecture_hash,\n            \"status\": self.status.value,\n            \"proof_steps\": self.proof_steps,\n            \"assumptions\": sorted(self.assumptions),\n            \"timestamp\": self.timestamp.isoformat(),\n            \"solver_version\": self.solver_version\n        }\n\n        # Create deterministic JSON\n        json_str = json.dumps(cert_dict, sort_keys=True)\n\n        # Compute SHA-256\n        hash_obj = hashlib.sha256(json_str.encode('utf-8'))\n        return hash_obj.hexdigest()\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Serialize proof certificate to JSON-compatible dictionary.\n\n        Returns:\n            Dictionary with all certificate fields\n\n        Example:\n            &gt;&gt;&gt; cert = ProofCertificate(\n            ...     property_hash=\"abc\",\n            ...     architecture_hash=\"def\",\n            ...     status=VerificationStatus.PROVED,\n            ...     timestamp=datetime(2024, 1, 1, 12, 0, 0),\n            ...     solver_version=\"z3-4.12.2\"\n            ... )\n            &gt;&gt;&gt; d = cert.to_dict()\n            &gt;&gt;&gt; d[\"status\"]\n            'PROVED'\n        \"\"\"\n        return {\n            \"property_hash\": self.property_hash,\n            \"architecture_hash\": self.architecture_hash,\n            \"status\": self.status.value,\n            \"proof_steps\": self.proof_steps.copy(),\n            \"assumptions\": self.assumptions.copy(),\n            \"timestamp\": self.timestamp.isoformat(),\n            \"solver_version\": self.solver_version\n        }\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -&gt; \"ProofCertificate\":\n        \"\"\"\n        Deserialize proof certificate from dictionary.\n\n        Args:\n            data: Dictionary containing certificate fields\n\n        Returns:\n            ProofCertificate instance\n\n        Example:\n            &gt;&gt;&gt; data = {\n            ...     \"property_hash\": \"abc\",\n            ...     \"architecture_hash\": \"def\",\n            ...     \"status\": \"PROVED\",\n            ...     \"proof_steps\": [],\n            ...     \"assumptions\": [],\n            ...     \"timestamp\": \"2024-01-01T12:00:00\",\n            ...     \"solver_version\": \"z3-4.12.2\"\n            ... }\n            &gt;&gt;&gt; cert = ProofCertificate.from_dict(data)\n            &gt;&gt;&gt; cert.status == VerificationStatus.PROVED\n            True\n        \"\"\"\n        return cls(\n            property_hash=data[\"property_hash\"],\n            architecture_hash=data[\"architecture_hash\"],\n            status=VerificationStatus(data[\"status\"]),\n            proof_steps=data.get(\"proof_steps\", []),\n            assumptions=data.get(\"assumptions\", []),\n            timestamp=datetime.fromisoformat(data[\"timestamp\"]),\n            solver_version=data.get(\"solver_version\", \"unknown\")\n        )\n\n    def __str__(self) -&gt; str:\n        \"\"\"Human-readable string representation.\"\"\"\n        return (\n            f\"ProofCertificate({self.status.value}, \"\n            f\"solver={self.solver_version})\"\n        )\n</code></pre>"},{"location":"api/verification/solver/#upir.verification.solver.ProofCertificate-functions","title":"Functions","text":""},{"location":"api/verification/solver/#upir.verification.solver.ProofCertificate.generate_hash","title":"<code>generate_hash()</code>","text":"<p>Generate SHA-256 hash of this certificate for integrity verification.</p> <p>The hash is computed over all certificate fields (except the hash itself) in a deterministic way to enable verification that the certificate has not been tampered with.</p> <p>Returns:</p> Type Description <code>str</code> <p>Hexadecimal SHA-256 hash string</p> Example <p>cert = ProofCertificate( ...     property_hash=\"abc\", ...     architecture_hash=\"def\", ...     status=VerificationStatus.PROVED, ...     timestamp=datetime(2024, 1, 1, 12, 0, 0), ...     solver_version=\"z3-4.12.2\" ... ) hash1 = cert.generate_hash() hash2 = cert.generate_hash() hash1 == hash2  # Deterministic True</p> <p>References: - SHA-256: Industry standard cryptographic hash - Python hashlib: https://docs.python.org/3/library/hashlib.html</p> Source code in <code>upir/verification/solver.py</code> <pre><code>def generate_hash(self) -&gt; str:\n    \"\"\"\n    Generate SHA-256 hash of this certificate for integrity verification.\n\n    The hash is computed over all certificate fields (except the hash itself)\n    in a deterministic way to enable verification that the certificate\n    has not been tampered with.\n\n    Returns:\n        Hexadecimal SHA-256 hash string\n\n    Example:\n        &gt;&gt;&gt; cert = ProofCertificate(\n        ...     property_hash=\"abc\",\n        ...     architecture_hash=\"def\",\n        ...     status=VerificationStatus.PROVED,\n        ...     timestamp=datetime(2024, 1, 1, 12, 0, 0),\n        ...     solver_version=\"z3-4.12.2\"\n        ... )\n        &gt;&gt;&gt; hash1 = cert.generate_hash()\n        &gt;&gt;&gt; hash2 = cert.generate_hash()\n        &gt;&gt;&gt; hash1 == hash2  # Deterministic\n        True\n\n    References:\n    - SHA-256: Industry standard cryptographic hash\n    - Python hashlib: https://docs.python.org/3/library/hashlib.html\n    \"\"\"\n    cert_dict = {\n        \"property_hash\": self.property_hash,\n        \"architecture_hash\": self.architecture_hash,\n        \"status\": self.status.value,\n        \"proof_steps\": self.proof_steps,\n        \"assumptions\": sorted(self.assumptions),\n        \"timestamp\": self.timestamp.isoformat(),\n        \"solver_version\": self.solver_version\n    }\n\n    # Create deterministic JSON\n    json_str = json.dumps(cert_dict, sort_keys=True)\n\n    # Compute SHA-256\n    hash_obj = hashlib.sha256(json_str.encode('utf-8'))\n    return hash_obj.hexdigest()\n</code></pre>"},{"location":"api/verification/solver/#upir.verification.solver.ProofCertificate.to_dict","title":"<code>to_dict()</code>","text":"<p>Serialize proof certificate to JSON-compatible dictionary.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with all certificate fields</p> Example <p>cert = ProofCertificate( ...     property_hash=\"abc\", ...     architecture_hash=\"def\", ...     status=VerificationStatus.PROVED, ...     timestamp=datetime(2024, 1, 1, 12, 0, 0), ...     solver_version=\"z3-4.12.2\" ... ) d = cert.to_dict() d[\"status\"] 'PROVED'</p> Source code in <code>upir/verification/solver.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Serialize proof certificate to JSON-compatible dictionary.\n\n    Returns:\n        Dictionary with all certificate fields\n\n    Example:\n        &gt;&gt;&gt; cert = ProofCertificate(\n        ...     property_hash=\"abc\",\n        ...     architecture_hash=\"def\",\n        ...     status=VerificationStatus.PROVED,\n        ...     timestamp=datetime(2024, 1, 1, 12, 0, 0),\n        ...     solver_version=\"z3-4.12.2\"\n        ... )\n        &gt;&gt;&gt; d = cert.to_dict()\n        &gt;&gt;&gt; d[\"status\"]\n        'PROVED'\n    \"\"\"\n    return {\n        \"property_hash\": self.property_hash,\n        \"architecture_hash\": self.architecture_hash,\n        \"status\": self.status.value,\n        \"proof_steps\": self.proof_steps.copy(),\n        \"assumptions\": self.assumptions.copy(),\n        \"timestamp\": self.timestamp.isoformat(),\n        \"solver_version\": self.solver_version\n    }\n</code></pre>"},{"location":"api/verification/solver/#upir.verification.solver.ProofCertificate.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Deserialize proof certificate from dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>Dictionary containing certificate fields</p> required <p>Returns:</p> Type Description <code>ProofCertificate</code> <p>ProofCertificate instance</p> Example <p>data = { ...     \"property_hash\": \"abc\", ...     \"architecture_hash\": \"def\", ...     \"status\": \"PROVED\", ...     \"proof_steps\": [], ...     \"assumptions\": [], ...     \"timestamp\": \"2024-01-01T12:00:00\", ...     \"solver_version\": \"z3-4.12.2\" ... } cert = ProofCertificate.from_dict(data) cert.status == VerificationStatus.PROVED True</p> Source code in <code>upir/verification/solver.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, Any]) -&gt; \"ProofCertificate\":\n    \"\"\"\n    Deserialize proof certificate from dictionary.\n\n    Args:\n        data: Dictionary containing certificate fields\n\n    Returns:\n        ProofCertificate instance\n\n    Example:\n        &gt;&gt;&gt; data = {\n        ...     \"property_hash\": \"abc\",\n        ...     \"architecture_hash\": \"def\",\n        ...     \"status\": \"PROVED\",\n        ...     \"proof_steps\": [],\n        ...     \"assumptions\": [],\n        ...     \"timestamp\": \"2024-01-01T12:00:00\",\n        ...     \"solver_version\": \"z3-4.12.2\"\n        ... }\n        &gt;&gt;&gt; cert = ProofCertificate.from_dict(data)\n        &gt;&gt;&gt; cert.status == VerificationStatus.PROVED\n        True\n    \"\"\"\n    return cls(\n        property_hash=data[\"property_hash\"],\n        architecture_hash=data[\"architecture_hash\"],\n        status=VerificationStatus(data[\"status\"]),\n        proof_steps=data.get(\"proof_steps\", []),\n        assumptions=data.get(\"assumptions\", []),\n        timestamp=datetime.fromisoformat(data[\"timestamp\"]),\n        solver_version=data.get(\"solver_version\", \"unknown\")\n    )\n</code></pre>"},{"location":"api/verification/solver/#upir.verification.solver.ProofCertificate.__str__","title":"<code>__str__()</code>","text":"<p>Human-readable string representation.</p> Source code in <code>upir/verification/solver.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Human-readable string representation.\"\"\"\n    return (\n        f\"ProofCertificate({self.status.value}, \"\n        f\"solver={self.solver_version})\"\n    )\n</code></pre>"},{"location":"api/verification/solver/#see-also","title":"See Also","text":"<ul> <li>Verifier - High-level verification API</li> </ul>"},{"location":"api/verification/verifier/","title":"Verifier","text":"<p>SMT-based formal verification of specifications.</p>"},{"location":"api/verification/verifier/#overview","title":"Overview","text":"<p>The <code>Verifier</code> class uses Z3 SMT solver to prove that architectures satisfy specifications:</p> <ul> <li>Encodes temporal properties as SMT formulas</li> <li>Checks satisfiability using Z3</li> <li>Caches proofs for incremental verification</li> <li>Returns verification results with counterexamples</li> </ul>"},{"location":"api/verification/verifier/#class-documentation","title":"Class Documentation","text":""},{"location":"api/verification/verifier/#upir.verification.verifier.Verifier","title":"<code>upir.verification.verifier.Verifier</code>","text":"<p>Main verification engine using Z3 SMT solver.</p> <p>The verifier encodes distributed system architectures and temporal properties as SMT constraints and uses Z3 to prove or disprove them. Supports caching of results and bounded model checking for temporal properties.</p> <p>Per TD Commons disclosure, verification is the foundation of UPIR, enabling formal guarantees about system behavior.</p> <p>Attributes:</p> Name Type Description <code>timeout</code> <p>Verification timeout in milliseconds (default: 30000 = 30s)</p> <code>enable_cache</code> <p>Whether to cache verification results</p> <code>cache</code> <p>Proof cache instance (if caching enabled)</p> <code>cache_size</code> <p>Maximum cache size with LRU eviction (default: 1000)</p> Example <p>verifier = Verifier(timeout=30000, enable_cache=True, cache_size=1000) result = verifier.verify_property(property, architecture) if result.verified: ...     print(\"Property proved!\") ... else: ...     print(f\"Status: {result.status}\")</p> <p>References: - TD Commons: https://www.tdcommons.org/dpubs_series/8852/ - Z3: https://z3prover.github.io/api/html/namespacez3py.html - Bounded model checking: Clarke et al. (1999)</p> Source code in <code>upir/verification/verifier.py</code> <pre><code>class Verifier:\n    \"\"\"\n    Main verification engine using Z3 SMT solver.\n\n    The verifier encodes distributed system architectures and temporal\n    properties as SMT constraints and uses Z3 to prove or disprove them.\n    Supports caching of results and bounded model checking for temporal\n    properties.\n\n    Per TD Commons disclosure, verification is the foundation of UPIR,\n    enabling formal guarantees about system behavior.\n\n    Attributes:\n        timeout: Verification timeout in milliseconds (default: 30000 = 30s)\n        enable_cache: Whether to cache verification results\n        cache: Proof cache instance (if caching enabled)\n        cache_size: Maximum cache size with LRU eviction (default: 1000)\n\n    Example:\n        &gt;&gt;&gt; verifier = Verifier(timeout=30000, enable_cache=True, cache_size=1000)\n        &gt;&gt;&gt; result = verifier.verify_property(property, architecture)\n        &gt;&gt;&gt; if result.verified:\n        ...     print(\"Property proved!\")\n        ... else:\n        ...     print(f\"Status: {result.status}\")\n\n    References:\n    - TD Commons: https://www.tdcommons.org/dpubs_series/8852/\n    - Z3: https://z3prover.github.io/api/html/namespacez3py.html\n    - Bounded model checking: Clarke et al. (1999)\n    \"\"\"\n\n    def __init__(\n        self,\n        timeout: int = 30000,\n        enable_cache: bool = True,\n        cache_size: int = 1000\n    ):\n        \"\"\"\n        Initialize verifier with timeout and caching options.\n\n        Args:\n            timeout: Timeout in milliseconds (default: 30000 = 30 seconds)\n            enable_cache: Enable result caching (default: True)\n            cache_size: Maximum cache entries with LRU eviction (default: 1000)\n\n        Raises:\n            RuntimeError: If Z3 is not available\n\n        Example:\n            &gt;&gt;&gt; verifier = Verifier(timeout=60000, enable_cache=True, cache_size=500)\n        \"\"\"\n        if not is_z3_available():\n            raise RuntimeError(\n                \"Z3 solver is not available. \"\n                \"Install with: pip install z3-solver\"\n            )\n\n        self.timeout = timeout\n        self.enable_cache = enable_cache\n        self.cache = ProofCache(max_size=cache_size) if enable_cache else None\n\n    def verify_property(\n        self,\n        property: TemporalProperty,\n        architecture: Architecture,\n        assumptions: Optional[List[str]] = None\n    ) -&gt; VerificationResult:\n        \"\"\"\n        Verify a single temporal property against an architecture.\n\n        This is the core verification method. It:\n        1. Checks cache if enabled\n        2. Encodes architecture as SMT constraints\n        3. Encodes property as SMT formula\n        4. Invokes Z3 solver with timeout\n        5. Extracts result, counterexample, or proof certificate\n        6. Caches result if enabled\n\n        Args:\n            property: Temporal property to verify\n            architecture: System architecture to verify against\n            assumptions: Optional list of assumptions (e.g., \"network_reliable\")\n\n        Returns:\n            VerificationResult with status, certificate, counterexample, timing\n\n        Example:\n            &gt;&gt;&gt; prop = TemporalProperty(\n            ...     operator=TemporalOperator.ALWAYS,\n            ...     predicate=\"data_consistent\"\n            ... )\n            &gt;&gt;&gt; result = verifier.verify_property(prop, architecture)\n            &gt;&gt;&gt; print(f\"Verified: {result.verified}\")\n\n        References:\n        - TD Commons: Property verification algorithm\n        - Z3: SMT solving and model extraction\n        \"\"\"\n        assumptions = assumptions or []\n\n        # Check cache first\n        if self.enable_cache:\n            cached = self.cache.get(property, architecture)\n            if cached is not None:\n                return cached\n\n        # Start timing\n        start_time = time.time()\n\n        try:\n            # Create Z3 solver\n            solver = z3.Solver()\n            solver.set(\"timeout\", self.timeout)\n\n            # Encode architecture as SMT constraints\n            arch_constraints = self._encode_architecture(architecture)\n            for constraint in arch_constraints:\n                solver.add(constraint)\n\n            # Encode property as SMT formula\n            # For ALWAYS/EVENTUALLY, we use bounded model checking\n            prop_constraint = self._encode_property(property, architecture)\n            solver.add(prop_constraint)\n\n            # Check satisfiability\n            check_result = solver.check()\n\n            # Compute execution time\n            execution_time = time.time() - start_time\n\n            # Process result\n            if check_result == z3.sat:\n                # Property is satisfiable - this means it CAN hold\n                # For ALWAYS properties, satisfiability means PROVED\n                # For EVENTUALLY, it means we found a path\n                status = VerificationStatus.PROVED\n                counterexample = None\n\n                # Generate proof certificate\n                certificate = ProofCertificate(\n                    property_hash=property.hash(),\n                    architecture_hash=architecture.hash(),\n                    status=status,\n                    proof_steps=[],  # Z3 doesn't expose proof steps easily\n                    assumptions=assumptions,\n                    solver_version=get_z3_version()\n                )\n\n            elif check_result == z3.unsat:\n                # Property is unsatisfiable - cannot hold\n                status = VerificationStatus.DISPROVED\n                counterexample = self._extract_counterexample(solver)\n                certificate = None\n\n            elif check_result == z3.unknown:\n                # Solver couldn't determine (incomplete theory, heuristics failed)\n                reason = solver.reason_unknown()\n                if \"timeout\" in reason.lower():\n                    status = VerificationStatus.TIMEOUT\n                else:\n                    status = VerificationStatus.UNKNOWN\n                counterexample = None\n                certificate = None\n\n            else:\n                # Unexpected result\n                status = VerificationStatus.ERROR\n                counterexample = None\n                certificate = None\n\n            # Create result\n            result = VerificationResult(\n                property=property,\n                status=status,\n                certificate=certificate,\n                counterexample=counterexample,\n                execution_time=execution_time,\n                cached=False\n            )\n\n            # Cache result\n            if self.enable_cache:\n                self.cache.put(property, architecture, result)\n\n            return result\n\n        except Exception as e:\n            # Handle errors gracefully\n            execution_time = time.time() - start_time\n            return VerificationResult(\n                property=property,\n                status=VerificationStatus.ERROR,\n                certificate=None,\n                counterexample={\"error\": str(e)},\n                execution_time=execution_time,\n                cached=False\n            )\n\n    def verify_specification(self, upir: UPIR) -&gt; List[VerificationResult]:\n        \"\"\"\n        Verify all properties in a UPIR specification.\n\n        Verifies both invariants and properties from the formal specification\n        against the architecture. Returns results for all properties.\n\n        Args:\n            upir: UPIR instance with specification and architecture\n\n        Returns:\n            List of VerificationResult, one per property\n\n        Raises:\n            ValueError: If UPIR is missing specification or architecture\n\n        Example:\n            &gt;&gt;&gt; upir = UPIR(...)\n            &gt;&gt;&gt; upir.specification = FormalSpecification(...)\n            &gt;&gt;&gt; upir.architecture = Architecture(...)\n            &gt;&gt;&gt; results = verifier.verify_specification(upir)\n            &gt;&gt;&gt; verified_count = sum(1 for r in results if r.verified)\n            &gt;&gt;&gt; print(f\"Verified {verified_count}/{len(results)} properties\")\n\n        References:\n        - TD Commons: Specification verification workflow\n        \"\"\"\n        if upir.specification is None:\n            raise ValueError(\"UPIR must have a specification to verify\")\n        if upir.architecture is None:\n            raise ValueError(\"UPIR must have an architecture to verify\")\n\n        results = []\n\n        # Verify invariants\n        for invariant in upir.specification.invariants:\n            result = self.verify_property(\n                property=invariant,\n                architecture=upir.architecture,\n                assumptions=upir.specification.assumptions\n            )\n            results.append(result)\n\n        # Verify properties\n        for property in upir.specification.properties:\n            result = self.verify_property(\n                property=property,\n                architecture=upir.architecture,\n                assumptions=upir.specification.assumptions\n            )\n            results.append(result)\n\n        return results\n\n    def verify_incremental(\n        self,\n        upir: UPIR,\n        changed_properties: Set[str]\n    ) -&gt; List[VerificationResult]:\n        \"\"\"\n        Incrementally verify only changed properties, using cache for rest.\n\n        Per TD Commons disclosure, incremental verification is critical for\n        performance when iterating on architectures. Only changed properties\n        are re-verified; unchanged properties use cached results when available.\n\n        This method:\n        1. Invalidates cache if architecture changed (detects by hash)\n        2. Re-verifies changed properties (forced verification)\n        3. Uses cache for unchanged properties (may still miss if not cached)\n        4. Logs cache statistics at INFO level\n\n        Args:\n            upir: UPIR instance with specification and architecture\n            changed_properties: Set of predicate names that changed\n                              (e.g., {\"data_consistent\", \"backup_complete\"})\n\n        Returns:\n            List of VerificationResult for all properties (invariants + properties)\n\n        Raises:\n            ValueError: If UPIR is missing specification or architecture\n\n        Example:\n            &gt;&gt;&gt; # Initial verification (all cache misses)\n            &gt;&gt;&gt; upir = UPIR(...)\n            &gt;&gt;&gt; upir.specification = FormalSpecification(\n            ...     properties=[\n            ...         TemporalProperty(ALWAYS, \"data_consistent\"),\n            ...         TemporalProperty(EVENTUALLY, \"backup_complete\"),\n            ...         TemporalProperty(WITHIN, \"response_fast\", time_bound=1.0)\n            ...     ]\n            ... )\n            &gt;&gt;&gt; upir.architecture = Architecture(...)\n            &gt;&gt;&gt; verifier = Verifier(enable_cache=True)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # First verification - all misses\n            &gt;&gt;&gt; results = verifier.verify_incremental(upir, set())\n            &gt;&gt;&gt; # INFO: Cache statistics: 0 hits, 3 misses, hit rate: 0.0%\n            &gt;&gt;&gt; assert verifier.cache.misses == 3\n            &gt;&gt;&gt; assert verifier.cache.hits == 0\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Second verification with one change - 2 hits, 1 miss\n            &gt;&gt;&gt; results = verifier.verify_incremental(\n            ...     upir,\n            ...     changed_properties={\"backup_complete\"}\n            ... )\n            &gt;&gt;&gt; # INFO: Cache statistics: 2 hits, 4 misses, hit rate: 33.3%\n            &gt;&gt;&gt; assert verifier.cache.hits == 2  # data_consistent, response_fast\n            &gt;&gt;&gt; assert verifier.cache.misses == 4  # 3 initial + 1 for backup_complete\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Third verification with no changes - 3 hits, 0 new misses\n            &gt;&gt;&gt; results = verifier.verify_incremental(upir, set())\n            &gt;&gt;&gt; # INFO: Cache statistics: 5 hits, 4 misses, hit rate: 55.6%\n            &gt;&gt;&gt; assert verifier.cache.hit_rate() &gt; 50.0\n\n        References:\n        - TD Commons: Incremental verification section\n        - Cache invalidation on architecture changes\n        \"\"\"\n        if upir.specification is None:\n            raise ValueError(\"UPIR must have a specification to verify\")\n        if upir.architecture is None:\n            raise ValueError(\"UPIR must have an architecture to verify\")\n\n        # Track previous architecture hash to detect changes\n        # (In production, could store this in UPIR or verifier state)\n        current_arch_hash = upir.architecture.hash()\n\n        # Invalidate cache entries for changed properties\n        if self.enable_cache and changed_properties:\n            # Remove cache entries for changed properties\n            # We need to recompute cache keys for changed properties\n            all_props = list(upir.specification.invariants) + list(upir.specification.properties)\n            for prop in all_props:\n                if prop.predicate in changed_properties:\n                    cache_key = self.cache._compute_key(prop, upir.architecture)\n                    if cache_key in self.cache.cache:\n                        del self.cache.cache[cache_key]\n\n        results = []\n\n        # Verify invariants\n        # verify_property will check cache; changed properties were invalidated above\n        for invariant in upir.specification.invariants:\n            result = self.verify_property(\n                property=invariant,\n                architecture=upir.architecture,\n                assumptions=upir.specification.assumptions\n            )\n            results.append(result)\n\n        # Verify properties\n        for property in upir.specification.properties:\n            result = self.verify_property(\n                property=property,\n                architecture=upir.architecture,\n                assumptions=upir.specification.assumptions\n            )\n            results.append(result)\n\n        # Log cache statistics\n        if self.enable_cache:\n            logger.info(\n                f\"Cache statistics: {self.cache.hits} hits, \"\n                f\"{self.cache.misses} misses, \"\n                f\"hit rate: {self.cache.hit_rate():.1f}%\"\n            )\n\n        return results\n\n    def _encode_architecture(self, architecture: Architecture) -&gt; List[Any]:\n        \"\"\"\n        Encode architecture as SMT constraints.\n\n        Converts architecture components, connections, and deployment\n        into Z3 constraints. This is a simplified encoding - full\n        implementation would need component-specific encodings.\n\n        Args:\n            architecture: Architecture to encode\n\n        Returns:\n            List of Z3 constraints\n\n        Example:\n            &gt;&gt;&gt; constraints = verifier._encode_architecture(arch)\n            &gt;&gt;&gt; for c in constraints:\n            ...     solver.add(c)\n\n        References:\n        - TD Commons: Architecture encoding for verification\n        - Z3: Constraint construction\n        \"\"\"\n        constraints = []\n\n        # For now, we create a simple encoding\n        # Full implementation would encode:\n        # - Component states (running, failed, etc.)\n        # - Network topology (connected, latency, etc.)\n        # - Data consistency (replicated, partitioned, etc.)\n\n        # Create boolean variables for component health\n        for component in architecture.components:\n            comp_id = component.get(\"id\", \"unknown\")\n            # Create a Z3 boolean variable for \"component is healthy\"\n            var = z3.Bool(f\"healthy_{comp_id}\")\n            # For now, assume all components can be healthy\n            # (no constraints - this is a placeholder)\n\n        # Create constraints for connections\n        # If components A and B are connected and both healthy,\n        # they can communicate\n        for connection in architecture.connections:\n            source = connection.get(\"source\", \"unknown\")\n            target = connection.get(\"target\", \"unknown\")\n            # Placeholder: no actual constraints yet\n\n        return constraints\n\n    def _encode_property(\n        self,\n        property: TemporalProperty,\n        architecture: Architecture\n    ) -&gt; Any:\n        \"\"\"\n        Encode temporal property as SMT formula.\n\n        Uses bounded model checking for temporal operators:\n        - ALWAYS: Holds in all states up to bound\n        - EVENTUALLY: Holds in at least one state up to bound\n        - WITHIN: Holds before time bound\n\n        Args:\n            property: Temporal property to encode\n            architecture: Architecture context\n\n        Returns:\n            Z3 constraint representing the property\n\n        Example:\n            &gt;&gt;&gt; constraint = verifier._encode_property(prop, arch)\n            &gt;&gt;&gt; solver.add(constraint)\n\n        References:\n        - Bounded model checking: Clarke et al. (1999)\n        - TD Commons: Temporal property encoding\n        \"\"\"\n        # Get predicate\n        predicate = property.predicate\n\n        # Create a Z3 boolean variable for the predicate\n        pred_var = z3.Bool(predicate)\n\n        # Encode based on operator\n        if property.operator == TemporalOperator.ALWAYS:\n            # ALWAYS P means P must hold in all states\n            # For simplification, we just require P to hold\n            # Full implementation would check all reachable states\n            return pred_var\n\n        elif property.operator == TemporalOperator.EVENTUALLY:\n            # EVENTUALLY P means P must hold in at least one state\n            # For simplification, we just require P to hold\n            # Full implementation would use bounded model checking\n            return pred_var\n\n        elif property.operator == TemporalOperator.WITHIN:\n            # WITHIN(t) P means P must hold before time t\n            # For simplification, we just require P to hold\n            # Full implementation would encode time bounds\n            return pred_var\n\n        elif property.operator == TemporalOperator.UNTIL:\n            # P UNTIL Q means P holds until Q becomes true\n            # For simplification, we just require both\n            # Full implementation would encode temporal sequence\n            param_predicate = property.parameters.get(\"until_predicate\", \"true\")\n            q_var = z3.Bool(param_predicate)\n            return z3.And(pred_var, q_var)\n\n        else:\n            # Unknown operator\n            return z3.BoolVal(True)\n\n    def _extract_counterexample(self, solver: Any) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"\n        Extract counterexample from unsatisfiable solver.\n\n        When a property is disproved, Z3 can provide a model (counterexample)\n        showing why the property doesn't hold.\n\n        Args:\n            solver: Z3 solver with unsat result\n\n        Returns:\n            Dictionary with counterexample data, or None if unavailable\n\n        Example:\n            &gt;&gt;&gt; if solver.check() == z3.unsat:\n            ...     ce = verifier._extract_counterexample(solver)\n            ...     print(f\"Counterexample: {ce}\")\n\n        References:\n        - Z3: Model extraction\n        - SMT-LIB: Counterexample representation\n        \"\"\"\n        # For UNSAT, there's no model\n        # For SAT, we could extract the model\n        # This is a placeholder implementation\n        try:\n            if solver.check() == z3.sat:\n                model = solver.model()\n                counterexample = {}\n                for decl in model.decls():\n                    counterexample[decl.name()] = str(model[decl])\n                return counterexample\n        except Exception:\n            pass\n\n        return None\n\n    def __str__(self) -&gt; str:\n        \"\"\"Human-readable representation.\"\"\"\n        cache_info = str(self.cache) if self.enable_cache else \"disabled\"\n        return (\n            f\"Verifier(timeout={self.timeout}ms, \"\n            f\"cache={cache_info})\"\n        )\n</code></pre>"},{"location":"api/verification/verifier/#upir.verification.verifier.Verifier-functions","title":"Functions","text":""},{"location":"api/verification/verifier/#upir.verification.verifier.Verifier.__init__","title":"<code>__init__(timeout=30000, enable_cache=True, cache_size=1000)</code>","text":"<p>Initialize verifier with timeout and caching options.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>int</code> <p>Timeout in milliseconds (default: 30000 = 30 seconds)</p> <code>30000</code> <code>enable_cache</code> <code>bool</code> <p>Enable result caching (default: True)</p> <code>True</code> <code>cache_size</code> <code>int</code> <p>Maximum cache entries with LRU eviction (default: 1000)</p> <code>1000</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If Z3 is not available</p> Example <p>verifier = Verifier(timeout=60000, enable_cache=True, cache_size=500)</p> Source code in <code>upir/verification/verifier.py</code> <pre><code>def __init__(\n    self,\n    timeout: int = 30000,\n    enable_cache: bool = True,\n    cache_size: int = 1000\n):\n    \"\"\"\n    Initialize verifier with timeout and caching options.\n\n    Args:\n        timeout: Timeout in milliseconds (default: 30000 = 30 seconds)\n        enable_cache: Enable result caching (default: True)\n        cache_size: Maximum cache entries with LRU eviction (default: 1000)\n\n    Raises:\n        RuntimeError: If Z3 is not available\n\n    Example:\n        &gt;&gt;&gt; verifier = Verifier(timeout=60000, enable_cache=True, cache_size=500)\n    \"\"\"\n    if not is_z3_available():\n        raise RuntimeError(\n            \"Z3 solver is not available. \"\n            \"Install with: pip install z3-solver\"\n        )\n\n    self.timeout = timeout\n    self.enable_cache = enable_cache\n    self.cache = ProofCache(max_size=cache_size) if enable_cache else None\n</code></pre>"},{"location":"api/verification/verifier/#upir.verification.verifier.Verifier.verify_property","title":"<code>verify_property(property, architecture, assumptions=None)</code>","text":"<p>Verify a single temporal property against an architecture.</p> <p>This is the core verification method. It: 1. Checks cache if enabled 2. Encodes architecture as SMT constraints 3. Encodes property as SMT formula 4. Invokes Z3 solver with timeout 5. Extracts result, counterexample, or proof certificate 6. Caches result if enabled</p> <p>Parameters:</p> Name Type Description Default <code>property</code> <code>TemporalProperty</code> <p>Temporal property to verify</p> required <code>architecture</code> <code>Architecture</code> <p>System architecture to verify against</p> required <code>assumptions</code> <code>Optional[List[str]]</code> <p>Optional list of assumptions (e.g., \"network_reliable\")</p> <code>None</code> <p>Returns:</p> Type Description <code>VerificationResult</code> <p>VerificationResult with status, certificate, counterexample, timing</p> Example <p>prop = TemporalProperty( ...     operator=TemporalOperator.ALWAYS, ...     predicate=\"data_consistent\" ... ) result = verifier.verify_property(prop, architecture) print(f\"Verified: {result.verified}\")</p> <p>References: - TD Commons: Property verification algorithm - Z3: SMT solving and model extraction</p> Source code in <code>upir/verification/verifier.py</code> <pre><code>def verify_property(\n    self,\n    property: TemporalProperty,\n    architecture: Architecture,\n    assumptions: Optional[List[str]] = None\n) -&gt; VerificationResult:\n    \"\"\"\n    Verify a single temporal property against an architecture.\n\n    This is the core verification method. It:\n    1. Checks cache if enabled\n    2. Encodes architecture as SMT constraints\n    3. Encodes property as SMT formula\n    4. Invokes Z3 solver with timeout\n    5. Extracts result, counterexample, or proof certificate\n    6. Caches result if enabled\n\n    Args:\n        property: Temporal property to verify\n        architecture: System architecture to verify against\n        assumptions: Optional list of assumptions (e.g., \"network_reliable\")\n\n    Returns:\n        VerificationResult with status, certificate, counterexample, timing\n\n    Example:\n        &gt;&gt;&gt; prop = TemporalProperty(\n        ...     operator=TemporalOperator.ALWAYS,\n        ...     predicate=\"data_consistent\"\n        ... )\n        &gt;&gt;&gt; result = verifier.verify_property(prop, architecture)\n        &gt;&gt;&gt; print(f\"Verified: {result.verified}\")\n\n    References:\n    - TD Commons: Property verification algorithm\n    - Z3: SMT solving and model extraction\n    \"\"\"\n    assumptions = assumptions or []\n\n    # Check cache first\n    if self.enable_cache:\n        cached = self.cache.get(property, architecture)\n        if cached is not None:\n            return cached\n\n    # Start timing\n    start_time = time.time()\n\n    try:\n        # Create Z3 solver\n        solver = z3.Solver()\n        solver.set(\"timeout\", self.timeout)\n\n        # Encode architecture as SMT constraints\n        arch_constraints = self._encode_architecture(architecture)\n        for constraint in arch_constraints:\n            solver.add(constraint)\n\n        # Encode property as SMT formula\n        # For ALWAYS/EVENTUALLY, we use bounded model checking\n        prop_constraint = self._encode_property(property, architecture)\n        solver.add(prop_constraint)\n\n        # Check satisfiability\n        check_result = solver.check()\n\n        # Compute execution time\n        execution_time = time.time() - start_time\n\n        # Process result\n        if check_result == z3.sat:\n            # Property is satisfiable - this means it CAN hold\n            # For ALWAYS properties, satisfiability means PROVED\n            # For EVENTUALLY, it means we found a path\n            status = VerificationStatus.PROVED\n            counterexample = None\n\n            # Generate proof certificate\n            certificate = ProofCertificate(\n                property_hash=property.hash(),\n                architecture_hash=architecture.hash(),\n                status=status,\n                proof_steps=[],  # Z3 doesn't expose proof steps easily\n                assumptions=assumptions,\n                solver_version=get_z3_version()\n            )\n\n        elif check_result == z3.unsat:\n            # Property is unsatisfiable - cannot hold\n            status = VerificationStatus.DISPROVED\n            counterexample = self._extract_counterexample(solver)\n            certificate = None\n\n        elif check_result == z3.unknown:\n            # Solver couldn't determine (incomplete theory, heuristics failed)\n            reason = solver.reason_unknown()\n            if \"timeout\" in reason.lower():\n                status = VerificationStatus.TIMEOUT\n            else:\n                status = VerificationStatus.UNKNOWN\n            counterexample = None\n            certificate = None\n\n        else:\n            # Unexpected result\n            status = VerificationStatus.ERROR\n            counterexample = None\n            certificate = None\n\n        # Create result\n        result = VerificationResult(\n            property=property,\n            status=status,\n            certificate=certificate,\n            counterexample=counterexample,\n            execution_time=execution_time,\n            cached=False\n        )\n\n        # Cache result\n        if self.enable_cache:\n            self.cache.put(property, architecture, result)\n\n        return result\n\n    except Exception as e:\n        # Handle errors gracefully\n        execution_time = time.time() - start_time\n        return VerificationResult(\n            property=property,\n            status=VerificationStatus.ERROR,\n            certificate=None,\n            counterexample={\"error\": str(e)},\n            execution_time=execution_time,\n            cached=False\n        )\n</code></pre>"},{"location":"api/verification/verifier/#upir.verification.verifier.Verifier.verify_specification","title":"<code>verify_specification(upir)</code>","text":"<p>Verify all properties in a UPIR specification.</p> <p>Verifies both invariants and properties from the formal specification against the architecture. Returns results for all properties.</p> <p>Parameters:</p> Name Type Description Default <code>upir</code> <code>UPIR</code> <p>UPIR instance with specification and architecture</p> required <p>Returns:</p> Type Description <code>List[VerificationResult]</code> <p>List of VerificationResult, one per property</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If UPIR is missing specification or architecture</p> Example <p>upir = UPIR(...) upir.specification = FormalSpecification(...) upir.architecture = Architecture(...) results = verifier.verify_specification(upir) verified_count = sum(1 for r in results if r.verified) print(f\"Verified {verified_count}/{len(results)} properties\")</p> <p>References: - TD Commons: Specification verification workflow</p> Source code in <code>upir/verification/verifier.py</code> <pre><code>def verify_specification(self, upir: UPIR) -&gt; List[VerificationResult]:\n    \"\"\"\n    Verify all properties in a UPIR specification.\n\n    Verifies both invariants and properties from the formal specification\n    against the architecture. Returns results for all properties.\n\n    Args:\n        upir: UPIR instance with specification and architecture\n\n    Returns:\n        List of VerificationResult, one per property\n\n    Raises:\n        ValueError: If UPIR is missing specification or architecture\n\n    Example:\n        &gt;&gt;&gt; upir = UPIR(...)\n        &gt;&gt;&gt; upir.specification = FormalSpecification(...)\n        &gt;&gt;&gt; upir.architecture = Architecture(...)\n        &gt;&gt;&gt; results = verifier.verify_specification(upir)\n        &gt;&gt;&gt; verified_count = sum(1 for r in results if r.verified)\n        &gt;&gt;&gt; print(f\"Verified {verified_count}/{len(results)} properties\")\n\n    References:\n    - TD Commons: Specification verification workflow\n    \"\"\"\n    if upir.specification is None:\n        raise ValueError(\"UPIR must have a specification to verify\")\n    if upir.architecture is None:\n        raise ValueError(\"UPIR must have an architecture to verify\")\n\n    results = []\n\n    # Verify invariants\n    for invariant in upir.specification.invariants:\n        result = self.verify_property(\n            property=invariant,\n            architecture=upir.architecture,\n            assumptions=upir.specification.assumptions\n        )\n        results.append(result)\n\n    # Verify properties\n    for property in upir.specification.properties:\n        result = self.verify_property(\n            property=property,\n            architecture=upir.architecture,\n            assumptions=upir.specification.assumptions\n        )\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"api/verification/verifier/#upir.verification.verifier.Verifier.verify_incremental","title":"<code>verify_incremental(upir, changed_properties)</code>","text":"<p>Incrementally verify only changed properties, using cache for rest.</p> <p>Per TD Commons disclosure, incremental verification is critical for performance when iterating on architectures. Only changed properties are re-verified; unchanged properties use cached results when available.</p> <p>This method: 1. Invalidates cache if architecture changed (detects by hash) 2. Re-verifies changed properties (forced verification) 3. Uses cache for unchanged properties (may still miss if not cached) 4. Logs cache statistics at INFO level</p> <p>Parameters:</p> Name Type Description Default <code>upir</code> <code>UPIR</code> <p>UPIR instance with specification and architecture</p> required <code>changed_properties</code> <code>Set[str]</code> <p>Set of predicate names that changed               (e.g., {\"data_consistent\", \"backup_complete\"})</p> required <p>Returns:</p> Type Description <code>List[VerificationResult]</code> <p>List of VerificationResult for all properties (invariants + properties)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If UPIR is missing specification or architecture</p> Example <p>References: - TD Commons: Incremental verification section - Cache invalidation on architecture changes</p> Source code in <code>upir/verification/verifier.py</code> <pre><code>def verify_incremental(\n    self,\n    upir: UPIR,\n    changed_properties: Set[str]\n) -&gt; List[VerificationResult]:\n    \"\"\"\n    Incrementally verify only changed properties, using cache for rest.\n\n    Per TD Commons disclosure, incremental verification is critical for\n    performance when iterating on architectures. Only changed properties\n    are re-verified; unchanged properties use cached results when available.\n\n    This method:\n    1. Invalidates cache if architecture changed (detects by hash)\n    2. Re-verifies changed properties (forced verification)\n    3. Uses cache for unchanged properties (may still miss if not cached)\n    4. Logs cache statistics at INFO level\n\n    Args:\n        upir: UPIR instance with specification and architecture\n        changed_properties: Set of predicate names that changed\n                          (e.g., {\"data_consistent\", \"backup_complete\"})\n\n    Returns:\n        List of VerificationResult for all properties (invariants + properties)\n\n    Raises:\n        ValueError: If UPIR is missing specification or architecture\n\n    Example:\n        &gt;&gt;&gt; # Initial verification (all cache misses)\n        &gt;&gt;&gt; upir = UPIR(...)\n        &gt;&gt;&gt; upir.specification = FormalSpecification(\n        ...     properties=[\n        ...         TemporalProperty(ALWAYS, \"data_consistent\"),\n        ...         TemporalProperty(EVENTUALLY, \"backup_complete\"),\n        ...         TemporalProperty(WITHIN, \"response_fast\", time_bound=1.0)\n        ...     ]\n        ... )\n        &gt;&gt;&gt; upir.architecture = Architecture(...)\n        &gt;&gt;&gt; verifier = Verifier(enable_cache=True)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # First verification - all misses\n        &gt;&gt;&gt; results = verifier.verify_incremental(upir, set())\n        &gt;&gt;&gt; # INFO: Cache statistics: 0 hits, 3 misses, hit rate: 0.0%\n        &gt;&gt;&gt; assert verifier.cache.misses == 3\n        &gt;&gt;&gt; assert verifier.cache.hits == 0\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Second verification with one change - 2 hits, 1 miss\n        &gt;&gt;&gt; results = verifier.verify_incremental(\n        ...     upir,\n        ...     changed_properties={\"backup_complete\"}\n        ... )\n        &gt;&gt;&gt; # INFO: Cache statistics: 2 hits, 4 misses, hit rate: 33.3%\n        &gt;&gt;&gt; assert verifier.cache.hits == 2  # data_consistent, response_fast\n        &gt;&gt;&gt; assert verifier.cache.misses == 4  # 3 initial + 1 for backup_complete\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Third verification with no changes - 3 hits, 0 new misses\n        &gt;&gt;&gt; results = verifier.verify_incremental(upir, set())\n        &gt;&gt;&gt; # INFO: Cache statistics: 5 hits, 4 misses, hit rate: 55.6%\n        &gt;&gt;&gt; assert verifier.cache.hit_rate() &gt; 50.0\n\n    References:\n    - TD Commons: Incremental verification section\n    - Cache invalidation on architecture changes\n    \"\"\"\n    if upir.specification is None:\n        raise ValueError(\"UPIR must have a specification to verify\")\n    if upir.architecture is None:\n        raise ValueError(\"UPIR must have an architecture to verify\")\n\n    # Track previous architecture hash to detect changes\n    # (In production, could store this in UPIR or verifier state)\n    current_arch_hash = upir.architecture.hash()\n\n    # Invalidate cache entries for changed properties\n    if self.enable_cache and changed_properties:\n        # Remove cache entries for changed properties\n        # We need to recompute cache keys for changed properties\n        all_props = list(upir.specification.invariants) + list(upir.specification.properties)\n        for prop in all_props:\n            if prop.predicate in changed_properties:\n                cache_key = self.cache._compute_key(prop, upir.architecture)\n                if cache_key in self.cache.cache:\n                    del self.cache.cache[cache_key]\n\n    results = []\n\n    # Verify invariants\n    # verify_property will check cache; changed properties were invalidated above\n    for invariant in upir.specification.invariants:\n        result = self.verify_property(\n            property=invariant,\n            architecture=upir.architecture,\n            assumptions=upir.specification.assumptions\n        )\n        results.append(result)\n\n    # Verify properties\n    for property in upir.specification.properties:\n        result = self.verify_property(\n            property=property,\n            architecture=upir.architecture,\n            assumptions=upir.specification.assumptions\n        )\n        results.append(result)\n\n    # Log cache statistics\n    if self.enable_cache:\n        logger.info(\n            f\"Cache statistics: {self.cache.hits} hits, \"\n            f\"{self.cache.misses} misses, \"\n            f\"hit rate: {self.cache.hit_rate():.1f}%\"\n        )\n\n    return results\n</code></pre>"},{"location":"api/verification/verifier/#upir.verification.verifier.Verifier.verify_incremental--initial-verification-all-cache-misses","title":"Initial verification (all cache misses)","text":"<p>upir = UPIR(...) upir.specification = FormalSpecification( ...     properties=[ ...         TemporalProperty(ALWAYS, \"data_consistent\"), ...         TemporalProperty(EVENTUALLY, \"backup_complete\"), ...         TemporalProperty(WITHIN, \"response_fast\", time_bound=1.0) ...     ] ... ) upir.architecture = Architecture(...) verifier = Verifier(enable_cache=True)</p>"},{"location":"api/verification/verifier/#upir.verification.verifier.Verifier.verify_incremental--first-verification-all-misses","title":"First verification - all misses","text":"<p>results = verifier.verify_incremental(upir, set())</p>"},{"location":"api/verification/verifier/#upir.verification.verifier.Verifier.verify_incremental--info-cache-statistics-0-hits-3-misses-hit-rate-00","title":"INFO: Cache statistics: 0 hits, 3 misses, hit rate: 0.0%","text":"<p>assert verifier.cache.misses == 3 assert verifier.cache.hits == 0</p>"},{"location":"api/verification/verifier/#upir.verification.verifier.Verifier.verify_incremental--second-verification-with-one-change-2-hits-1-miss","title":"Second verification with one change - 2 hits, 1 miss","text":"<p>results = verifier.verify_incremental( ...     upir, ...     changed_properties={\"backup_complete\"} ... )</p>"},{"location":"api/verification/verifier/#upir.verification.verifier.Verifier.verify_incremental--info-cache-statistics-2-hits-4-misses-hit-rate-333","title":"INFO: Cache statistics: 2 hits, 4 misses, hit rate: 33.3%","text":"<p>assert verifier.cache.hits == 2  # data_consistent, response_fast assert verifier.cache.misses == 4  # 3 initial + 1 for backup_complete</p>"},{"location":"api/verification/verifier/#upir.verification.verifier.Verifier.verify_incremental--third-verification-with-no-changes-3-hits-0-new-misses","title":"Third verification with no changes - 3 hits, 0 new misses","text":"<p>results = verifier.verify_incremental(upir, set())</p>"},{"location":"api/verification/verifier/#upir.verification.verifier.Verifier.verify_incremental--info-cache-statistics-5-hits-4-misses-hit-rate-556","title":"INFO: Cache statistics: 5 hits, 4 misses, hit rate: 55.6%","text":"<p>assert verifier.cache.hit_rate() &gt; 50.0</p>"},{"location":"api/verification/verifier/#upir.verification.verifier.Verifier.__str__","title":"<code>__str__()</code>","text":"<p>Human-readable representation.</p> Source code in <code>upir/verification/verifier.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Human-readable representation.\"\"\"\n    cache_info = str(self.cache) if self.enable_cache else \"disabled\"\n    return (\n        f\"Verifier(timeout={self.timeout}ms, \"\n        f\"cache={cache_info})\"\n    )\n</code></pre>"},{"location":"api/verification/verifier/#usage-example","title":"Usage Example","text":"<pre><code>from upir import UPIR, FormalSpecification, Architecture\nfrom upir.verification.verifier import Verifier\nfrom upir.verification.solver import VerificationStatus\n\n# Create UPIR instance\nupir = UPIR(specification=spec, architecture=arch)\n\n# Create verifier\nverifier = Verifier(timeout_ms=10000)\n\n# Verify specification\nresults = verifier.verify_specification(upir)\n\n# Check results\nif results.status == VerificationStatus.PROVED:\n    print(f\"\u2713 Verified! {len(results.proved_properties)} properties proved\")\n    print(f\"Time: {results.total_time_ms:.2f}ms\")\nelif results.status == VerificationStatus.FAILED:\n    print(f\"\u2717 Failed!\")\n    for counterexample in results.counterexamples:\n        print(f\"  Counterexample: {counterexample}\")\nelif results.status == VerificationStatus.UNKNOWN:\n    print(\"? Unknown (timeout or incomplete)\")\n</code></pre>"},{"location":"api/verification/verifier/#incremental-verification","title":"Incremental Verification","text":"<p>The verifier caches proven properties:</p> <pre><code># First verification\nresults1 = verifier.verify_specification(upir)  # ~100ms\n\n# Modify architecture slightly\nupir.architecture.components[0][\"latency_ms\"] += 1\n\n# Second verification (uses cached proofs)\nresults2 = verifier.verify_specification(upir)  # ~10ms\n</code></pre>"},{"location":"api/verification/verifier/#see-also","title":"See Also","text":"<ul> <li>SMT Solver - Low-level SMT solving</li> <li>FormalSpecification - Create specifications</li> </ul>"},{"location":"contributing/setup/","title":"Development Setup","text":"<p>Set up UPIR for development.</p>"},{"location":"contributing/setup/#installation","title":"Installation","text":"<pre><code>git clone https://github.com/bassrehab/upir.git\ncd upir\npip install -e \".[dev]\"\n</code></pre>"},{"location":"contributing/setup/#running-tests","title":"Running Tests","text":"<pre><code>pytest tests/ -v\n</code></pre>"},{"location":"contributing/setup/#see-also","title":"See Also","text":"<ul> <li>Code Style</li> <li>Testing</li> </ul>"},{"location":"contributing/style/","title":"Code Style","text":"<p>UPIR follows standard Python conventions.</p>"},{"location":"contributing/style/#formatting","title":"Formatting","text":"<ul> <li>black for code formatting</li> <li>isort for import sorting</li> <li>Type hints throughout</li> </ul>"},{"location":"contributing/style/#see-also","title":"See Also","text":"<ul> <li>Development Setup</li> </ul>"},{"location":"contributing/testing/","title":"Testing","text":"<p>UPIR maintains &gt;90% test coverage.</p>"},{"location":"contributing/testing/#running-tests","title":"Running Tests","text":"<pre><code>pytest tests/ -v --cov=upir\n</code></pre>"},{"location":"contributing/testing/#see-also","title":"See Also","text":"<ul> <li>Development Setup</li> </ul>"},{"location":"examples/batch/","title":"Batch Processing Example","text":"<p>Example of designing a batch processing pipeline.</p>"},{"location":"examples/batch/#overview","title":"Overview","text":"<p>This example shows how to design and verify a batch data processing pipeline using UPIR.</p>"},{"location":"examples/batch/#see-also","title":"See Also","text":"<ul> <li>Streaming Example</li> <li>Quick Start</li> </ul>"},{"location":"examples/patterns/","title":"Custom Patterns Example","text":"<p>Learn how to create and use custom architectural patterns.</p>"},{"location":"examples/patterns/#overview","title":"Overview","text":"<p>This example demonstrates:</p> <ul> <li>Extracting patterns from architectures</li> <li>Storing patterns in the library</li> <li>Matching new architectures to patterns</li> <li>Updating success rates</li> </ul>"},{"location":"examples/patterns/#see-also","title":"See Also","text":"<ul> <li>Pattern Library Guide</li> <li>Pattern Library API</li> </ul>"},{"location":"examples/streaming/","title":"Streaming Pipeline Example","text":"<p>Complete example of designing a streaming data pipeline with UPIR.</p>"},{"location":"examples/streaming/#overview","title":"Overview","text":"<p>This example demonstrates the full UPIR workflow:</p> <ol> <li>Define formal specification</li> <li>Create architecture</li> <li>Verify specification</li> <li>Synthesize implementation</li> <li>Simulate production metrics</li> <li>Optimize with RL</li> <li>Extract and save pattern</li> </ol>"},{"location":"examples/streaming/#complete-code","title":"Complete Code","text":"<p>See examples/streaming_example.py for the complete working example.</p>"},{"location":"examples/streaming/#step-by-step","title":"Step-by-Step","text":""},{"location":"examples/streaming/#1-define-specification","title":"1. Define Specification","text":"<pre><code>from upir.core.specification import FormalSpecification\nfrom upir.core.temporal import TemporalProperty, TemporalOperator\n\nspec = FormalSpecification(\n    properties=[\n        TemporalProperty(\n            operator=TemporalOperator.EVENTUALLY,\n            predicate=\"all_events_processed\",\n            time_bound=100000  # 100 seconds\n        ),\n        TemporalProperty(\n            operator=TemporalOperator.WITHIN,\n            predicate=\"process_event\",\n            time_bound=100  # 100ms\n        )\n    ],\n    invariants=[\n        TemporalProperty(\n            operator=TemporalOperator.ALWAYS,\n            predicate=\"data_consistent\"\n        )\n    ],\n    constraints={\n        \"latency_p99\": {\"max\": 100.0},\n        \"monthly_cost\": {\"max\": 5000.0},\n        \"throughput_qps\": {\"min\": 10000.0}\n    }\n)\n</code></pre>"},{"location":"examples/streaming/#2-create-architecture","title":"2. Create Architecture","text":"<pre><code>from upir.core.architecture import Architecture\n\ncomponents = [\n    {\n        \"id\": \"pubsub_source\",\n        \"type\": \"pubsub_source\",\n        \"latency_ms\": 5.0,\n        \"cost_monthly\": 500.0\n    },\n    {\n        \"id\": \"beam_processor\",\n        \"type\": \"streaming_processor\",\n        \"latency_ms\": 50.0,\n        \"cost_monthly\": 3000.0\n    },\n    {\n        \"id\": \"bigquery_sink\",\n        \"type\": \"database\",\n        \"latency_ms\": 30.0,\n        \"cost_monthly\": 1200.0\n    }\n]\n\nconnections = [\n    {\"from\": \"pubsub_source\", \"to\": \"beam_processor\", \"latency_ms\": 2.0},\n    {\"from\": \"beam_processor\", \"to\": \"bigquery_sink\", \"latency_ms\": 3.0}\n]\n\narch = Architecture(components=components, connections=connections)\n</code></pre>"},{"location":"examples/streaming/#3-verify","title":"3. Verify","text":"<pre><code>from upir import UPIR\nfrom upir.verification.verifier import Verifier\n\nupir = UPIR(specification=spec, architecture=arch)\nverifier = Verifier()\nresults = verifier.verify_specification(upir)\n\nprint(f\"\u2713 {len(results.proved_properties)} properties verified\")\n</code></pre>"},{"location":"examples/streaming/#see-also","title":"See Also","text":"<ul> <li>Quick Start Guide</li> <li>Specifications Guide</li> </ul>"},{"location":"getting-started/concepts/","title":"Core Concepts","text":"<p>Understanding the key concepts behind UPIR's architecture.</p>"},{"location":"getting-started/concepts/#overview","title":"Overview","text":"<p>UPIR combines three powerful techniques:</p> <ol> <li>Formal Verification - Prove correctness using SMT solvers</li> <li>Program Synthesis - Generate code from specifications</li> <li>Reinforcement Learning - Optimize from production data</li> </ol>"},{"location":"getting-started/concepts/#the-upir-workflow","title":"The UPIR Workflow","text":"<pre><code>graph LR\n    A[Specification] --&gt; B[Architecture]\n    B --&gt; C[Verification]\n    C --&gt; D{Valid?}\n    D --&gt;|Yes| E[Synthesis]\n    D --&gt;|No| F[Fix Architecture]\n    F --&gt; C\n    E --&gt; G[Deployment]\n    G --&gt; H[Metrics]\n    H --&gt; I[Learning]\n    I --&gt; J[Optimized Architecture]\n    J --&gt; C\n</code></pre> <ol> <li>Specify requirements using temporal logic</li> <li>Verify architecture satisfies requirements</li> <li>Synthesize implementation code</li> <li>Deploy and collect metrics</li> <li>Learn to optimize architecture</li> <li>Iterate for continuous improvement</li> </ol>"},{"location":"getting-started/concepts/#formal-specification","title":"Formal Specification","text":""},{"location":"getting-started/concepts/#temporal-properties","title":"Temporal Properties","text":"<p>UPIR uses Linear Temporal Logic (LTL) to express properties that must hold over time.</p>"},{"location":"getting-started/concepts/#operators","title":"Operators","text":"<ul> <li>ALWAYS (<code>\u25a1</code>): Property holds at all times</li> <li> <p>Example: <code>\u25a1 data_consistent</code> - Data is always consistent</p> </li> <li> <p>EVENTUALLY (<code>\u25c7</code>): Property eventually holds</p> </li> <li> <p>Example: <code>\u25c7 task_complete</code> - Task eventually completes</p> </li> <li> <p>WITHIN (<code>\u25c7_{\u2264t}</code>): Property holds within time bound</p> </li> <li> <p>Example: <code>\u25c7_{\u2264100ms} respond</code> - Response within 100ms</p> </li> <li> <p>UNTIL (<code>U</code>): P holds until Q becomes true</p> </li> <li>Example: <code>processing U complete</code> - Keep processing until complete</li> </ul>"},{"location":"getting-started/concepts/#example","title":"Example","text":"<pre><code>from upir.core.temporal import TemporalOperator, TemporalProperty\n\n# Safety property: Data consistency must always hold\nsafety = TemporalProperty(\n    operator=TemporalOperator.ALWAYS,\n    predicate=\"data_consistent\"\n)\n\n# Liveness property: All events eventually processed\nliveness = TemporalProperty(\n    operator=TemporalOperator.EVENTUALLY,\n    predicate=\"all_events_processed\",\n    time_bound=60000  # within 60 seconds\n)\n\n# Performance property: Low latency response\nperformance = TemporalProperty(\n    operator=TemporalOperator.WITHIN,\n    predicate=\"respond\",\n    time_bound=100  # within 100ms\n)\n</code></pre>"},{"location":"getting-started/concepts/#constraints","title":"Constraints","text":"<p>Hard constraints on resources and performance:</p> <pre><code>constraints = {\n    \"latency_p99\": {\"max\": 100.0},      # Max p99 latency\n    \"latency_p50\": {\"max\": 50.0},       # Max median latency\n    \"monthly_cost\": {\"max\": 5000.0},    # Max monthly cost\n    \"throughput_qps\": {\"min\": 10000.0}, # Min queries per second\n    \"availability\": {\"min\": 0.999}      # Min 99.9% uptime\n}\n</code></pre>"},{"location":"getting-started/concepts/#architecture","title":"Architecture","text":""},{"location":"getting-started/concepts/#components","title":"Components","text":"<p>A component is a building block of your system:</p> <pre><code>component = {\n    \"id\": \"unique_id\",\n    \"name\": \"Human-readable name\",\n    \"type\": \"component_type\",  # e.g., \"database\", \"api_gateway\"\n    \"latency_ms\": 50.0,        # Component latency\n    \"cost_monthly\": 500.0,     # Monthly cost in USD\n    \"config\": {                # Component-specific config\n        \"key\": \"value\"\n    }\n}\n</code></pre>"},{"location":"getting-started/concepts/#connections","title":"Connections","text":"<p>Connections define how components communicate:</p> <pre><code>connection = {\n    \"from\": \"component_a\",\n    \"to\": \"component_b\",\n    \"latency_ms\": 5.0  # Network latency\n}\n</code></pre>"},{"location":"getting-started/concepts/#architecture-example","title":"Architecture Example","text":"<pre><code>from upir.core.architecture import Architecture\n\narchitecture = Architecture(\n    components=[\n        {\"id\": \"api\", \"type\": \"api_gateway\", \"latency_ms\": 10},\n        {\"id\": \"db\", \"type\": \"database\", \"latency_ms\": 50}\n    ],\n    connections=[\n        {\"from\": \"api\", \"to\": \"db\", \"latency_ms\": 5}\n    ]\n)\n</code></pre>"},{"location":"getting-started/concepts/#verification","title":"Verification","text":"<p>UPIR uses Z3, a state-of-the-art SMT (Satisfiability Modulo Theories) solver, to verify that architectures satisfy specifications.</p>"},{"location":"getting-started/concepts/#how-it-works","title":"How It Works","text":"<ol> <li>Encode specification as SMT formulas</li> <li>Translate temporal properties to first-order logic</li> <li>Solve using Z3's constraint solver</li> <li>Cache proven properties for incremental verification</li> </ol>"},{"location":"getting-started/concepts/#verification-results","title":"Verification Results","text":"<pre><code>from upir.verification.verifier import Verifier\nfrom upir.verification.solver import VerificationStatus\n\nverifier = Verifier()\nresults = verifier.verify_specification(upir)\n\n# Possible statuses:\n# - VerificationStatus.PROVED: All properties verified\n# - VerificationStatus.FAILED: Counterexample found\n# - VerificationStatus.UNKNOWN: Solver timeout or incomplete\n</code></pre>"},{"location":"getting-started/concepts/#incremental-verification","title":"Incremental Verification","text":"<p>UPIR caches proofs to speed up repeated verification:</p> <pre><code># First verification: ~100ms\nresults1 = verifier.verify_specification(upir)\n\n# Modify architecture slightly\nupir.architecture.components[0][\"latency_ms\"] = 11\n\n# Second verification: ~10ms (cached proofs reused)\nresults2 = verifier.verify_specification(upir)\n</code></pre>"},{"location":"getting-started/concepts/#synthesis","title":"Synthesis","text":"<p>UPIR uses CEGIS (Counterexample-Guided Inductive Synthesis) to generate implementation code from specifications.</p>"},{"location":"getting-started/concepts/#how-cegis-works","title":"How CEGIS Works","text":"<ol> <li>Sketch: Start with a program template with \"holes\"</li> <li>Synthesize: Fill holes to satisfy specification</li> <li>Verify: Check if synthesis is correct</li> <li>Counterexample: If incorrect, use counterexample to refine</li> <li>Iterate: Repeat until correct or max iterations</li> </ol>"},{"location":"getting-started/concepts/#synthesis-example","title":"Synthesis Example","text":"<pre><code>from upir.synthesis.cegis import Synthesizer\n\nsynthesizer = Synthesizer(max_iterations=10)\n\n# Generate sketch from specification\nsketch = synthesizer.generate_sketch(spec)\n\n# Synthesize implementation\nresult = synthesizer.synthesize(upir, sketch)\n\nif result.status.value == \"SUCCESS\":\n    print(f\"Generated code:\\n{result.implementation}\")\n    print(f\"Iterations: {result.iterations}\")\n</code></pre>"},{"location":"getting-started/concepts/#sketch-templates","title":"Sketch Templates","text":"<p>Sketches define the structure with holes (<code>??</code>) to be filled:</p> <pre><code>def process_??_streaming(events):\n    # Configure ??_framework\n    pipeline = beam.Pipeline(options=??)\n\n    # Read from ??_source\n    events = pipeline | beam.io.ReadFromPubSub(??)\n\n    # Transform with ??_logic\n    processed = events | beam.Map(??)\n\n    # Write to ??_sink\n    processed | beam.io.WriteToBigQuery(??)\n\n    return pipeline.run()\n</code></pre>"},{"location":"getting-started/concepts/#learning-optimization","title":"Learning &amp; Optimization","text":"<p>UPIR uses PPO (Proximal Policy Optimization), a reinforcement learning algorithm, to optimize architectures from production metrics.</p>"},{"location":"getting-started/concepts/#how-it-works_1","title":"How It Works","text":"<ol> <li>Observe: Collect production metrics (latency, cost, throughput)</li> <li>Reward: Compute reward based on performance</li> <li>Policy: Neural network maps architecture to action probabilities</li> <li>Update: Adjust architecture to maximize reward</li> <li>Iterate: Repeat over many episodes</li> </ol>"},{"location":"getting-started/concepts/#learning-example","title":"Learning Example","text":"<pre><code>from upir.learning.learner import ArchitectureLearner\n\nlearner = ArchitectureLearner(upir)\n\n# Simulate production metrics\nmetrics = {\n    \"latency_p99\": 85.0,\n    \"monthly_cost\": 4500.0,\n    \"throughput_qps\": 12000.0\n}\n\n# Learn to optimize\noptimized_upir = learner.learn(metrics, episodes=100)\n\nprint(f\"Original cost: ${upir.architecture.total_cost}\")\nprint(f\"Optimized cost: ${optimized_upir.architecture.total_cost}\")\n</code></pre>"},{"location":"getting-started/concepts/#reward-function","title":"Reward Function","text":"<p>The reward balances multiple objectives:</p> <pre><code>reward = (\n    throughput_bonus  # Higher throughput = better\n    - latency_penalty  # Lower latency = better\n    - cost_penalty     # Lower cost = better\n    + correctness_bonus  # Meets all requirements = big bonus\n)\n</code></pre>"},{"location":"getting-started/concepts/#pattern-management","title":"Pattern Management","text":"<p>UPIR can extract, store, and reuse proven architectural patterns.</p>"},{"location":"getting-started/concepts/#pattern-extraction","title":"Pattern Extraction","text":"<pre><code>from upir.patterns.extractor import PatternExtractor\n\nextractor = PatternExtractor()\npattern = extractor.extract(upir)\n\nprint(f\"Pattern: {pattern.name}\")\nprint(f\"Success rate: {pattern.success_rate:.2%}\")\n</code></pre>"},{"location":"getting-started/concepts/#pattern-library","title":"Pattern Library","text":"<pre><code>from upir.patterns.library import PatternLibrary\n\nlibrary = PatternLibrary()\n\n# Add pattern to library\nlibrary.add_pattern(pattern)\n\n# Find matching patterns\nmatches = library.match_architecture(upir, threshold=0.8)\n\nfor pattern, similarity in matches:\n    print(f\"{pattern.name}: {similarity:.2%} similar\")\n</code></pre>"},{"location":"getting-started/concepts/#built-in-patterns","title":"Built-in Patterns","text":"<p>UPIR includes 10 common distributed system patterns:</p> <ol> <li>Streaming ETL - Real-time data pipelines</li> <li>Batch Processing - Large-scale batch jobs</li> <li>Request-Response API - Synchronous request handling</li> <li>Event-Driven Microservices - Asynchronous event processing</li> <li>Lambda Architecture - Batch + streaming hybrid</li> <li>Kappa Architecture - Pure streaming architecture</li> <li>CQRS - Command Query Responsibility Segregation</li> <li>Event Sourcing - Event-driven state management</li> <li>Pub/Sub Fanout - Message broadcasting</li> <li>MapReduce - Distributed data processing</li> </ol>"},{"location":"getting-started/concepts/#upir-instance","title":"UPIR Instance","text":"<p>The <code>UPIR</code> class ties everything together:</p> <pre><code>from upir import UPIR\n\nupir = UPIR(\n    specification=spec,  # FormalSpecification\n    architecture=arch    # Architecture\n)\n\n# Access components\nupir.specification.properties  # List of temporal properties\nupir.architecture.components   # List of components\n\n# Compute metrics\ntotal_latency = upir.architecture.total_latency_ms\ntotal_cost = upir.architecture.total_cost\n\n# Validate\nis_valid = upir.validate()  # Basic validation\n\n# Serialize\nupir_json = upir.to_json()\nupir_loaded = UPIR.from_json(upir_json)\n</code></pre>"},{"location":"getting-started/concepts/#next-steps","title":"Next Steps","text":"<p>Now that you understand the core concepts:</p> <ul> <li>Formal Specifications Guide - Deep dive into specifications</li> <li>Verification Guide - Learn SMT solving</li> <li>Synthesis Guide - Master CEGIS</li> <li>Learning Guide - Understand PPO optimization</li> <li>Pattern Guide - Use pattern library</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide will help you install UPIR on your system.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":""},{"location":"getting-started/installation/#required","title":"Required","text":"<ul> <li>Python 3.11+ - UPIR requires Python 3.11 or later</li> <li>pip - Python package installer</li> </ul>"},{"location":"getting-started/installation/#recommended","title":"Recommended","text":"<ul> <li>Virtual environment - Keep UPIR isolated from other projects</li> <li>Git - For installing from source</li> </ul>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#from-pypi-recommended","title":"From PyPI (Recommended)","text":"<p>Install the latest stable release:</p> <pre><code>pip install upir\n</code></pre>"},{"location":"getting-started/installation/#from-source","title":"From Source","text":"<p>For the latest development version:</p> <pre><code>git clone https://github.com/bassrehab/upir.git\ncd upir\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>UPIR has several optional dependency groups:</p>"},{"location":"getting-started/installation/#google-cloud-platform-integration","title":"Google Cloud Platform Integration","text":"<p>For GCP-specific components (BigQuery, Pub/Sub, Cloud Storage):</p> <pre><code>pip install upir[gcp]\n</code></pre>"},{"location":"getting-started/installation/#development-tools","title":"Development Tools","text":"<p>For contributing to UPIR (testing, linting, formatting):</p> <pre><code>pip install upir[dev]\n</code></pre>"},{"location":"getting-started/installation/#documentation","title":"Documentation","text":"<p>For building documentation:</p> <pre><code>pip install upir[docs]\n</code></pre>"},{"location":"getting-started/installation/#all-optional-dependencies","title":"All Optional Dependencies","text":"<p>Install everything:</p> <pre><code>pip install upir[gcp,dev,docs]\n</code></pre>"},{"location":"getting-started/installation/#core-dependencies","title":"Core Dependencies","text":"<p>UPIR automatically installs these core dependencies:</p> <ul> <li>z3-solver (\u22654.12.2) - SMT solving for formal verification</li> <li>numpy (\u22651.24.3, &lt;2.0.0) - Numerical computing for RL</li> <li>scikit-learn (\u22651.3.0, &lt;2.0.0) - Clustering for pattern extraction</li> </ul>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<p>After installation, verify UPIR is working:</p> <pre><code>import upir\nprint(upir.__version__)  # Should print: 0.1.0\n</code></pre> <p>Test core functionality:</p> <pre><code>from upir import UPIR, FormalSpecification, TemporalProperty, TemporalOperator\n\n# Create a simple temporal property\nprop = TemporalProperty(\n    operator=TemporalOperator.ALWAYS,\n    predicate=\"test_predicate\"\n)\nprint(f\"\u2713 UPIR installed successfully!\")\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#python-version-issues","title":"Python Version Issues","text":"<p>If you get version errors, check your Python version:</p> <pre><code>python --version  # Should be 3.11 or higher\n</code></pre> <p>If you have multiple Python versions, use:</p> <pre><code>python3.11 -m pip install upir\n</code></pre>"},{"location":"getting-started/installation/#z3-solver-issues","title":"Z3 Solver Issues","text":"<p>If Z3 installation fails, try installing it separately:</p> <pre><code>pip install z3-solver\n</code></pre>"},{"location":"getting-started/installation/#permission-errors","title":"Permission Errors","text":"<p>On Linux/macOS, you might need:</p> <pre><code>pip install --user upir\n</code></pre> <p>Or use a virtual environment (recommended):</p> <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\npip install upir\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Once installed, proceed to:</p> <ul> <li>Quick Start Guide - Your first UPIR program</li> <li>Core Concepts - Understand key ideas</li> <li>Examples - Real-world use cases</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get up and running with UPIR in minutes. This guide walks through creating, verifying, and optimizing a simple distributed system architecture.</p>"},{"location":"getting-started/quickstart/#your-first-upir-program","title":"Your First UPIR Program","text":"<p>Let's verify a simple distributed system architecture:</p> <pre><code>from upir import (\n    UPIR,\n    Architecture,\n    FormalSpecification,\n    TemporalProperty,\n    TemporalOperator,\n    Verifier\n)\nfrom upir.verification.solver import VerificationStatus\n\n# Step 1: Define temporal properties\nproperties = [\n    # Data must always be consistent\n    TemporalProperty(\n        operator=TemporalOperator.ALWAYS,\n        predicate=\"data_consistent\"\n    ),\n    # Respond within 100ms\n    TemporalProperty(\n        operator=TemporalOperator.WITHIN,\n        predicate=\"respond\",\n        time_bound=100  # milliseconds\n    )\n]\n\n# Step 2: Create formal specification\nspec = FormalSpecification(\n    properties=properties,\n    constraints={\n        \"latency_p99\": {\"max\": 100.0},  # 100ms max\n        \"monthly_cost\": {\"max\": 1000.0}  # $1000/month max\n    }\n)\n\n# Step 3: Define architecture components\ncomponents = [\n    {\n        \"id\": \"api_gateway\",\n        \"name\": \"API Gateway\",\n        \"type\": \"api_gateway\",\n        \"latency_ms\": 10.0,\n        \"cost_monthly\": 300.0\n    },\n    {\n        \"id\": \"database\",\n        \"name\": \"Database\",\n        \"type\": \"database\",\n        \"latency_ms\": 50.0,\n        \"cost_monthly\": 500.0\n    }\n]\n\nconnections = [\n    {\"from\": \"api_gateway\", \"to\": \"database\", \"latency_ms\": 5.0}\n]\n\narch = Architecture(\n    components=components,\n    connections=connections\n)\n\n# Step 4: Create UPIR instance\nupir = UPIR(\n    specification=spec,\n    architecture=arch\n)\n\n# Step 5: Verify the architecture\nverifier = Verifier()\nresults = verifier.verify_specification(upir)\n\n# Step 6: Check results\nif results.status == VerificationStatus.PROVED:\n    print(\"\u2713 Architecture verified successfully!\")\n    print(f\"  - {len(results.proved_properties)} properties proved\")\n    print(f\"  - Verification time: {results.total_time_ms:.2f}ms\")\nelse:\n    print(\"\u2717 Verification failed\")\n    for failure in results.counterexamples:\n        print(f\"  - {failure}\")\n</code></pre> <p>Expected output: <pre><code>\u2713 Architecture verified successfully!\n  - 2 properties proved\n  - Verification time: 45.23ms\n</code></pre></p>"},{"location":"getting-started/quickstart/#step-by-step-explanation","title":"Step-by-Step Explanation","text":""},{"location":"getting-started/quickstart/#1-define-temporal-properties","title":"1. Define Temporal Properties","text":"<p>UPIR uses Linear Temporal Logic (LTL) to express requirements:</p> <pre><code># ALWAYS operator: Property must hold at all times\nalways_consistent = TemporalProperty(\n    operator=TemporalOperator.ALWAYS,\n    predicate=\"data_consistent\"\n)\n\n# WITHIN operator: Property must occur within time bound\nlow_latency = TemporalProperty(\n    operator=TemporalOperator.WITHIN,\n    predicate=\"respond\",\n    time_bound=100  # milliseconds\n)\n\n# EVENTUALLY operator: Property must eventually occur\neventually_complete = TemporalProperty(\n    operator=TemporalOperator.EVENTUALLY,\n    predicate=\"all_tasks_complete\",\n    time_bound=60000  # 1 minute\n)\n</code></pre>"},{"location":"getting-started/quickstart/#2-create-formal-specification","title":"2. Create Formal Specification","text":"<p>Combine properties and constraints:</p> <pre><code>spec = FormalSpecification(\n    properties=[always_consistent, low_latency],\n    invariants=[eventually_complete],\n    constraints={\n        \"latency_p99\": {\"max\": 100.0},\n        \"monthly_cost\": {\"max\": 5000.0},\n        \"throughput_qps\": {\"min\": 10000.0}\n    }\n)\n</code></pre>"},{"location":"getting-started/quickstart/#3-define-architecture","title":"3. Define Architecture","text":"<p>Specify components and connections:</p> <pre><code>components = [\n    {\n        \"id\": \"component_1\",\n        \"name\": \"Component Name\",\n        \"type\": \"component_type\",\n        \"latency_ms\": 10.0,\n        \"cost_monthly\": 100.0,\n        \"config\": {/* component-specific config */}\n    }\n]\n\nconnections = [\n    {\n        \"from\": \"component_1\",\n        \"to\": \"component_2\",\n        \"latency_ms\": 5.0\n    }\n]\n\narch = Architecture(components=components, connections=connections)\n</code></pre>"},{"location":"getting-started/quickstart/#4-create-upir-instance","title":"4. Create UPIR Instance","text":"<p>Combine specification and architecture:</p> <pre><code>upir = UPIR(\n    specification=spec,\n    architecture=arch\n)\n</code></pre>"},{"location":"getting-started/quickstart/#5-verify","title":"5. Verify","text":"<p>Use the SMT-based verifier:</p> <pre><code>verifier = Verifier()\nresults = verifier.verify_specification(upir)\n\n# Check verification status\nif results.status == VerificationStatus.PROVED:\n    print(f\"\u2713 Verified! {len(results.proved_properties)} properties proved\")\nelif results.status == VerificationStatus.FAILED:\n    print(f\"\u2717 Failed: {results.counterexamples}\")\nelif results.status == VerificationStatus.UNKNOWN:\n    print(\"? Unknown (solver timeout or incomplete)\")\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":""},{"location":"getting-started/quickstart/#synthesis","title":"Synthesis","text":"<p>Generate code from specifications using CEGIS:</p> <pre><code>from upir.synthesis.cegis import Synthesizer\n\nsynthesizer = Synthesizer(max_iterations=10)\nresult = synthesizer.synthesize(upir, sketch)\n\nif result.status.value == \"SUCCESS\":\n    print(f\"Generated implementation:\\n{result.implementation}\")\n</code></pre>"},{"location":"getting-started/quickstart/#optimization","title":"Optimization","text":"<p>Learn from production metrics:</p> <pre><code>from upir.learning.learner import ArchitectureLearner\n\nlearner = ArchitectureLearner(upir)\noptimized_upir = learner.learn(metrics, episodes=100)\n</code></pre>"},{"location":"getting-started/quickstart/#patterns","title":"Patterns","text":"<p>Use the pattern library:</p> <pre><code>from upir.patterns.library import PatternLibrary\n\nlibrary = PatternLibrary()\nmatches = library.match_architecture(upir, threshold=0.8)\n\nfor pattern, similarity in matches:\n    print(f\"{pattern.name}: {similarity:.2%} match\")\n</code></pre>"},{"location":"getting-started/quickstart/#complete-example","title":"Complete Example","text":"<p>For a full end-to-end example including synthesis, optimization, and pattern extraction, see:</p> <ul> <li>Streaming Pipeline Example</li> <li>Batch Processing Example</li> </ul>"},{"location":"getting-started/quickstart/#learn-more","title":"Learn More","text":"<ul> <li>Core Concepts - Understand UPIR's architecture</li> <li>Formal Specifications Guide - Deep dive into specs</li> <li>Verification Guide - Learn about SMT solving</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"guide/learning/","title":"Learning &amp; Optimization","text":"<p>Optimize architectures from production metrics using reinforcement learning.</p>"},{"location":"guide/learning/#overview","title":"Overview","text":"<p>UPIR uses PPO (Proximal Policy Optimization) to learn from production metrics and optimize architectures.</p>"},{"location":"guide/learning/#quick-start","title":"Quick Start","text":"<pre><code>from upir.learning.learner import ArchitectureLearner\n\nlearner = ArchitectureLearner(upir)\noptimized_upir = learner.learn(production_metrics, episodes=100)\n\nprint(f\"Original cost: ${upir.architecture.total_cost}\")\nprint(f\"Optimized cost: ${optimized_upir.architecture.total_cost}\")\n</code></pre>"},{"location":"guide/learning/#see-also","title":"See Also","text":"<ul> <li>RL Optimizer API - Complete API reference</li> <li>PPO API - PPO implementation</li> </ul>"},{"location":"guide/patterns/","title":"Pattern Library","text":"<p>Extract, store, and reuse proven architectural patterns.</p>"},{"location":"guide/patterns/#overview","title":"Overview","text":"<p>UPIR's pattern library helps you:</p> <ul> <li>Extract patterns from verified architectures</li> <li>Store patterns with success rates</li> <li>Match new architectures to existing patterns</li> <li>Reuse proven solutions</li> </ul>"},{"location":"guide/patterns/#quick-start","title":"Quick Start","text":"<pre><code>from upir.patterns.library import PatternLibrary\n\n# Create library\nlibrary = PatternLibrary()\n\n# Match architecture to patterns\nmatches = library.match_architecture(upir, threshold=0.8)\n\nfor pattern, similarity in matches:\n    print(f\"{pattern.name}: {similarity:.2%} match\")\n</code></pre>"},{"location":"guide/patterns/#built-in-patterns","title":"Built-in Patterns","text":"<p>The library includes 10 common distributed system patterns:</p> <ol> <li>Streaming ETL</li> <li>Batch Processing</li> <li>Request-Response API</li> <li>Event-Driven Microservices</li> <li>Lambda Architecture</li> <li>Kappa Architecture</li> <li>CQRS</li> <li>Event Sourcing</li> <li>Pub/Sub Fanout</li> <li>MapReduce</li> </ol>"},{"location":"guide/patterns/#see-also","title":"See Also","text":"<ul> <li>Pattern Library API - Complete API reference</li> <li>Pattern Extractor API - Extract patterns</li> </ul>"},{"location":"guide/specifications/","title":"Formal Specifications","text":"<p>Learn how to write formal specifications using temporal logic and constraints.</p>"},{"location":"guide/specifications/#overview","title":"Overview","text":"<p>Formal specifications define what your system must do, without specifying how it does it. UPIR uses:</p> <ul> <li>Temporal Logic: Express properties that hold over time</li> <li>Constraints: Hard bounds on resources and performance</li> </ul>"},{"location":"guide/specifications/#temporal-properties","title":"Temporal Properties","text":""},{"location":"guide/specifications/#linear-temporal-logic-ltl","title":"Linear Temporal Logic (LTL)","text":"<p>UPIR uses LTL operators to express properties:</p> Operator Symbol Meaning Example ALWAYS \u25a1 Holds at all times <code>\u25a1 data_consistent</code> EVENTUALLY \u25c7 Eventually holds <code>\u25c7 task_complete</code> WITHIN \u25c7_{\u2264t} Holds within time t <code>\u25c7_{\u2264100ms} respond</code> UNTIL U P until Q <code>processing U complete</code>"},{"location":"guide/specifications/#creating-temporal-properties","title":"Creating Temporal Properties","text":"<pre><code>from upir.core.temporal import TemporalOperator, TemporalProperty\n\n# Safety property: Data must always be consistent\nsafety = TemporalProperty(\n    operator=TemporalOperator.ALWAYS,\n    predicate=\"data_consistent\"\n)\n\n# Liveness property: All tasks eventually complete\nliveness = TemporalProperty(\n    operator=TemporalOperator.EVENTUALLY,\n    predicate=\"all_tasks_complete\",\n    time_bound=60000  # 60 seconds\n)\n\n# Performance property: Low latency\nperformance = TemporalProperty(\n    operator=TemporalOperator.WITHIN,\n    predicate=\"respond\",\n    time_bound=100  # 100ms\n)\n</code></pre>"},{"location":"guide/specifications/#constraints","title":"Constraints","text":"<p>Hard constraints define acceptable bounds:</p> <pre><code>constraints = {\n    # Latency constraints\n    \"latency_p99\": {\"max\": 100.0},  # p99 \u2264 100ms\n    \"latency_p50\": {\"max\": 50.0},   # median \u2264 50ms\n\n    # Cost constraints\n    \"monthly_cost\": {\"max\": 5000.0},  # \u2264 $5000/month\n\n    # Throughput constraints\n    \"throughput_qps\": {\"min\": 10000.0},  # \u2265 10k queries/sec\n\n    # Availability constraints\n    \"availability\": {\"min\": 0.999}  # \u2265 99.9% uptime\n}\n</code></pre>"},{"location":"guide/specifications/#complete-specification","title":"Complete Specification","text":"<p>Combine properties and constraints:</p> <pre><code>from upir.core.specification import FormalSpecification\n\nspec = FormalSpecification(\n    properties=[safety, liveness, performance],\n    invariants=[data_integrity, no_data_loss],\n    constraints=constraints\n)\n</code></pre>"},{"location":"guide/specifications/#common-patterns","title":"Common Patterns","text":""},{"location":"guide/specifications/#high-availability-system","title":"High-Availability System","text":"<pre><code>spec = FormalSpecification(\n    properties=[\n        TemporalProperty(\n            operator=TemporalOperator.ALWAYS,\n            predicate=\"system_available\"\n        )\n    ],\n    constraints={\n        \"availability\": {\"min\": 0.9999},  # 99.99% uptime\n        \"failover_time_ms\": {\"max\": 1000}  # Failover in 1s\n    }\n)\n</code></pre>"},{"location":"guide/specifications/#low-latency-api","title":"Low-Latency API","text":"<pre><code>spec = FormalSpecification(\n    properties=[\n        TemporalProperty(\n            operator=TemporalOperator.WITHIN,\n            predicate=\"respond\",\n            time_bound=100  # 100ms\n        )\n    ],\n    constraints={\n        \"latency_p99\": {\"max\": 100.0},\n        \"latency_p50\": {\"max\": 50.0}\n    }\n)\n</code></pre>"},{"location":"guide/specifications/#data-processing-pipeline","title":"Data Processing Pipeline","text":"<pre><code>spec = FormalSpecification(\n    properties=[\n        TemporalProperty(\n            operator=TemporalOperator.EVENTUALLY,\n            predicate=\"all_events_processed\",\n            time_bound=300000  # 5 minutes\n        ),\n        TemporalProperty(\n            operator=TemporalOperator.ALWAYS,\n            predicate=\"no_data_loss\"\n        )\n    ],\n    constraints={\n        \"throughput_events_per_sec\": {\"min\": 10000},\n        \"monthly_cost\": {\"max\": 10000}\n    }\n)\n</code></pre>"},{"location":"guide/specifications/#best-practices","title":"Best Practices","text":""},{"location":"guide/specifications/#1-start-simple","title":"1. Start Simple","text":"<p>Begin with essential properties:</p> <pre><code># Good: Simple, focused\nspec = FormalSpecification(\n    properties=[\n        TemporalProperty(TemporalOperator.ALWAYS, \"data_consistent\")\n    ],\n    constraints={\"latency_p99\": {\"max\": 100.0}}\n)\n\n# Avoid: Too complex initially\nspec = FormalSpecification(\n    properties=[...20 properties...],\n    constraints={...15 constraints...}\n)\n</code></pre>"},{"location":"guide/specifications/#2-use-meaningful-predicates","title":"2. Use Meaningful Predicates","text":"<pre><code># Good: Clear, descriptive\npredicate=\"data_consistent\"\npredicate=\"all_events_processed\"\npredicate=\"payment_confirmed\"\n\n# Avoid: Vague\npredicate=\"ready\"\npredicate=\"done\"\npredicate=\"ok\"\n</code></pre>"},{"location":"guide/specifications/#3-set-realistic-time-bounds","title":"3. Set Realistic Time Bounds","text":"<pre><code># Good: Based on requirements\ntime_bound=100  # 100ms for API response\ntime_bound=60000  # 60s for batch job\n\n# Avoid: Arbitrary\ntime_bound=1  # 1ms (unrealistic)\ntime_bound=999999999  # Effectively unbounded\n</code></pre>"},{"location":"guide/specifications/#4-combine-safety-and-liveness","title":"4. Combine Safety and Liveness","text":"<pre><code># Good: Both safety and liveness\nspec = FormalSpecification(\n    properties=[\n        # Safety: Nothing bad happens\n        TemporalProperty(TemporalOperator.ALWAYS, \"no_data_corruption\"),\n        # Liveness: Something good eventually happens\n        TemporalProperty(TemporalOperator.EVENTUALLY, \"task_complete\")\n    ]\n)\n</code></pre>"},{"location":"guide/specifications/#see-also","title":"See Also","text":"<ul> <li>Verification Guide - Verify specifications</li> <li>API Reference - Specification API</li> <li>Temporal Logic API - Temporal properties</li> </ul>"},{"location":"guide/synthesis/","title":"Synthesis","text":"<p>Generate implementation code from specifications using CEGIS.</p>"},{"location":"guide/synthesis/#overview","title":"Overview","text":"<p>UPIR uses CEGIS (Counterexample-Guided Inductive Synthesis) to automatically generate implementation code from formal specifications.</p>"},{"location":"guide/synthesis/#quick-start","title":"Quick Start","text":"<pre><code>from upir.synthesis.cegis import Synthesizer\n\nsynthesizer = Synthesizer(max_iterations=10)\nsketch = synthesizer.generate_sketch(upir.specification)\nresult = synthesizer.synthesize(upir, sketch)\n\nif result.status.value == \"SUCCESS\":\n    print(f\"Generated code:\\n{result.implementation}\")\n</code></pre>"},{"location":"guide/synthesis/#see-also","title":"See Also","text":"<ul> <li>CEGIS API - Complete API reference</li> <li>Sketch API - Program sketches</li> </ul>"},{"location":"guide/verification/","title":"Verification","text":"<p>Learn how to verify architectures using SMT solving.</p>"},{"location":"guide/verification/#overview","title":"Overview","text":"<p>UPIR uses Z3, a state-of-the-art SMT (Satisfiability Modulo Theories) solver, to formally verify that architectures satisfy specifications.</p>"},{"location":"guide/verification/#quick-start","title":"Quick Start","text":"<pre><code>from upir import UPIR\nfrom upir.verification.verifier import Verifier\nfrom upir.verification.solver import VerificationStatus\n\n# Create verifier\nverifier = Verifier(timeout_ms=10000)\n\n# Verify UPIR instance\nresults = verifier.verify_specification(upir)\n\n# Check results\nif results.status == VerificationStatus.PROVED:\n    print(f\"\u2713 Verified! {len(results.proved_properties)} properties proved\")\n</code></pre>"},{"location":"guide/verification/#see-also","title":"See Also","text":"<ul> <li>Verifier API - Complete API reference</li> <li>Specifications Guide - Write specifications</li> </ul>"}]}